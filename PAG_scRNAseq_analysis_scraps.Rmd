---
title: "Topographic, single-cell gene expression profiling of Periaqueductal Gray neurons"
subtitle: "Code scraps"
author:
  - name: "Oriol Pavon Arocas, Sarah F. Olesen, and Tiago Branco"
    affiliation: "Sainsbury Wellcome Centre for Neural Circuits and Behaviour, University College London, UK"
    email: "oriol.pavon.16@ucl.ac.uk"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_notebook:
    highlight: pygments
    number_sections: FALSE
    theme: lumen
    toc: TRUE
    toc_float: TRUE

#output: rmdformats::readthedown:
  #highlight: pygments
---

***

This is a pipeline to analyse single-cell RNA sequencing data from Periaqueductal Gray neurons (1) isolated from acute midbrain slices of transgenic mice using visually guided aspiration via patch pipettes and (2) processed using SMART-seq2 (Picelli et al., Nature Protocols 2014).

This pipeline has been generated after attending the [EMBL-EBI RNA-Sequence Analysis Course](https://www.ebi.ac.uk/training/events/2019/rna-sequence-analysis) and [attending](https://training.csx.cam.ac.uk/bioinformatics/event/2823386) and following the online course on [Analysis of single cell RNA-seq data](https://github.com/hemberg-lab/scRNA.seq.course) by the [Hemberg Lab](https://www.sanger.ac.uk/science/groups/hemberg-group). Many other resources have been used, including the [Orchestrating Single-Cell Analysis with Bioconductor book](https://bioconductor.org/books/release/OSCA/) by Robert Amezquita, Aaron Lun, Stephanie Hicks, and Raphael Gottardo, the [simpleSingleCell workflow in Bioconductor](https://bioconductor.org/packages/3.9/workflows/html/simpleSingleCell.html) maintained by Aaron Lun, the [rnaseqGene workflow](https://bioconductor.org/packages/3.10/workflows/html/rnaseqGene.html) maintained by Michael Love, the [RNAseq123 workflow](https://bioconductor.org/packages/3.10/workflows/html/RNAseq123.html) maintained by Matthew Ritchie, and the [EGSEA123 workflow](https://bioconductor.org/packages/3.10/workflows/html/EGSEA123.html) maintained by Matthew Ritchie.

Other key resources include [Bioconductor](http://www.bioconductor.org/) (Huber et al., Nature Methods 2015), [scRNA-tools](https://www.scrna-tools.org/), `scater` (McCarthy et al., Bioinformatics 2017), `scran` (Lun et al., F1000Res 2016), `SC3` (Kiselev et al., Nature Methods 2017), `Seurat` (Butler et al., Nature Biotechnology 2018), `clusterExperiment` (Risso et al., PLOS Computational Biology 2018), `limma` (Ritchie et al., Nucleic Acids Research 2015), `DESeq2` (Love et al., Genome Biology 2014), `edgeR` (Robinson et al., Bioinformatics 2010), `MAST` (Finak, McDavid, Yajima et al., Genome Biology 2015), `iSEE` (Rue-Albrecht & Marini et al., F1000Research 2018), `t-SNE` (van der Maaten & Hinton, Journal of Machine Learning Research 2008), `UMAP` (McInnes et al., arXiv 2018), and the [Mathematical Statistics and Machine Learning for Life Sciences](https://towardsdatascience.com/tagged/stats-ml-life-sciences) column by Nikolay Oskolkov.

***

Below are some code scraps that have been taken out of the pipeline in favour of other preferred alternatives. They most likely require further adaptations before being reincorporated.

# STEP 1 | Read Data and Metadata
## Step 1.5 | Add gene-based annotations
We can add gene-based annotation by pulling the annotation from `org.Mm.eg.db` to relate the ENSEMBL identifiers to gene symbols. The `mapIds()` function ensures that only one gene symbol is returned if two symbols map to the same ENSEMBL ID.
```{r}
library(org.Mm.eg.db)
gene_symbols <- mapIds(org.Mm.eg.db, keys = rownames(PAG_data), 
                       keytype ="ENSEMBL", column = "SYMBOL")
PAG_gene_annotations <- data.frame(ENSEMBL = rownames(PAG_data), 
                                   gene_symbols = gene_symbols, 
                                   stringsAsFactors = FALSE)
head(PAG_gene_annotations)
```

We can also identify which rows correspond to mitochondrial genes. To do that we need to use extra annotation describing the genomic location of each gene. For ENSEMBL, this involves using the `TxDb.Mmusculus.UCSC.mm10.ensGene` package.
```{r}
library(TxDb.Mmusculus.UCSC.mm10.ensGene)
gene_location <- mapIds(TxDb.Mmusculus.UCSC.mm10.ensGene, keys = rownames(PAG_data), 
                        keytype = "GENEID", column = "CDSCHROM")
PAG_gene_annotations$Chromosome <- gene_location
which(gene_location=="chrM")
table(PAG_gene_annotations$Chromosome)
```

# STEP 2 | Pre-processing and data Quality Control
## Step 2.3 | Cell QC
### 2.3.5 | Xist expression
A final quality check could be to look at the expression of _Xist_, which should only appear in cells obtained from female mice, as it only expresses in the inactive X chromosome:
```{r}
# Log transform the counts for Xist
PAG_sceset$log2_total_counts_Xist <- log2(PAG_sceset$subsets_XIST_sum + 1)
summary(PAG_sceset$log2_total_counts_Xist)

# Plot Expression values of VGAT and VGluT2 in several ways:
plotExpression(PAG_sceset, "Xist", x = "mouse.id", 
               exprs_values = "logcounts_raw", # alternatively try "counts"
               colour_by = "mouse.sex",
               xlab = "Mouse ID")

plotExpression(PAG_sceset, "Xist", x = "mouse.sex", 
               exprs_values = "logcounts_raw", # alternatively try "counts"
               colour_by = "cell.type",
               xlab = "Mouse sex")

plotExpression(PAG_sceset[, PAG_sceset$mouse.sex == "M"], "Xist", x = "mouse.id", 
               exprs_values = "logcounts_raw", # alternatively try "counts"
               colour_by = "batch.processing",
               xlab = "Mouse ID")

plotExpression(PAG_sceset[, PAG_sceset$mouse.sex == "F"], "Xist", x = "mouse.id", 
               exprs_values = "logcounts_raw", # alternatively try "counts"
               colour_by = "batch.processing",
               xlab = "Mouse ID")

plotColData(PAG_sceset,
            x = "detected",
            y = "subsets_Chromosome_X_sum",
            colour_by = "mouse.id",
            other_fields = "mouse.sex") +
  facet_wrap(~mouse.sex) + 
  xlab("Number of detected genes") +
  ylab("X Chromosome counts")

plotColData(PAG_sceset,
            x = "detected",
            y = "subsets_Chromosome_Y_sum",
            colour_by = "mouse.id",
            other_fields = "mouse.sex") +
  facet_wrap(~mouse.sex) + 
  xlab("Number of detected genes") +
  ylab("Y Chromosome counts")
```

## Step 2.5 | Gene QC
### 2.5.3 | Identities of the most highly expressed genes
It can also be useful to plot gene expression frequency versus mean expression level to assess the effects of technical dropout in the dataset. The `plotExprsFreqVsMean()` function fits a non-linear least squares curve for the relationship between expression frequency and mean expression and uses this to define the number of genes above high technical dropout and the numbers of genes that are expressed (here defined as at least 4 counts) in at least 50% and at least 25% of cells. A subset of genes to be treated as feature controls can be specified, otherwise any feature controls previously defined are used. 

Plot frequency of expression (number of cells with non-zero expression) against the mean. These two metrics should be positively correlated with each other for most genes. Outliers from the trend may warrant further investigation. For example, alignment errors for pseudo-genes of highly-expressed genes will result in features with low means that are expressed in all cells. Conversely, PCR amplification biases (or the presence of rare populations) may result in genes with very high means that are expressed in very few cells. 

*Check out the "Scater Case Study" by McCarthy et al. 2016 for more information.*
```{r}
plotExprsFreqVsMean(PAG_sceset,
                    control = "is_spike") 
plotExprsFreqVsMean(PAG_sceset[ , colData(PAG_sceset)$use],
                    control = "is_spike")
```

## Step 2.5b | Cell cycle phase [OPTIONAL]
It is possible to classify cells into cell cycle phases based on the gene expression data using the function `cyclone` (Sciealdone et al. 2015). This, however, is not super useful in the brain, as many neuronal cell types are expected to lie in the G0 resting phase, which is distinct from the other phases of the cell cycle (Coller et al., 2006). We run it anyway and observe that all the cells are in G1.

Cells are classified as being in G1 phase if the G1 score is above 0.5 and greater than the G2/M score; in G2/M phase if the G2/M score is above 0.5 and greater than the G1 score; and in S phase if neither score is above 0.5.

*See McCarthy et al. F1000Research 2016 for an explanation of this step and the code.*
```{r}
library(scran)
library(AnnotationDbi)
library(org.Mm.eg.db)
set.seed(1991)

mm.pairs <- readRDS(system.file("exdata", "mouse_cycle_markers.rds", package="scran")) 
assignments <- cyclone(PAG_sceset, mm.pairs, gene.names=rowData(PAG_sceset)$ENSEMBL) 
plot(assignments$score$G1, assignments$score$G2M, xlab="G1 score", ylab="G2/M score", pch=16, xlim=c(0,1), ylim=c(0,1))
```

We could store the cell cycle phases for future use:
```{r}
PAG_sceset$cycle_phase <- assignments$phases
```

# STEP 3 | Normalization of cell-specific biases
## Step 3.1b | Compute size factors for spike-in transcripts (Bioconductor =< 3.9)
Size factors computed from the counts for endogenous genes are usually not appropriate for normalizing the counts for spike-in transcripts. The normalization we implemented in the previous step corrects for RNA content. HOwever, spike-in transcripts are not affected by RNA content, as we theoretically added the same amount to each sample. Therefore, using gene-based size factors would "over-normalize" spike-ins. To ensure spike-in normalization is performed correctly, we compute a separate set of size factors for the spike-in set. For each cell, the spike-in-specific size factor is defined as the total count across all transcripts in the spike-in set.

These size factors are stored in a separate field of the `SingleCellExperiment` object by setting `general.use=FALSE` in `computeSpikeFactors()`. This ensures that they will only be used with the spike-in transcripts and not the endogenous genes. Although we are only using spike-ins for quality control, we could use the spike-in size factors to normalize all genes by setting `general.use=TRUE`.
```{r}
PAG_sceset_qc <- computeSpikeFactors(PAG_sceset_qc, type = "ERCC", assay.type = "counts", general.use = FALSE)
# The warning "zero spike-in counts during spike-in normalization" appears because some size factors are 0, which below 1e-8 (see https://github.com/MarioniLab/scran/blob/master/R/computeSpikeFactors.R).
summary(sizeFactors(PAG_sceset_qc, "ERCC"))
length(sizeFactors(PAG_sceset_qc, "ERCC"))
which(sizeFactors(PAG_sceset_qc, "ERCC") < 1e-8)

# To overcome this inconvenience, we add 1e-7 to each spike-in size factor. This should be okay as it is a change barely above this limit and it allows us to circumvent the warning and run normalize() later on. Otherwise normalize() requires all size factors to be positive real numbers, and unlike computeSizeFactors(), computeSpikeFactors() does not have a postive=TRUE argument as input.
sizeFactors(PAG_sceset_qc, "ERCC") <- sizeFactors(PAG_sceset_qc, "ERCC") + 1e-8
summary(sizeFactors(PAG_sceset_qc, "ERCC"))
which(sizeFactors(PAG_sceset_qc, "ERCC") < 1e-8)
```

The two sets of size factors tend to agree less due to the effects of heterogeneity in total RNA content between cells - this is expected.
```{r}
plot(sizeFactors(PAG_sceset_qc, 'ERCC'), 
     sizeFactors(PAG_sceset_qc),
     log = "xy", 
     xlab = "Size factor (ERCC)", 
     ylab = "Size factor (genes)", 
     col = c(rgb(255, 119, 124, 255/2, maxColorValue = 255), rgb(0, 156, 181, 255/3, maxColorValue = 255))[factor(PAG_sceset_qc$cell.type)],
     type = "p", pch = 19, cex = 1.25)
legend("bottomleft", col = c(rgb(255, 119, 124, 255/2, maxColorValue = 255), rgb(0, 156, 181, 255/3, maxColorValue = 255)), pch = 19, cex = 1.25,  bty = "n",
       legend = levels(factor(PAG_sceset_qc$cell.type)))
```

_If we were to apply the the spike-in factors to all counts, we would set `general.use=TRUE` and use the `computeSpikeFactors` method to estimate size factors for all cells. This would compute the total count over all spike-in transcripts in each cell, and calculate size factors to equalize the total spike-in count across cells. Finally, running normalize would use the spike-in-based size factors to compute normalized log-expression values, and unlike what we actually did, we would not have to define separate size factors for the spike-in transcripts, as the relevant factors would already be used for all genes and spike-in transcripts when `general.use=TRUE`._

# STEP 4 | Modelling technical and biological variability in gene expression to identify highly variable genes
## PATH "A" to feature selection - trendVar (Bioconductor =< 3.9)
### Step 4a.1 | Modelling the technical component of variation
Variability in the observed expression values across genes can be driven by genuine biological heterogeneity or uninteresting technical noise. To distinguish between these two possibilties, we need to model the technical component of the variance of the expression values for each gene. 

We will use the `trendVar()` function to fit a mean-variance trend to the read counts.

* We can set `block=` to block on the plate/animal of origin for each cell, to ensure that technical differences between plates do not inflate the variances. This involves estimating the mean and variance of the log-expression separately in each plate, followed by fitting a single trend to the plate-specific means and variances of all spike-in transcripts. In doing so, we implicitly assume that the trend is the same between plates. The use of `block=` also assumes that the average size factor within each plate is close to unity for both endogenous genes and spike-in transcripts.
* Some tuning of trend parameters such as `span` may be required to achieve a suitable fit (default is 0.75).
* Setting `parametric=TRUE` is especially useful for modelling the expected wave-like shape of the mean-variance relationship. (This is not the default setting as it is not robust for arbitrary trend shapes.)

In addition, the `trendVar` function automatically filters out low-abundance genes prior to trend fitting. This ensures that low-abundance genes do not interfere with the fit due to discreteness, which biases the estimate of variability of the variances around the trend; or due to the frequency of low-abundance genes, which reduces the sensitivity of span-based smoothing algorithms at higher abundances. The internal choice of filtering strategy involves a number of considerations:

* Filtering uses the average of log-expression values rather than the (library size-adjusted) average count. The mean log-expression is independent of the variance estimate in a linear modelling framework (Bourgon, Gentleman, and Huber 2010), which ensures that the filter does not introduce spurious trends in the variances at the filter boundary.
* The filter threshold is specified with the `min.mean` argument in `trendVar`. We use the default threshold of 0.1 (`min.mean`) based on the appearance of discrete patterns in the variance estimates for simulated Poisson-distributed counts. Lower thresholds of 0.001-0.01 may be more suitable for very sparse data, e.g., from droplet-based protocols.
* The filter used in `trendVar` is not applied in `decomposeVar` by default. Retention of all genes ensures that weak biological signal from rare subpopulations is not discarded. To apply the filter in `decomposeVar`, users should set `subset.row=rowMeans(logcounts(sce)) > 0.1` in the function call.

__One last comment from Aaron Lun__: On occasion, users may observe a warning from `trendVar()` about the lack of centering in the size factors. Recall that the trend is fitted to the mean and variances of the spike-in transcripts, and the technical component for each endogenous gene is estimated by interpolation. This assumes that an endogenous gene is comparable to a spike-in transcript of the same abundance. In particular, we assume that variation is primarily driven by the magnitude of the counts, based on the well-known mean-variance relationships in count models. Thus, we need to ensure that similarities in the average counts are preserved in the normalized expression values. This is achieved by centering the gene- and spike-in-based size factors in `normalize()`, such that features with similar average counts will also have similar normalized abundances. However, if the `SingleCellExperiment` object was manipulated (e.g., subsetted) after `normalize()` and before `trendVar()`, centering may not be preserved - hence the warning.

#### 4a.1.1 | Using spike-ins
One way to do so is using the set of spike-in transcripts. These were in theory added in the same quantity to each cell. Thus, the spike-in transcripts should exhibit non-biological variability, i.e., any variance in their counts should be technical in origin.

We use the `trendVar()` function to fit a mean-dependent trend to the variances of the log-expression values for the spike-in transcripts. Given the mean abundance of a gene, the fitted value of the trend is then used as an estimate of the technical component for that gene. The biological component of the variance is finally calculated by subtracting the technical component from the total variance of each gene with the `decomposeVar` function.
```{r}
# Fit variance-mean trend without blocking
start_time <- Sys.time() # Takes around 1s
var_fit_spikes <- trendVar(PAG_sceset_qc_norm,
                           #method="loess",
                           use.spikes=TRUE, 
                           parametric=TRUE,
                           min.mean=0.1,
                           loess.args=list(span=0.3)
                           ) 

var_out_spikes <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes)
head(var_out_spikes)
end_time <- Sys.time()
end_time - start_time

# Fit variance-mean trend blocking on mouse.id
start_time <- Sys.time() # Takes around 2s
var_fit_spikes_mouseid <- trendVar(PAG_sceset_qc_norm,
                                   #method="loess",
                                   use.spikes=TRUE, 
                                   parametric=TRUE, 
                                   block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                   min.mean=0.1,
                                   loess.args=list(span=0.3)
                                   ) 

var_out_spikes_mouseid <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes_mouseid)
head(var_out_spikes_mouseid)
end_time <- Sys.time()
end_time - start_time

# Fit variance-mean trend blocking on batch.processing
start_time <- Sys.time() # Takes around 2s
var_fit_spikes_batch <- trendVar(PAG_sceset_qc_norm,
                                 #method="loess",
                                 use.spikes=TRUE, 
                                 parametric=TRUE, 
                                 block=PAG_sceset_qc_norm$batch.processing, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                 min.mean=0.1,
                                 loess.args=list(span=0.3)
                                 ) 

var_out_spikes_batch <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes_batch)
head(var_out_spikes_batch)
end_time <- Sys.time()
end_time - start_time
```

We visually inspect the trend to confirm that it corresponds to the spike-in variances. A wave-like shape is typical of the mean-variance trend for log-expression values. A linear increase in the variance is observed as the mean increases from zero, as larger variances are possible when the counts increase. At very high abundances, the effect of sampling noise decreases due to the law of large numbers, resulting in a decrease in the variance.
```{r}
# Plot variance-mean trend without blocking
plot(var_out_spikes$mean, 
     var_out_spikes$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression",
     ylab="Variance of log-expression")

curve(var_fit_spikes$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes$mean[current_spikes], var_out_spikes$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on mouse.id
plot(var_out_spikes_mouseid$mean, 
     var_out_spikes_mouseid$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on mouse.id",
     ylab="Variance of log-expression")

curve(var_fit_spikes_mouseid$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes_mouseid$mean[current_spikes], var_out_spikes_mouseid$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on batch.processing
plot(var_out_spikes_batch$mean, 
     var_out_spikes_batch$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on batch.processing",
     ylab="Variance of log-expression")

curve(var_fit_spikes_batch$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes_batch$mean[current_spikes], var_out_spikes_batch$total[current_spikes], col="red", pch=16)
```

***
Notes from Aaron Lun's `simpleSingleCell` workflow:

* In practice, trend fitting is complicated by the small number of spike-in transcripts and the uneven distribution of their abundances. In the absence of spike-ins or as an alternative approach, we can set `use.spikes=FALSE` to fit a trend to the variances of the endogenous genes (see 4.1.2). Another alternative would be to create a trend based on the assumption of Poisson technical noise.
* Negative biological components are often obtained from `decomposeVar`. These are intuitively meaningless as it is impossible for a gene to have total variance below technical noise. Nonetheless, such values occur due to imprecise estimation of the total variance, especially for low numbers of cells.
* `decomposeVar` also yields p-values that can be used to define highly variable genes (HVGs) at a specific threshold for the false discovery rate (FDR). We will discuss this in more detail later, as formal detection of HVGs is not necessary for feature selection during data exploration.

#### 4a.1.2 | Without spike-ins
Ideally, the technical component would be estimated by fitting a mean-variance trend to the spike-in transcripts using the `trendVar` function as we have seen before. In practice, this strategy is compromised by the small number of spike-in transcripts, the uneven distribution of their abundances, and (for low numbers of cells) the imprecision of their variance estimates. This makes it difficult to accurately fit a complex mean-dependent trend to the spike-in variances. In some datasets, spike-in RNA may not have been added in appropriate quantities (or indeed at all). It may also be inappropriate to assume Poisson technical noise with `makeTechTrend()`, especially for read count data where amplification noise is non-negligible. In such cases, an alternative approach is to fit the trend to the variance estimates of the endogenous genes, using the `use.spikes=FALSE` setting. This assumes that the majority of genes are not variably expressed, such that the technical component dominates the total variance for those genes. The fitted value of the trend is then used as an estimate of the technical component. 

__NB__: fitting the trend to the variances of the genes with `use.spikes=FALSE` probably overestimates the technical component.
```{r}
# Fit variance-mean trend without blocking
start_time <- Sys.time() # Takes around 5s
var_fit_no_spikes <- trendVar(PAG_sceset_qc_norm, 
                              method="loess", 
                              use.spikes=FALSE,
                              parametric=TRUE,
                              #block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                              min.mean=0.1,
                              loess.args=list(span=0.3)
                              ) 
var_out_no_spikes <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes)
head(var_out_no_spikes)
end_time <- Sys.time()
end_time - start_time

# Fit variance-mean trend blocking on mouse.id
start_time <- Sys.time() # Takes around 20min
var_fit_no_spikes_mouseid <- trendVar(PAG_sceset_qc_norm, 
                                      method="loess", 
                                      use.spikes=FALSE,
                                      parametric=TRUE,
                                      block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                      min.mean=0.1,
                                      loess.args=list(span=0.3)
                                      ) 
var_out_no_spikes_mouseid <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes_mouseid)
head(var_out_no_spikes_mouseid)
end_time <- Sys.time()
end_time - start_time

# Fit variance-mean trend blocking on batch.processing
start_time <- Sys.time() # Takes around 20min
var_fit_no_spikes_batch <- trendVar(PAG_sceset_qc_norm, 
                                    method="loess", 
                                    use.spikes=FALSE,
                                    parametric=TRUE,
                                    block=PAG_sceset_qc_norm$batch.processing, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                    min.mean=0.1,
                                    loess.args=list(span=0.3)
                                    ) 
var_out_no_spikes_batch  <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes_batch)
head(var_out_no_spikes_batch)
end_time <- Sys.time()
end_time - start_time
```

We assess the suitability of the trend fitted to the endogenous variances by examining whether it is consistent with the spike-in variances. If the trend passes through or close to most of the spike-in variances, this indicates that our assumption (that most genes have low levels of biological variability) is valid. This strategy exploits the large number of endogenous genes to obtain a stable trend, with the spike-in transcripts used as diagnostic features rather than in the trend fitting itself. However, if our assumption does not hold, we would instead fit the trend directly to the spike-in variances with the default use.spikes=TRUE. This sacrifices stability to reduce systematic errors in the estimate of the biological component for each gene.
```{r}
# Plot variance-mean trend without blocking
plot(var_out_no_spikes$mean, 
     var_out_no_spikes$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes$mean[current_spikes], var_out_no_spikes$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on mouse.id
plot(var_out_no_spikes_mouseid$mean, 
     var_out_no_spikes_mouseid$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on mouse.id", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes_mouseid$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes_mouseid$mean[current_spikes], var_out_no_spikes_mouseid$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on batch.processing
plot(var_out_no_spikes_batch$mean, 
     var_out_no_spikes_batch$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on batch.processing", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes_batch$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes_batch$mean[current_spikes], var_out_no_spikes_batch$total[current_spikes], col="red", pch=16)
```

### Step 4a.2 | Identifying highly variable genes
HVGs are defined as genes with biological components that are significantly greater than zero. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. Formal detection of HVGs allows us to avoid genes that are highly variable due to technical factors such as sampling noise during RNA capture and library preparation. This adds another level of statistical rigour to our previous analyses, in which we only modelled the technical component.

Identifying HVGs requires estimation of the variance in expression for each gene, followed by decomposition of the variance into biological and technical components (done in the previous step). HVGs are then identified as those genes with the largest biological components. 

We define HVGs as genes with biological components that are significantly greater than zero at a false discovery rate (FDR) of 5% or 1%. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. In addition, we could consider a gene to be a HVG only if it has a biological component greater than or equal to 0.5. For transformed expression values on the log2 scale, this would mean that the average difference in true expression between any two cells will be at least 2-fold. (This reasoning assumes that the true log-expression values are Normally distributed with variance of 0.5. The root-mean-square of the difference between two values is treated as the average log2-fold change between cells and is equal to unity.) We rank the results by the biological component to focus on genes with larger biological variability.
```{r}
# Consider playing with the $FDR threshold and the $bio threshold, try reducing/increasing them to make them more stringent or to get more genes.

# from trendVar with spikes
hvg_out_spikes <- var_out_spikes[which(var_out_spikes$FDR <= 0.05 & var_out_spikes$bio > 0.5), ]
nrow(hvg_out_spikes)

hvg_out_spikes_mouseid <- var_out_spikes_mouseid[which(var_out_spikes_mouseid$FDR <= 0.05 & var_out_spikes_mouseid$bio > 0.5), ]
nrow(hvg_out_spikes_mouseid)

hvg_out_spikes_batch <- var_out_spikes_batch[which(var_out_spikes_batch$FDR <= 0.05 & var_out_spikes_batch$bio > 0.5), ]
nrow(hvg_out_spikes_batch)


# from trendVar without spikes
hvg_out_no_spikes <- var_out_no_spikes[which(var_out_no_spikes$FDR <= 0.05 & var_out_no_spikes$bio > 0.5), ]
nrow(hvg_out_no_spikes)

hvg_out_no_spikes_mouseid <- var_out_no_spikes_mouseid[which(var_out_no_spikes_mouseid$FDR <= 0.05 & var_out_no_spikes_mouseid$bio > 0.5), ]
nrow(hvg_out_no_spikes_mouseid)

hvg_out_no_spikes_batch <- var_out_no_spikes_batch[which(var_out_no_spikes_batch$FDR <= 0.05 & var_out_no_spikes_batch$bio > 0.5), ]
nrow(hvg_out_no_spikes_batch)
```

#### 4a.2.1 | Rank and filter the results
We rank the results to focus on genes with larger biological components. This highlights an interesting aspect of the underlying hypothesis test, which is based on the ratio of the total variance to the expected technical variance. Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be interesting. This is because the ratio can be very large for HVGs that have very low total variance and do not contribute much to the cell-cell heterogeneity.
```{r}
# from trendVar with spikes
hvg_out_spikes <- hvg_out_spikes[order(hvg_out_spikes$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes)
head(hvg_out_spikes)

hvg_out_spikes_mouseid <- hvg_out_spikes_mouseid[order(hvg_out_spikes_mouseid$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes_mouseid)
head(hvg_out_spikes_mouseid)

hvg_out_spikes_batch <- hvg_out_spikes_batch[order(hvg_out_spikes_batch$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes_batch)
head(hvg_out_spikes_batch)

# from trendVar without spikes
hvg_out_no_spikes <- hvg_out_no_spikes[order(hvg_out_no_spikes$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes)
head(hvg_out_no_spikes)

hvg_out_no_spikes_mouseid <- hvg_out_no_spikes_mouseid[order(hvg_out_no_spikes_mouseid$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes_mouseid)
head(hvg_out_no_spikes_mouseid)

hvg_out_no_spikes_batch <- hvg_out_no_spikes_batch[order(hvg_out_no_spikes_batch$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes_batch)
head(hvg_out_no_spikes_batch)
```

__One important thing...__
Before moving on to downstream analysis, we consider dropping any Ribosomal, Mitochondrial, ERCC, sex-specific genes (e.g. XIST), transgenes (EYFP, tdTomato, Cre, TSO concatemers) and genes used for transgenic labeling of cells (VGAT and VGluT2) from the dataset, as some of them are not be biologically informative and others have been used to select the cells. We do so for both our `SingleCellExperiment` object and for the different HVGs:
```{r}
# Identify the genes we want to drop:
is_X_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "X") # Find which genes correspond to chromosome X 
is_Y_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "Y") # Find which genes correspond to chromosome Y
is_feature_control_qc_norm <- (rowData(PAG_sceset_qc_norm)$is_feature_control) # Find genes assigned to feature controls (ERCC, Mitochondrial, Ribosomal, tdTomato, EYFP, Cre, TSO concatemers, VGAT, VGluT2)

filter_genes_qc_norm <- !(is_X_qc_norm | is_Y_qc_norm | is_feature_control_qc_norm)

# Apply the filter to the SingleCellExperiment object:
dim(PAG_sceset_qc_norm)
PAG_sceset_qc_norm_filt <- PAG_sceset_qc_norm[filter_genes_qc_norm, ]
dim(PAG_sceset_qc_norm_filt)

# Apply the filter to the HVGs:
filt_hvg_1 <- rownames(hvg_out_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_filt <- hvg_out_spikes[filt_hvg_1, ]
nrow(hvg_out_spikes_filt)

filt_hvg_2 <- rownames(hvg_out_spikes_mouseid) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_mouseid_filt <- hvg_out_spikes_mouseid[filt_hvg_2,]
nrow(hvg_out_spikes_mouseid_filt)

filt_hvg_3 <- rownames(hvg_out_spikes_batch) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_batch_filt <- hvg_out_spikes_batch[filt_hvg_3,]
nrow(hvg_out_spikes_batch_filt)

filt_hvg_4 <- rownames(hvg_out_no_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_filt <- hvg_out_no_spikes[filt_hvg_4,]
nrow(hvg_out_no_spikes_filt)

filt_hvg_5 <- rownames(hvg_out_no_spikes_mouseid) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_mouseid_filt <- hvg_out_no_spikes_mouseid[filt_hvg_5,]
nrow(hvg_out_no_spikes_mouseid_filt)

filt_hvg_6 <- rownames(hvg_out_no_spikes_batch) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_batch_filt <- hvg_out_no_spikes_batch[filt_hvg_6,]
nrow(hvg_out_no_spikes_batch_filt)
```

#### 4a.2.2 | Visualize, store, and export HVGs
Now that we have removed the genes we are not interested in, we can use `limma` and `vennDiagram` to compare how many genes we get in each condition with or without spike-ins, and with or without blocking:
```{r}
library(limma)
#####
# Inspect the overlap between HVGs obtained with or without using spike-ins to fit a variance trend:
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_mouseid_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_spikes_batch_filt))
venn_diag_spikes_nospikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_filt),
                                              rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_filt))
                                        )

vennDiagram(venn_diag_spikes_nospikes,
            names = c("Spikes", "No_Spikes"),
            circle.col = c("#E69F00", "#56B4E9")
            )
venn_diag_spikes_nospikes_mouseid <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_mouseid_filt),
                                                      rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt))
                                                )

vennDiagram(venn_diag_spikes_nospikes_mouseid,
            names = c("Spikes_MouseID", "No_Spikes_MouseID"),
            circle.col = c("#E69F00", "#56B4E9")
            )

venn_diag_spikes_nospikes_batch <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_batch_filt),
                                                    rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_batch_filt))
                                              )
vennDiagram(venn_diag_spikes_nospikes_batch,
            names = c("Spikes_Batch", "No_Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9")
            )

#####
# Inspect the overlap between HVGs obtained using spike-ins to fit a variance trend, and with or without blocking:
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_spikes_batch_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_spikes_batch_filt))
venn_diag_spikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_filt),
                                     rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_mouseid_filt),
                                     rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_batch_filt))
                               )
vennDiagram(venn_diag_spikes,
            names = c("Spikes", "Spikes_MouseID", "Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9", "#009E73")
            )

#####
# Inspect the overlap between HVGs obtained without using spike-ins to fit a variance trend, and with or without blocking:
sum(rownames(hvg_out_no_spikes_mouseid_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt))
venn_diag_no_spikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_filt),
                                        rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt),
                                        rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_batch_filt))
                                  )
vennDiagram(venn_diag_no_spikes,
            names = c("No_Spikes", "No_Spikes_MouseID", "No_Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9", "#009E73")
            )
```

Taking into account the above Venn Diagrams, the plots of the mean-variance trend we obtained in `Step 4a.1`, and the profiles and total numbers of HVGs selected by each approach, we will proceed by using the HVGs obtained without using spike-ins and without blocking, the trend and variance-mean plot of which achieves the best compromise between spike-ins and endogenous genes.

Before we store the identified HVGs in our `SingleCellExperiment` object, we check the distribution of expression values for the genes with the largest biological components to ensure that the variance estimate is not being dominated by one or two outlier cells.
```{r}
fontsize <- theme(axis.text=element_text(size=12), axis.title=element_text(size=16))

#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_mouseid_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_batch_filt)[1:20]) + fontsize

plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_filt)[1:50]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_mouseid_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_batch_filt)[1:20]) + fontsize
```

Now that we have ordered, filtered, and inspected the identified HVGs we can add the results into the `metadata` component of the `PAG_sceset_qc_norm_filt` object. The metadata component can hold any object, as it is a list container. Any results that weâ€™d like to keep are safe to store here, and a great way to save or share intermediate results that would otherwise be kept in separate objects. Even though we have decided to use the HVGs identified by fitting `trendVar` to the endogenous genes (i.e. without using spike-ins) without any blocking factor, we store the HVGs from the different approaches we used (even without filtering) so we can go back and find them again for any future comparisons we might want to do:
```{r}
# hvg_out_spikes
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes <- rownames(hvg_out_spikes)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_filt <- rownames(hvg_out_spikes_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_filt)

# hvg_out_spikes_mouseid
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid <- rownames(hvg_out_spikes_mouseid)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid_filt <- rownames(hvg_out_spikes_mouseid_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid_filt)

# hvg_out_spikes_batch
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch <- rownames(hvg_out_spikes_batch)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch_filt <- rownames(hvg_out_spikes_batch_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch_filt)

# hvg_out_no_spikes
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes <- rownames(hvg_out_no_spikes)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt <- rownames(hvg_out_no_spikes_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt)

# hvg_out_no_spikes_mouseid
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid <- rownames(hvg_out_no_spikes_mouseid)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid_filt <- rownames(hvg_out_no_spikes_mouseid_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid_filt)

# hvg_out_no_spikes_batch
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch <- rownames(hvg_out_no_spikes_batch)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch_filt <- rownames(hvg_out_no_spikes_batch_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch_filt)
```

We can also export them into a `.tsv` file:
```{r}
# Before filtering unwanted genes:
#write.table(file="PAG_hvg_out_spikes.tsv", hvg_out_spikes, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_mouseid.tsv", hvg_out_spikes_mouseid, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_batch.tsv", hvg_out_spikes_batch, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes.tsv", hvg_out_no_spikes, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_mouseid.tsv", hvg_out_no_spikes_mouseid, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_batch.tsv", hvg_out_no_spikes_batch, sep="\t", quote=FALSE, col.names=NA)

# After filtering unwanted genes:
#write.table(file="PAG_hvg_out_spikes_filt.tsv", hvg_out_spikes_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_mouseid_filt.tsv", hvg_out_spikes_mouseid_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_batch_filt.tsv", hvg_out_spikes_batch_filt, sep="\t", quote=FALSE, col.names=NA)
write.table(file="PAG_hvg_out_no_spikes_filt.tsv", hvg_out_no_spikes_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_mouseid_filt.tsv", hvg_out_no_spikes_mouseid_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_batch_filt.tsv", hvg_out_no_spikes_batch_filt, sep="\t", quote=FALSE, col.names=NA)
```

***
Notes from Aaron Lun's `simpleSingleCell` workflow:

One reason to use the variance of the log-expression values is the fact that the log-transformation protects against genes with strong expression in only one or two cells. This reduces the risk that the set of top HVGs is not dominated by genes with (mostly uninteresting) outlier expression patterns. There are, however, many other strategies for defining HVGs, based on a variety of metrics:

* the coefficient of variation, using the `technicalCV2()` function (Brennecke et al. 2013) or the `DM()` function (Kim et al. 2015) in _scran_.
* the dispersion parameter in the negative binomial distribution, using the `estimateDisp()` function in _edgeR_ (McCarthy, Chen, and Smyth 2012).
* a proportion of total variability, using methods in the _BASiCS_ package (Vallejos, Marioni, and Richardson 2015).

### Step 4a.3 | Identifying correlated gene pairs with Spearman's rho
Another useful procedure is to identify the HVGs that are highly correlated with one another. This distinguishes between HVGs caused by random noise and those involved in driving systematic differences between subpopulations. Correlations between genes can be quantified by computing Spearman's rho, which accommodates non-linear relationships in the expression values. Gene pairs with significantly large positive or negative values of rho are identified using the `correlatePairs()` function. We only apply this function to the set of HVGs, because these genes have large biological components and are more likely to exhibit strong correlations driven by biology. In contrast, calculating correlations for all possible gene pairs would require too much computational time and increase the severity of the multiple testing correction. It may also prioritize uninteresting genes that have strong correlations but low variance, e.g., tightly co-regulated house-keeping genes.
```{r}
# Takes around 1 min
set.seed(1991)
library(scran)

start_time_c1 <- Sys.time()
var.cor <- correlatePairs(PAG_sceset_qc_norm_filt, 
                          subset.row=metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt) 
write.table(file="PAG_correlated_genes.tsv", var.cor, sep="\t", quote=FALSE, row.names=FALSE) 
head(var.cor)
end_time_c1 <- Sys.time()
end_time_c1 - start_time_c1
```

The significance of each correlation is determined using a permutation test. For each pair of genes, the null hypothesis is that the expression profiles of two genes are independent. Shuffling the profiles and recalculating the correlation yields a null distribution that is used to obtain a p-value for each observed correlation value (Phipson & Smyth, 2010). Correction for multiple testing across many gene pairs is performed by controlling the FDR at 5%. Correlated gene pairs can be directly used for experimental validation with orthogonal techniques (e.g., fluorescence-activated cell sorting, immunohistochemistry or RNA fluorescence in situ hybridization) to verify that these expression patterns are genuinely present across the cell population.
```{r}
sig.cor <- var.cor$FDR <= 0.05 
summary(sig.cor)
```

The use of `correlatePairs()` is primarily intended to identify correlated gene pairs for validation studies. Obviously, non-zero correlations do not provide evidence for a direct regulatory interaction, let alone specify causality. To construct regulatory networks involving many genes, we suggest using dedicated packages such as _[WGCNA](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/)_.

#### 4a.4.1 | Larger sets of genes
Larger sets of correlated genes are assembled by treating genes as nodes in a graph and each pair of genes with significantly large correlations as an edge. In particular, an undirected graph is constructed using methods in the `RBGL` package. Highly connected subgraphs are then identified and defined as gene sets. This provides a convenient summary of the pairwise correlations between genes.
```{r}
# Takes around 15 min
library(RBGL) 
start_time_c2 <- Sys.time()
g <- ftM2graphNEL(cbind(var.cor$gene1, var.cor$gene2)[sig.cor,], W=NULL, V=NULL, edgemode="undirected")
cl <- highlyConnSG(g)$clusters # Time consuming step
cl <- cl[order(lengths(cl), decreasing=TRUE)] 
head(cl)
end_time_c2 <- Sys.time()
end_time_c2 - start_time_c2
```

Significant correlations provide evidence for substructure in the dataset, i.e., subpopulations of cells with systematic differences in their expression profiles. The number of significantly correlated HVG pairs represents the strength of the substructure. If many pairs were significant, this would indicate that the subpopulations were clearly defined and distinct from one another. 

## PATH "B" to feature selection - M3Drop
__Based on the online course on [Analysis of single cell RNA-seq data](https://github.com/hemberg-lab/scRNA.seq.course) by the [Hemberg Lab](https://www.sanger.ac.uk/science/groups/hemberg-group).__

We continue using the `PAG_sceset_qc_norm` after normalization. We should thus have a `logcounts` slot in`assays`:
```{r}
# If starting from stored results, load saved filtered dataset from previous Step:
set.seed(1991)
options(stringsAsFactors = FALSE)
library(SingleCellExperiment)
library(scater)
library(scran)
library(matrixStats)
library(M3Drop)

PAG_sceset_qc_norm <- readRDS("PAG_sceset_qc_norm.rds") # Contains filtered cells and genes, and normalized data
assayNames(PAG_sceset_qc_norm)
PAG_sceset_qc_norm
```

scRNA-seq is capable of measuring the expression of many thousands of genes in every cell. However, in most situations only a portion of those will show a response to the biological condition of interest, e.g. differences in cell-type, drivers of differentiation, respond to an environmental stimulus. Most genes detected in a scRNA-seq experiment will only be detected at different levels due to technical noise. One consequence of this is that technical noise and batch effects can obscure the biological signal of interest. Thus, it is often advantageous to perform feature selection to remove those genes which only exhibit technical noise from downstream analysis. Not only does this generally increase the signal:noise ratio in the data; it also reduces the computational complexity of analyses, by reducing the total amount of data to be processed.

For scRNA-seq data, we will be focusing on unsupervised methods of feature selection which donâ€™t require any a priori information, such as cell-type labels or biological group, since they may not be available, or may be unreliable, for many experiments. In contrast, differential expression can be considered a form of supervised feature selection since it uses the known biological label of each sample to identify features (i.e. genes) which are expressed at different levels across groups.

__One important thing...__
Before identifying HVGs, consider dropping any Ribosomal, Mitochondrial, ERCC, sex-specific genes (e.g. XIST), transgenes (EYFP, tdTomato, Cre, TSO concatemers) and genes used for transgenic labeling of cells (VGAT and VGluT2) from the dataset before proceeding to downstream analysis, as some of them are not be biologically informative and others have been used to select the cells.
```{r}
dim(PAG_sceset_qc_norm)

is_X_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "X") # Find which genes correspond to chromosome X 
is_Y_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "Y") # Find which genes correspond to chromosome Y
is_feature_control_qc_norm <- (rowData(PAG_sceset_qc_norm)$is_feature_control) # Find which genes correspond to feature controls (Ribosomal, Mitochondrial, ERCC, transgenes)

filter_genes_qc_norm <- !(is_X_qc_norm | is_Y_qc_norm | is_feature_control_qc_norm)
PAG_sceset_qc_norm <- PAG_sceset_qc_norm[filter_genes_qc_norm, ]
dim(PAG_sceset_qc_norm)
```

Feature selection is performed after QC. `M3Drop` contains two different feature selection methods `M3DropFeatureSelection` which is based on a Michaelis-Menten curve and is designed for full-transcript single-cell RNA-seq data (such as Smart-seq2) and `NBumiFeatureSelectionCombinedDrop` which is based on a negative binomial model and is designed for UMI count data. We will use the former.

`M3Drop` feature selection runs on a normalized (but not log-transformed) expression matrix. This can be extracted from our `SingleCellExperiment` object using the command below. This function is compatible with most single-cell RNA-seq analysis packages including: `scater`, `SingleCellExperiment`, `monocle`, and `Seurat`. It can also convert an existing expression matrix to the correct form (removing undetected genes & normalizing/delogging) if you specify whether the matrix is raw counts, or log transformed. Check the manual for details.
```{r}
#expr_matrix <- M3Drop::M3DropConvertData(PAG_sceset_qc_norm) #
#expr_matrix <- M3DropConvertData(PAG_sceset_qc_norm@assays[["logcounts_raw"]], is.log=TRUE, is.counts=FALSE)
expr_matrix <- M3DropConvertData(PAG_sceset_qc_norm@assays[["counts"]], is.log=FALSE, is.counts=TRUE)
nrow(counts(PAG_sceset_qc_norm)) - nrow(expr_matrix) # check the ConvertData function has dropped undetected genes
```

### Step 4b.1 | Identifying genes vs a Null Model
There are two main approaches to unsupervised feature selection. The first is to identify genes which behave differently from a null model describing just the technical noise expected in the dataset.

If the dataset contains spike-in RNAs they can be used to directly model technical noise. However, measurements of spike-ins may not experience the same technical noise as endogenous transcripts (Svensson et al., 2017). In addition, scRNA-seq experiments often contain only a small number of spike-ins which reduces our confidence in fitted model parameters. We will not use spike-in transcripts beyond QC in our analysis.

#### 4b.1.1 | Highly Variable Genes
The first method proposed to identify features in scRNA-seq datasets was to identify highly variable genes (HVG). HVG assumes that if genes have large differences in expression across cells some of those differences are due to biological difference between the cells rather than technical noise. However, because of the nature of count data, there is a positive relationship between the mean expression of a gene and the variance in the read counts across cells. This relationship must be corrected for to properly identify HVGs.
```{r}
# Plot the relationship between mean expression and variance for all genes in this dataset:
plot(
    rowMeans(expr_matrix), 
    rowVars(expr_matrix), 
    log="xy", 
    pch=16,
    xlab="Mean Expression", 
    ylab="Variance", 
    main=""
)
```

A popular method to correct for the relationship between variance and mean expression was proposed by [Brennecke et al. 2013](http://www.nature.com/nmeth/journal/v10/n11/full/nmeth.2645.html). To use the Brennecke method, we first normalize for library size and then calculate the mean and the square coefficient of variation (variation divided by the squared mean expression). A quadratic curve is fit to the relationship between these two variables for the ERCC spike-in, and then a chi-square test is used to find genes significantly above the curve. This method is included in the `M3Drop` package as the `Brennecke_getVariableGenes(counts, spikes)` function. If the dataset does not contain spike-ins we can use the entire dataset to estimate the technical noise.

In the figure below the red curve is the fitted technical noise model and the dashed line is the 95% CI. Pink dots are the genes with significant biological variability after multiple-testing correction.
```{r}
Brennecke_HVG <- M3Drop::BrenneckeGetVariableGenes(expr_matrix,
                                                   fdr = 0.01,
                                                   minBiolDisp = 0.5
                                                   )
```

This function returns a matrix of significant genes as well as their estimated effect size (difference between observed and expected coefficient of variation), and their significance as raw p.values and FDR corrected q.values. For now we will just keep the names of the significant HVG genes.
```{r}
HVG_genes <- Brennecke_HVG$Gene
length(HVG_genes)
```

#### 4b.1.2 | High Dropout Genes
An alternative to finding HVGs is to identify genes with unexpectedly high numbers of zeros. The frequency of zeros, known as the "dropout rate", is very closely related to expression level in scRNASeq data. Zeros are the dominant feature of single-cell RNASeq data, typically accounting for over half of the entries in the final expression matrix. These zeros predominantly result from the failure of mRNAs failing to be reversed transcribed (Andrews and Hemberg, 2016). Reverse transcription is an enzyme reaction thus can be modelled using the Michaelis-Menten equation: Pdropout=1âˆ’S/(K+S) where S is the mRNA concentration in the cell (we will estimate this as average expression) and K is the Michaelis-Menten constant.

Because the Michaelis-Menten equation is a convex non-linear function, genes which are differentially expression across two or more populations of cells in our dataset will be shifted up/right of the Michaelis-Menten model. We use M3Drop to identify significant outliers to the right of the MM curve. We also apply 1% FDR multiple testing correction:
```{r}
M3Drop_genes <- M3DropFeatureSelection(expr_matrix,
                                       mt_method = "fdr",
                                       mt_threshold = 0.01
                                       )
M3Drop_genes <- M3Drop_genes$Gene
length(M3Drop_genes)
```

### Step 4b.2 | Correlated Expression
A completely different approach to feature selection is to use gene-gene correlations. This method is based on the idea that multiple genes will be differentially expressed between different cell-types or cell-states. Genes which are expressed in the same cell-population will be positively correlated with each other where as genes expressed in different cell-populations will be negatively correated with each other. Thus important genes can be identified by the magnitude of their correlation with other genes.

The limitation of this method is that it assumes technical noise is random and independent for each cell, thus shouldn't produce gene-gene correlations, but this assumption is violated by batch effects which are generally systematic between different experimental batches and will produce gene-gene correlations. As a result it is more appropriate to take the top few thousand genes as ranked by gene-gene correlation than consider the significance of the correlations.
```{r}
cor_feat <- M3Drop::corFS(expr_matrix) # Takes a long time
Cor_genes <- names(cor_feat)[1:1500]
```

### Step 4b.3 | PCA loadings
Lastly, another common method for feature selection in scRNASeq data is to use PCA loadings. Genes with high PCA loadings are likely to be highly variable and correlated with many other variable genes, thus may be relevant to the underlying biology. However, as with gene-gene correlations PCA loadings tend to be susceptible to detecting systematic variation due to batch effects; thus it is recommended to plot the PCA results to determine those components corresponding to the biological variation rather than batch effects.
```{r}
# PCA is typically performed on log-transformed expression data
pca <- prcomp(log(expr_matrix + 1) / log(2))

# plot projection
plot(
    pca$rotation[,1], 
    pca$rotation[,2], 
    pch = 16
)
```

```{r}
# calculate loadings for components 1 and 2
score <- rowSums(abs(pca$x[,c(1,2)])) 
names(score) <- rownames(expr_matrix)
score <- score[order(-score)]
PCA_genes <- names(score[1:1500])
```

```{r}
#Consider the top 5 principal components. Which ones appear to be most biologically relevant? How do the top 1,500 features change if you consider the loadings for those components?
plot(
    pca$rotation[,2], 
    pca$rotation[,3], 
    pch = 16
)

plot(
    pca$rotation[,3], 
    pca$rotation[,4], 
    pch = 16
)

# calculate loadings for components 1 and 2
score <- rowSums(abs(pca$x[,c(2, 3, 4)]))
names(score) <- rownames(expr_matrix)
score <- score[order(-score)]
PCA_genes2 = names(score[1:1500])
```

### Step 4b.4 | Comparing Methods
We can check whether the identified features really do represent genes differentially expressed between cell-types in this dataset.
```{r}
M3DropExpressionHeatmap(M3Drop_genes,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

```{r}
M3DropExpressionHeatmap(HVG_genes,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

```{r}
M3DropExpressionHeatmap(Cor_genes,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

```{r}
M3DropExpressionHeatmap(PCA_genes,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

```{r}
M3DropExpressionHeatmap(PCA_genes2,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

We can also consider how consistent each feature selection method is with the others using the Jaccard Index:
```{r}
list_of_features <- list(M3Drop_genes,
                         HVG_genes, 
                         Cor_genes, 
                         PCA_genes, 
                         PCA_genes2
                         )

Out <- matrix(0, 
              ncol = length(list_of_features), 
              nrow = length(list_of_features)
              )

for(i in 1:length(list_of_features) ) {
    for(j in 1:length(list_of_features) ) {
        Out[i,j] <- sum(list_of_features[[i]] %in% list_of_features[[j]])/
            length(unique(c(list_of_features[[i]], list_of_features[[j]])))
     }
}

colnames(Out) <- rownames(Out) <- c("M3Drop", "HVG", "Cor", "PCA", "PCA2")
Out
```

# STEP 5 | Identifying confounding factors, correcting batch effects, and dimensionality reduction
## Step 5.3 | Dimensionality reduction
### 5.3.3 | Denoising expression values using PCA
Another strategy is to retain all PCs until the percentage of total variation explained reaches some threshold. For example, we might retain the top set of PCs that explains 80% of the total variation in the data. Of course, it would be pointless to swap one arbitrary parameter `d` for another `T`. Instead, we derive a suitable value for `T` by calculating the proportion of variance in the data that is attributed to the biological component. This is done using the `denoisePCA()` function with the variance modelling results from `modelGeneVar()` or related functions, where `T` is defined as the ratio of the sum of the biological components to the sum of total variances. Here, explicit feature selection is not strictly necessary, as `denoisePCA()` will automatically restrict the PCA to genes with positive biological components to ensure that `T` is always a positive value. The function returns a `SingleCellExperiment` object containing the PC scores for each cell in the `reducedDims` slot, where cells are rows and PCs are columns. The aim is to eliminate technical noise and enrich for biological signal in the retained PCs. This improves resolution of the underlying biology during downstream procedures such as clustering.

Importantly, this method tends to perform best when the mean-variance trend reflects the actual technical noise, i.e., when it is estimated by `modelGeneVarByPoisson()` or `modelGeneVarWithSpikes()` instead of `modelGeneVar()`. Variance modelling results from `modelGeneVar()` tend to understate the actual biological variation, especially in highly heterogeneous datasets where secondary factors of variation inflate the fitted values of the trend. Fewer PCs are subsequently retained because `T` is artificially lowered. We will thus not be using this method, as we couldn't use spike-ins for our variance modelling.
```{r}
set.seed(1991)

# Run denoisePCA for hvg_var_out_no_spikes_filt
dim(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes)
var_model_no_spikes$filter <- rownames(var_model_no_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes)
var_model_no_spikes_filt <- var_model_no_spikes[var_model_no_spikes$filter==TRUE, ]
dim(var_model_no_spikes_filt)

PAG_var_denoised <- denoisePCA(PAG_sceset_qc_norm_filt,
                               technical = var_model_no_spikes_filt, 
                               subset.row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_filt,
                               assay.type = "logcounts")

print(ncol(reducedDim(PAG_var_denoised, "PCA")))

plotReducedDim(PAG_var_denoised,
               dimred = "PCA",
               colour_by = "cell.type",
               shape_by = "PAG.arearegistration",
               by_exprs_values = "logcounts")

# Run denoisePCA for hvg_var_out_no_spikes_block_filt
dim(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes_block)
var_model_no_spikes_block$filter <- rownames(var_model_no_spikes_block) %in% rownames(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes_block)
var_model_no_spikes_block_filt <- var_model_no_spikes_block[var_model_no_spikes_block$filter==TRUE, ]
dim(var_model_no_spikes_block_filt)

PAG_var_block_denoised <- denoisePCA(PAG_sceset_qc_norm_filt,
                                     technical = var_model_no_spikes_block_filt, 
                                     subset.row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block_filt,
                                     assay.type = "logcounts")

print(ncol(reducedDim(PAG_var_block_denoised, "PCA")))

plotReducedDim(PAG_var_block_denoised,
               dimred = "PCA",
               colour_by = "cell.type",
               shape_by = "PAG.arearegistration",
               by_exprs_values = "logcounts")
```

### 5.3.6 | Correlating PCs with experimental and QC variables of interest
`scater` allows us to identify principal components that correlate with experimental and QC variables of interest (it ranks principle components by R^2^ from a linear model regressing PC value against any variable or annotation we have associated with each cell) to see which factor is driving a particular principal component.
```{r}
plotExplanatoryPCs(PAG_sceset_qc_norm_filt,
                   dimred = "PCA_HVG_var_logcounts",
                   n_dimred = 20,
                   nvars_to_plot = 17,
                   npcs_to_plot = 20,                 
                   variables = c("mouse.id",
                                 "mouse.sex",
                                 "mouse.age",
                                 "mouse.singlehousedays",
                                 "cell.type",
                                 "cell.fluorophor",
                                 "slice.number",
                                 "slice.depth",
                                 "PAG.areacollection",
                                 "PAG.hemisphere",
                                 "PAG.APaxis",
                                 "time.sinceslicinghour",
                                 "PAGarea_celltype",
                                 "batch.processing",
                                 "batch.sequencing_round",
                                 "detected",
                                 "total"
                                 )
                   )
```

# STEP 6 | Clustering
## Step 6.1 | Graph-based clustering
### 6.1.2 | Assessing cluster separation
One useful approach is to use the ratio matrix to form another graph where the nodes are clusters rather than cells. Edges between nodes are weighted according to the ratio of observed to expected edge weights between cells in those clusters. We can then repeat our graph operations on this new cluster-level graph. For example, we could obtain clusters of clusters, or we could simply create a new cluster-based layout for visualization. This is analogous to the â€œgraph abstractionâ€ approach described by Wolf et al. 2017.
```{r}
set.seed(1991)
cluster_graph_var_rank <- igraph::graph_from_adjacency_matrix(log2(ratio_var_rank + 1), mode = "upper", weighted = TRUE, diag = FALSE)
# Increasing the weight to increase the visibility of the lines.
plot(cluster_graph_var_rank, edge.width = igraph::E(cluster_graph_var_rank)$weight*5, layout = igraph::layout_with_lgl)

cluster_graph_var_jaccard <- igraph::graph_from_adjacency_matrix(log2(ratio_var_jaccard + 1), mode = "upper", weighted = TRUE, diag = FALSE)
# Increasing the weight to increase the visibility of the lines.
plot(cluster_graph_var_jaccard, edge.width = igraph::E(cluster_graph_var_jaccard)$weight*5, layout = igraph::layout_with_lgl)

cluster_graph_cv2_rank <- igraph::graph_from_adjacency_matrix(log2(ratio_cv2_rank + 1), mode = "upper", weighted = TRUE, diag = FALSE)
plot(cluster_graph_cv2_rank, edge.width = igraph::E(cluster_graph_cv2_rank)$weight*5, layout = igraph::layout_with_lgl)

cluster_graph_cv2_jaccard <- igraph::graph_from_adjacency_matrix(log2(ratio_cv2_jaccard + 1), mode = "upper", weighted = TRUE, diag = FALSE)
plot(cluster_graph_cv2_jaccard, edge.width = igraph::E(cluster_graph_cv2_jaccard)$weight*5, layout = igraph::layout_with_lgl)
```

The graph itself can be visualized using a force-directed layout. This yields a dimensionality reduction result that is closely related to t-SNE and UMAP.
```{r}
set.seed(1991)
reducedDim(PAG_sceset_qc_norm_filt_corr, "force_var") <- igraph::layout_with_fr(graph_var)
plotReducedDim(PAG_sceset_qc_norm_filt_corr, colour_by = "SNN_clusters_var", dimred = "force_var")

reducedDim(PAG_sceset_qc_norm_filt_corr, "force_cv2") <- igraph::layout_with_fr(graph_cv2)
plotReducedDim(PAG_sceset_qc_norm_filt_corr, colour_by = "SNN_clusters_cv2", dimred = "force_cv2")
```

## Step 6.1b | Subclustering or Cell-type specific Graph-based clustering
Another approach to improving resolution is to repeat the feature selection and clustering within a single cluster. This aims to select HVGs and PCs that are more relevant to internal structure, improving resolution by avoiding noise from unnecessary features. Subsetting also encourages clustering methods to separate cells according to more modest heterogeneity in the absence of distinct subpopulations. We can repeat the steps we have done after normalization but subsetting the `SingleCellExperiment` object in excitatory and inhibitory cells:
```{r}
PAG_sceset_qc_norm_filt_corr_VGAT <- PAG_sceset_qc_norm_filt_corr[, PAG_sceset_qc_norm_filt_corr$cell.type=="VGAT"]
PAG_sceset_qc_norm_filt_corr_VGluT2 <- PAG_sceset_qc_norm_filt_corr[, PAG_sceset_qc_norm_filt_corr$cell.type=="VGluT2"]
```

### 6.1b.1 | Implementation of graph-based clustering
```{r}
library(scran)
# Using HVG from Var with rank-based weights
graph_var_rank_VGAT <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGAT,
                                     k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                     #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                     type = "rank", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                     subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_var_out_no_spikes_block_filt,
                                     assay.type = "logcounts",
                                     use.dimred = "PCA_HVG_var_logcounts")
clusters_var_rank_VGAT <- igraph::cluster_walktrap(graph_var_rank_VGAT)$membership
PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_var_rank_VGAT <- factor(clusters_var_rank_VGAT)
table(PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_var_rank_VGAT)

graph_var_rank_VGluT2 <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGluT2,
                                       k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                       #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                       type = "rank", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                       subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_var_out_no_spikes_block_filt,
                                       assay.type = "logcounts",
                                       use.dimred = "PCA_HVG_var_logcounts")
clusters_var_rank_VGluT2 <- igraph::cluster_walktrap(graph_var_rank_VGluT2)$membership
PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_var_rank_VGluT2 <- factor(clusters_var_rank_VGluT2)
table(PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_var_rank_VGluT2)

# Using HVG from Var with jaccard-based weights
graph_var_jaccard_VGAT <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGAT,
                                        k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                        #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                        type = "jaccard", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                        subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_var_out_no_spikes_block_filt,
                                        assay.type = "logcounts",
                                        use.dimred = "PCA_HVG_var_logcounts")
clusters_var_jaccard_VGAT <- igraph::cluster_louvain(graph_var_jaccard_VGAT)$membership
PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_var_jaccard_VGAT <- factor(clusters_var_jaccard_VGAT)
table(PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_var_jaccard_VGAT)

graph_var_jaccard_VGluT2 <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGluT2,
                                          k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                          #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                          type = "jaccard", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                          subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_var_out_no_spikes_block_filt,
                                          assay.type = "logcounts",
                                          use.dimred = "PCA_HVG_var_logcounts")
clusters_var_jaccard_VGluT2 <- igraph::cluster_louvain(graph_var_jaccard_VGluT2)$membership
PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_var_jaccard_VGluT2 <- factor(clusters_var_jaccard_VGluT2)
table(PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_var_jaccard_VGluT2)
 
# Using HVG from CV2 with rank-based weights
graph_cv2_rank_VGAT <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGAT,
                                     k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                     #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                     type = "rank", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                     subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_cv2_out_no_spikes_filt,
                                     assay.type = "logcounts",
                                     use.dimred = "PCA_HVG_cv2_logcounts")
clusters_cv2_rank_VGAT <- igraph::cluster_walktrap(graph_cv2_rank_VGAT)$membership
PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_cv2_rank_VGAT <- factor(clusters_cv2_rank_VGAT)
table(PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_cv2_rank_VGAT)

graph_cv2_rank_VGluT2 <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGluT2,
                                       k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                       #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                       type = "rank", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                       subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_cv2_out_no_spikes_filt,
                                       assay.type = "logcounts",
                                       use.dimred = "PCA_HVG_cv2_logcounts")
clusters_cv2_rank_VGluT2 <- igraph::cluster_walktrap(graph_cv2_rank_VGluT2)$membership
PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_cv2_rank_VGluT2 <- factor(clusters_cv2_rank_VGluT2)
table(PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_cv2_rank_VGluT2)

# Using HVG from CV2 with jaccard-based weights
graph_cv2_jaccard_VGAT <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGAT,
                                        k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                        #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                        type = "jaccard", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                        subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_cv2_out_no_spikes_filt,
                                        assay.type = "logcounts",
                                        use.dimred = "PCA_HVG_cv2_logcounts")
clusters_cv2_jaccard_VGAT <- igraph::cluster_louvain(graph_cv2_jaccard_VGAT)$membership
PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_cv2_jaccard_VGAT <- factor(clusters_cv2_jaccard_VGAT)
table(PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_cv2_jaccard_VGAT)

graph_cv2_jaccard_VGluT2 <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGluT2,
                                          k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                          #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                          type = "jaccard", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                          subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_cv2_out_no_spikes_filt,
                                          assay.type = "logcounts",
                                          use.dimred = "PCA_HVG_cv2_logcounts")
clusters_cv2_jaccard_VGluT2 <- igraph::cluster_louvain(graph_cv2_jaccard_VGluT2)$membership
PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_cv2_jaccard_VGluT2 <- factor(clusters_cv2_jaccard_VGluT2)
table(PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_cv2_jaccard_VGluT2)
```

```{r}
# VGAT cells
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "PAG.arearegistration", shape_by = "VGAT_VGluT2_expression") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from Var with rank-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_var_rank_VGAT", text_by = "SNN_clusters_var_rank_VGAT") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from Var with jaccard-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_var_jaccard_VGAT", text_by = "SNN_clusters_var_jaccard_VGAT") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from CV2 with rank-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_cv2_rank_VGAT", text_by = "SNN_clusters_cv2_rank_VGAT") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from CV2 with jaccard-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_cv2_jaccard_VGAT", text_by = "SNN_clusters_cv2_jaccard_VGAT") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")

# VGluT2 cells
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "PAG.arearegistration", shape_by = "VGAT_VGluT2_expression") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from Var with rank-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_var_rank_VGluT2", text_by = "SNN_clusters_var_rank_VGluT2") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from Var with jaccard-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_var_jaccard_VGluT2", text_by = "SNN_clusters_var_jaccard_VGluT2") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from CV2 with rank-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_cv2_rank_VGluT2", text_by = "SNN_clusters_cv2_rank_VGluT2") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from CV2 with jaccard-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_cv2_jaccard_VGluT2", text_by = "SNN_clusters_cv2_jaccard_VGluT2") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
```

### 6.1b.2 | Assessing cluster separation
```{r}
# VGAT
ratio_var_rank_VGAT <- clusterModularity(graph_var_rank_VGAT, clusters_var_rank_VGAT, as.ratio = TRUE)
ratio_var_rank_VGAT

ratio_var_jaccard_VGAT <- clusterModularity(graph_var_jaccard_VGAT, clusters_var_jaccard_VGAT, as.ratio = TRUE)
ratio_var_jaccard_VGAT

ratio_cv2_rank_VGAT <- clusterModularity(graph_cv2_rank_VGAT, clusters_cv2_rank_VGAT, as.ratio = TRUE)
ratio_cv2_rank_VGAT

ratio_cv2_jaccard_VGAT <- clusterModularity(graph_cv2_jaccard_VGAT, clusters_cv2_jaccard_VGAT, as.ratio = TRUE)
ratio_cv2_jaccard_VGAT

# VGluT2
ratio_var_rank_VGluT2 <- clusterModularity(graph_var_rank_VGluT2, clusters_var_rank_VGluT2, as.ratio = TRUE)
ratio_var_rank_VGluT2

ratio_var_jaccard_VGluT2 <- clusterModularity(graph_var_jaccard_VGluT2, clusters_var_jaccard_VGluT2, as.ratio = TRUE)
ratio_var_jaccard_VGluT2

ratio_cv2_rank_VGluT2 <- clusterModularity(graph_cv2_rank_VGluT2, clusters_cv2_rank_VGluT2, as.ratio = TRUE)
ratio_cv2_rank_VGluT2

ratio_cv2_jaccard_VGluT2 <- clusterModularity(graph_cv2_jaccard_VGluT2, clusters_cv2_jaccard_VGluT2, as.ratio = TRUE)
ratio_cv2_jaccard_VGluT2
```

```{r}
library(pheatmap)
# VGAT
pheatmap(log2(ratio_var_rank_VGAT + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_var_jaccard_VGAT + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_cv2_rank_VGAT + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_cv2_jaccard_VGAT + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

# VGluT2
pheatmap(log2(ratio_var_rank_VGluT2 + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_var_jaccard_VGluT2 + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_cv2_rank_VGluT2 + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_cv2_jaccard_VGluT2 + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))
```

## Step 6.4 | Consensus clustering with SC3
### 6.4.10 | SC3 Clustering using ion channels genes
Here we will run the same clustering approach but using only ion channel genes instead of all the genes. First we load the list of ion channel genes and keep the ones that are present in our dataset:
```{r}
PAG_ionchannels <- PAG_scRNAseq_gene_lists$genes.ionchannels
sum(PAG_scRNAseq_gene_lists$genes.ionchannels != "") # Ion Channel genes present in the list

# Filter any genes not present in our dataset by comparing the gene names with the rownames of our expression matrix (which contains gene names)
PAG_ionchannels <- PAG_ionchannels[(PAG_ionchannels %in% rownames(PAG_sceset_qc))]
length(PAG_ionchannels) # Ion Channel genes present in the dataset
```

Generate a SCE object with only ion channel genes:
```{r}
# Keep only the rows that correspond to ion channel genes:
PAG_sceset_qc_ionchannels <- PAG_sceset_qc[rownames(PAG_sceset_qc) %in% PAG_ionchannels, ]
dim(PAG_sceset_qc_ionchannels)
```

Estimate K:
```{r}
PAG_sceset_qc_ionchannels <- sc3_estimate_k(PAG_sceset_qc_ionchannels)
metadata(PAG_sceset_qc_ionchannels)$sc3$k_estimation
```

Run SC3:
```{r}
rowData(PAG_sceset_qc_ionchannels)$feature_symbol <- rownames(PAG_sceset_qc_ionchannels)
PAG_sceset_qc_ionchannels <- sc3(PAG_sceset_qc_ionchannels, ks = 2:6, biology = TRUE)
```

Explore results interactively:
```{r}
library(shiny)
sc3_interactive(PAG_sceset_qc_ionchannels)
```

For k=2
```{r}
# Consensus Matrix
sc3_plot_consensus(PAG_sceset_qc_ionchannels, 
                   k = 2, 
                   show_pdata = "cell.type"
)

# DE Genes
sc3_plot_de_genes(PAG_sceset_qc_ionchannels, 
                 k = 2,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )

# Marker Genes
sc3_plot_markers(PAG_sceset_qc_ionchannels, 
                 k = 2,
                 auroc = 0.65,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )

```

Figures for k=3
```{r}
# Consensus Matrix
sc3_plot_consensus(PAG_sceset_qc_ionchannels, 
                   k = 3, 
                   show_pdata = "cell.type"
                   )

# DE Genes
sc3_plot_de_genes(PAG_sceset_qc_ionchannels, 
                 k = 3,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )

# Marker Genes
sc3_plot_markers(PAG_sceset_qc_ionchannels, 
                 k = 3,
                 auroc = 0.65,
                 p.val = 0.01,
                 show_pdata = c("cell.type", "PAG.areacollection")
                 )
```

### 6.4.11 | SC3 Clustering using Neuropeptides and Neuromodulators
Here we will run the same clustering approach but using only ion channel genes instead of all the genes. First we load the list of ion channel genes and keep the ones that are present in our dataset:
```{r}
PAG_Neuromodulators_Peptides <- PAG_scRNAseq_gene_lists$genes.NeuromodulatorsPeptides
length(PAG_Neuromodulators_Peptides) # This won't be the length of the $genes.ionchannels column but the one of the longest column of the .csv file (there will be a lot of empty cells).
#typeof(PAG_ionchannels)

# Filter any genes not present in our dataset by comparing the gene names with the rownames of our expression matrix (which contains gene names)
PAG_Neuromodulators_Peptides <- PAG_Neuromodulators_Peptides[(PAG_Neuromodulators_Peptides %in% rownames(PAG_sceset_qc))]
length(PAG_Neuromodulators_Peptides)
```

Generate a SCE object with only ion channel genes:
```{r}
# Keep only the rows that correspond to ion channel genes:
PAG_sceset_qc_Neuromodulators_Peptides <- PAG_sceset_qc[rownames(PAG_sceset_qc) %in% PAG_Neuromodulators_Peptides, ]
dim(PAG_sceset_qc_Neuromodulators_Peptides)
```

Estimate K:
```{r}
PAG_sceset_qc_Neuromodulators_Peptides <- sc3_estimate_k(PAG_sceset_qc_Neuromodulators_Peptides)
metadata(PAG_sceset_qc_Neuromodulators_Peptides)$sc3$k_estimation
```

Run SC3:
```{r}
rowData(PAG_sceset_qc_Neuromodulators_Peptides)$feature_symbol <- rownames(PAG_sceset_qc_Neuromodulators_Peptides)
PAG_sceset_qc_Neuromodulators_Peptides <- sc3(PAG_sceset_qc_Neuromodulators_Peptides, ks = 2:6, biology = TRUE)
```

Explore results interactively:
```{r}
sc3_interactive(PAG_sceset_qc_Neuromodulators_Peptides)
```

```{r}
# Consensus Matrix
sc3_plot_consensus(PAG_sceset_qc_Neuromodulators_Peptides, 
                   k = 2, 
                   show_pdata = "cell.type"
                   )

# DE Genes
sc3_plot_de_genes(PAG_sceset_qc_Neuromodulators_Peptides, 
                 k = 2,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )

# Marker Genes
sc3_plot_markers(PAG_sceset_qc_Neuromodulators_Peptides, 
                 k = 2,
                 auroc = 0.65,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )
```

### 6.4.12 | SC3 Clustering ion channels genes instead of cells (need to transpose matrix in a new SCE object)
Run STEP 5.1.2 first, so you already have the subset of genes and the SCE object. Then transpose the SCE object with only ion channel genes you generated in the previous step:
```{r}
# Keep only the rows that correspond to ion channel genes:
dim(PAG_sceset_qc_ionchannels)
# Trying to transpose a SCEset as follows doesn't work, so we have to transpose the original data prior to creating the SCEset
# PAG_sceset_qc_ionchannels_T <- PAG_sceset_qc_ionchannels[colData(PAG_sceset_qc_ionchannels), rowData(PAG_sceset_qc_ionchannels)]
dim(PAG_sceset_qc_ionchannels_T)
```

```{r}
head(PAG_data)
head(PAG_metadata)
PAG_sceset_qc
PAG_sceset_transposed <- SingleCellExperiment(
  assays = list(counts = as.matrix(PAG_data)), 
  colData = PAG_metadata, 
  rowData = PAG_gene_information
)
```

Estimate K:
```{r}
PAG_sceset_qc_ionchannels <- sc3_estimate_k(PAG_sceset_qc_ionchannels)
metadata(PAG_sceset_qc_ionchannels)$sc3$k_estimation
```

Run SC3:
```{r}
rowData(PAG_sceset_qc_ionchannels)$feature_symbol <- rownames(PAG_sceset_qc_ionchannels)
PAG_sceset_qc_ionchannels <- sc3(PAG_sceset_qc_ionchannels, ks = 2:6, biology = TRUE)
```

Explore results interactively:
```{r}
sc3_interactive(PAG_sceset_qc_ionchannels)
```

### 6.4.13 | SC3 Clustering VGAT and VGluT2 cells separately
Do this when you are ready for downstream analysis, after quality checks.  
```{r}
# Is this the best way to subset the data? Probably not, there must be a way to do it without creating a new SCE object.
VGAT_sceset <- PAG_sceset[,grep("^[I]", colnames(PAG_sceset), value=TRUE)]

VGluT2_sceset <- PAG_sceset[,grep("^[E]", colnames(PAG_sceset), value=TRUE)]
```

## Step 6.7 | clusterExperiment [not yet implemented]
## Step 6.8 | bigSCale2
bigSCale is an analytical framework for big-scale single-cell data, available [here](https://github.com/iaconogi/bigSCale2).

```{r}
library(bigscale)
PAG_sceset_qc <- bigscale(PAG_sceset_qc)
```

OR, if you want to run it step by step:
```{r}
# Pre-process the data and generate the model:
PAG_sceset_qc = preProcess(PAG_sceset_qc)
PAG_sceset_qc = storeNormalized(PAG_sceset_qc) # stores normalized data
PAG_sceset_qc = setModel(PAG_sceset_qc) # computes the numerical model
PAG_sceset_qc = storeTransformed(PAG_sceset_qc) # stores transformed expression data (needed for some plots)
viewModel(PAG_sceset_qc) # check the model

# Compute the highly variable genes and we calculate cell to cell distances. 
PAG_sceset_qc = setODgenes(PAG_sceset_qc, min_ODscore=2.33) # min_ODscore is the Z-score treshold for selecting the genes (default 2.33). Increase it to be more stringent (less genes) and viceversa.
viewODgenes(PAG_sceset_qc) # visually inspect the selected highly variable genes

# Compute the distances and the t-SNE:
PAG_sceset_qc = setDistances(PAG_sceset_qc)
PAG_sceset_qc = storeTsne(PAG_sceset_qc)

# Cluster the data
PAG_sceset_qc = setClusters(PAG_sceset_qc)

# Make pseudotime analysis:
PAG_sceset_qc = storePseudo(PAG_sceset_qc)

# Compute the differential expression for all genes. Select speed.preset='slow' for maximum accuracy with long computational time.
PAG_sceset_qc = computeMarkers(PAG_sceset_qc, speed.preset='slow')

# Organize all the genes into markers and signatures of co-expression. cutoff is a Z-score which filters the genes and retains only those with significant changes of expression. Inrease it if you want to be more stringent or viceversa.
PAG_sceset_qc = setMarkers(PAG_sceset_qc, cutoff=3)

# Restore some matrices from the virtual memory to complete the analysis.
PAG_sceset_qc = restoreData(PAG_sceset_qc)
```

Plot of the clusters and signatures of coexpressed genes and visualise:

* The dendrogram representing how the cells are phenotypically organized and clustered.
* Colored bars representing the clusters, the library size (meant as a proxy to transcriptome size/complexity) and the pseudotime of the cells. An additional color bar is displayed for any user custom `colData()` (for example, sample batches, conditions and so on ...). For custom user `colData`, the color codes are automatically chosen upoen the type of data (numeric or factor).
* The clustered signatures of coexpressed genes alogside their size. Here, all the genes differentially expressed are organized in signatures of co-expressed genes.
```{r}
viewSignatures(PAG_sceset_qc)
```

We can also inspect the markers of a specific cluster, with markers of level 1 being the most specific to a given cluster.
```{r}
viewSignatures(PAG_sceset_qc, selected.cluster=2)
```

Barplots and violin plots of selected genes to visualize gene expression at single cell level with colored clusters:
```{r}
viewGeneBarPlot(PAG_sceset_qc, gene.list = c('Aqp4','Olig1','Thy1'))
```

```{r}
viewGeneViolin(PAG_sceset_qc, 'Asic4')
```

t-SNE and UMAP plots:

* If you want to color the cell according to some custom annotation you can pass a `factor` variable in place of a gene name. If you want to visualize a UMAP plot first compute the UMAP data with `sce=storeUMAP(sce)` and then `viewReduced(sce,method = 'UMAP')`
```{r}
viewReduced(PAG_sceset_qc) # to see t-SNE with clusters
viewReduced(PAG_sceset_qc, color.by = 'Stmn2') # to see t-SNE with gene expression
```

Browsing markers:
* To have a look to the markers found by bigscale we retrive `Mlist` from the single cell object. `Mlist` is a 2 dimensional list containing for each cluster the markers of the different levels. Let's inspect the markers of level 1 (most specific) of cluster 4. We will take advantage of the package `DT` for interactive visualization. Running the next command line we will see the markers specific to cluster 4 sorted from the highest (most significant) to the lowest (less significant) Zscore.
```{r}
Mlist = getMarkers(PAG_sceset_qc)
DT::datatable(Mlist[[4,1]])
```

Browsing signatures:
* Alternatevely to a cluster-based organization of the markers (as shown before, we have clusters and we have levels) there is also a more compact organization of markers into lists of co-expressed genes. This are the same lists shown with `viewSignatures(sce)`.
```{r}
Signatures = getSignatures(PAG_sceset_qc)
DT::datatable(Signatures[[1]])
```

## Step 6.9 | BackSPIN - biclustering hierarchical [MATLAB/Python]
The BackSPIN biclustering algorithm was developed by Amit Zeisel and is described in Zeisel et al. Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq Science 2015 (PMID: 25700174, doi: 10.1126/science.aaa1934). Please cite this paper if you use the BackSPIN algorithm in your work.

Original MATLAB implementation by Amit Zeisel. This repo contains a standalone command-line version of BackSPIN, implemented in Python by Gioele La Manno (https://github.com/linnarsson-lab/BackSPIN)

## Step 6.10 | SINCERA (hierarchical clustering) [not yet implemented]
It performs a gene-level z-score transformation before doing clustering.
It can also identify k as the minimum height of the hierarchical tree that generates no more than a specified number of singleton clusters (clusters containing only 1 cell).

SINCERA: a computational pipeline for SINgle CEll RNA-seq profiling Analysis, can be used for processing scRNA-seq data from a whole organ or sorted cells. The pipeline supports the analysis for: 1) the distinction and identification of major cell types; 2) the identification of cell type specific gene signatures; and 3) the determination of driving forces of given cell types.

Website: https://research.cchmc.org/pbge/sincera.html
Paper: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004575

## Step 6.11 | pcaReduce [not yet implemented, but very stochastic and does not provide a stable result] 
pcaReduce combines PCA, k-means and iterative hierarchical clustering. Starting from a large number of clusters pcaReduce iteratively merges similar clusters; after each merging event it removes the principle component explaning the least variance in the data.

pcaReduce operates directly on the expression matrix. It is recommended to use a gene filter and log transformation before running pcaReduce. We will use the default SC3 gene filter (note that the exprs slot of a scater object is log-transformed by default).
```{r}
# use the same gene filter as in SC3
library(pcaReduce) # You need to install PCAReduce first
input <- logcounts(PAG_sceset_qc[rowData(PAG_sceset_qc)$sc3_gene_filter, ])
```

There are several parameters used by pcaReduce: nbt defines a number of pcaReduce runs (it is stochastic and may have different solutions after different runs). q defines number of dimensions to start clustering with. The output will contain partitions for all k from 2 to q+1. method defines a method used for clustering. S - to perform sampling based merging, M - to perform merging based on largest probability.
```{r}
# run pcaReduce 1 time creating hierarchies from 1 to 30 clusters
pca.red <- PCAreduce(t(input), nbt = 1, q = 30, method = 'S')[[1]]
colData(PAG_sceset_qc)$pcaReduce <- as.character(pca.red[,32 - 10])
plotPCA(PAG_sceset_qc, colour_by = "pcaReduce")
```

# STEP 7 | Differential Expression Analysis
## Step 7.1 | DESeq2
Initial designs
```{r}
library(DESeq2)
#### Design 0 ####
PAG_DESeq_set_0 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                          colData = colData(PAG_sceset_qc_norm_filt_corr),
                                          design = ~ mouse.sex + batch.processing + batch.sequencing_round)

#### Design 1 ####
PAG_DESeq_set_1 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                          colData = colData(PAG_sceset_qc_norm_filt_corr),
                                          design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type)

#### Design 2 ####
PAG_DESeq_set_2 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"),
                                          colData = colData(PAG_sceset_qc_norm_filt_corr),
                                          design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.arearegistration)

#### Design 3 ####
PAG_DESeq_set_3 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                          colData = colData(PAG_sceset_qc_norm_filt_corr),
                                          design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.APaxis)

#### Design 1&2 ####
PAG_DESeq_set_12 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                           colData = colData(PAG_sceset_qc_norm_filt_corr),
                                           design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.arearegistration)

#### Design 1&2 with interaction ####
PAG_DESeq_set_12a <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                            colData = colData(PAG_sceset_qc_norm_filt_corr),
                                            design = ~ mouse.sex + batch.processing + batch.sequencing_round + group)

#### Design 1&3 ####
PAG_DESeq_set_13 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                           colData = colData(PAG_sceset_qc_norm_filt_corr),
                                           design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.APaxis)

#### Design 1&3 with interaction ####
PAG_DESeq_set_13b <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                            colData = colData(PAG_sceset_qc_norm_filt_corr),
                                            design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.APaxis + cell.type:PAG.APaxis)

#### Design 2&3 ####
PAG_DESeq_set_23 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                           colData = colData(PAG_sceset_qc_norm_filt_corr),
                                           design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.arearegistration + PAG.APaxis)

#### Design 2&3 with interaction #### --> Can't be done as PAG.APaxis:PAG.arearegistrations seems to be a linear combination of the others
#PAG_DESeq_set_23c <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                            #colData = colData(PAG_sceset_qc_norm_filt_corr),
                                            #design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.arearegistration + PAG.APaxis + PAG.APaxis:PAG.arearegistration)

#### Design 1&2&3 ####
PAG_DESeq_set_123 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                            colData = colData(PAG_sceset_qc_norm_filt_corr),
                                            design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.arearegistration + PAG.APaxis)

#### Design 1&2&3 with interaction ####
PAG_DESeq_set_123ab <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
                                               colData = colData(PAG_sceset_qc_norm_filt_corr),
                                               design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.arearegistration + PAG.APaxis + cell.type:PAG.arearegistration + cell.type:PAG.APaxis)
```

## Step 7.5 | Non-parametric tests
The main argument against using non-parametric tests is that scaling normalization does not adjust for differences in the distributions, which results in incorrect rejection of the null hypothesis in fairly innocuous cases (see Aaron Lun's github: https://github.com/LTLA/SingleCellThoughts/blob/master/workflows/de.Rmd). Also, from _Soneson et al 2018_, the main drawback is that these methods don't allow for complex designs.

### 7.5.1 | Kolmogorov-Smirnov test (non-parametric)
To compare the distributions for each gene in two individuals/groups. The KS-test quantifies the distance between the empirical cummulative distributions of the expression of each gene in each of the two populations. It is sensitive to changes in mean expression and changes in variability. However it assumes data are continuous and may perform poorly when data contains a large number of identical values (eg. zeros). Another issue with the KS-test is that it can be very sensitive for large sample sizes and thus it may end up as significant even though the magnitude of the difference is very small.
```{r}
pValues_KS <- apply(
    norm, 1, function(x) {
        ks.test(
            x[PAG_sceset_qc$cell.type == "VGluT2"], 
            x[PAG_sceset_qc$cell.type == "VGAT"]
        )$p.value
    }
)
# multiple testing correction
pValues_KS <- p.adjust(pValues_KS, method = "fdr")
```

```{r}
# How many of the significant DE genes are detected
significant_DE_genes_KS <- names(pValues_KS)[pValues_KS < 0.05]
length(significant_DE_genes_KS)
```

Often it is informative to vary the threshold and evaluate performance across a range of values. This is then plotted as a receiver-operating-characteristic curve (ROC) and a general accuracy statistic can be calculated as the area under this curve (AUC). The ROCR package facilitates this plotting.

### 7.5.2 | Wilcox/Mann-Whitney-U Test (non-parametric)
The Wilcox-rank-sum test is another non-parametric test, but tests specifically if values in one group are greater/less than the values in the other group. Thus it is often considered a test for difference in median expression between two groups; whereas the KS-test is sensitive to any change in distribution of expression values.
```{r}
pValues_W <- apply(
    norm, 1, function(x) {
        wilcox.test(
            x[PAG_sceset_qc$cell.type == "VGluT2"], 
            x[PAG_sceset_qc$cell.type == "VGAT"]
        )$p.value
    }
)
# multiple testing correction
pValues_W <- p.adjust(pValues_W, method = "fdr")
```

```{r}
#How many of the significant DE genes are detected
significant_DE_genes_W <- names(pValues_W)[pValues_W < 0.05]
length(significant_DE_genes_W)
```
## Step 7.6 | SCDE [not yet implemented]
The `scde` package implements routines for fitting individual error models for single-cell RNA-seq measurements. Briefly, the read counts observed for each gene are modeled using a mixture of a negative binomial (NB) distribution (for the amplified/detected transcripts) and low-level Poisson distribution (for the unobserved or background-level signal of genes that failed to amplify or were not detected for other reasons). These models can then be used to identify robustly differentially expressed genes between groups of cells.

Website: http://hms-dbmi.github.io/scde/

### 7.6.1 | Preparing the data
The analysis starts with a matrix of read counts (the values must be integers). We first need to define the two groups we are going to compare:
```{r}
library(scde)
# load example dataset
data(es.mef.small)

# factor determining cell types
sg <- factor(gsub("(MEF|ESC).*", "\\1", colnames(es.mef.small)), levels = c("ESC", "MEF"))
# the group factor should be named accordingly
names(sg) <- colnames(es.mef.small)  
table(sg)

# clean up the dataset
cd <- clean.counts(es.mef.small, min.lib.size=1000, min.reads = 1, min.detected = 1)
```

### 7.6.2 | Fitting error models
As a next step we fit the error models on which all subsequent calculations will rely. The fitting process relies on a subset of robust genes that are detected in multiple cross-cell comparisons. Here we supply the groups = sg argument, so that the error models for the two cell types are fit independently (using two different sets of "robust" genes). If the groups argument is omitted, the models will be fit using a common set.

Note this step takes a considerable amount of time unless multiple cores are used.
```{r}

```

## Step 7.7 | BPSC [not yet implemented]
BPSC uses the Poisson-Beta model of single-cell gene expression, which we discussed in the previous chapter, and combines it with generalized linear models which weâ€™ve already encountered when using edgeR. BPSC performs comparisons of one or more groups to a reference group (â€œcontrolâ€) and can include other factors such as batches in the model.

# STEP 10 | Seurat
[Seurat](https://satijalab.org/seurat/) is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. For more info see Butler et al. 2018 and Stuart, Butler et al. 2018. The following code has been adapted from the [Guided Clustering Tutorial](https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html).

_IMP:_ Before starting this, run "Part I: from a gene expression matrix to a SingleCellExperiment object" again to load the raw data. Seurat uses its own object class, so we need to create a new object with the raw (non-normalised) data. Setting up a Seurat object:
```{r}
library(Seurat)
library(dplyr)
PAG_seurat <- CreateSeuratObject(counts = PAG_data,
                                 project = "PAG_seurat_analysis",
                                 min.cells = 1, # keep genes expressed in more than 1 cell
                                 min.features = 2000, # keep cells with at least 2000 detected genes
                                 meta.data = PAG_metadata)
```

## Step 10.1 | Pre-processing, data QC and normalization
Selection and filtration of cells based on QC metrics, data normalization and scaling, and detection of highly variable features.
```{r}
# The [[ operator can add columns to object metadata. This is a great place to stash QC stats
PAG_seurat[["percent.mt"]] <- PercentageFeatureSet(PAG_seurat, pattern = "^MT-")
PAG_seurat[["percent.ercc"]] <- PercentageFeatureSet(PAG_seurat, pattern = "^ERCC-")
head(PAG_seurat@meta.data, 5) # Metadata for the first 5 cells
```

Visualize QC metrics using a violin plot:
```{r}
VlnPlot(PAG_seurat, features = c("nFeature_RNA", "nCount_RNA", "percent.mt", "percent.ercc"), ncol = 4)
```

```{r}
# FeatureScatter is typically used to visualize feature-feature relationships, but can be used
# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.

plot1 <- FeatureScatter(PAG_seurat, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(PAG_seurat, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
CombinePlots(plots = list(plot1, plot2))
```

Filter the dataset according to specific quality  metrics:
```{r}
PAG_seurat <- subset(PAG_seurat, subset = nFeature_RNA > 2000 & percent.mt < 5)
```

After removing unwanted cells from the dataset, the next step is to normalize the data. By default, Seurat employs a global-scaling normalization method `LogNormalize` that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. Normalized values are stored in `SeuratObject[["RNA"]]@data`.
```{r}
PAG_seurat <- NormalizeData(PAG_seurat, normalization.method = "LogNormalize", scale.factor = 10000)
PAG_seurat <- NormalizeData(PAG_seurat)
```

## Step 10.2 | Feature selection: identify highly variable features
The next step consists on calculating a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). Focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets. Seurat directly models the mean-variance relationship inherent in single-cell data and implements in in the `FindVariableFeatures` function, returning 2000 features per dataset by default.
```{r}
PAG_seurat <- FindVariableFeatures(PAG_seurat, selection.method = "vst", nfeatures = 2000)

# Identify the 10 most highly variable genes
top10 <- head(VariableFeatures(PAG_seurat), 10)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(PAG_seurat)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
CombinePlots(plots = list(plot1, plot2))
```

## Step 10.3 | Scaling the data, peforming dimensionality reduction and determining the dimensionality of the dataset
Linear transformation (scaling) is a standard pre-processing step prior to dimensionality reduction techniques like PCA. The `ScaleData` function:

* Shifts the expression of each gene, so that the mean expression across cells is 0
* Scales the expression of each gene, so that the variance across cells is and highly-expressed genes do not dominate
* Stores the resuts in `SeuratObject[["RNA"]]@scale.data`
```{r}
all.genes <- rownames(PAG_seurat)
PAG_seurat <- ScaleData(PAG_seurat, features = all.genes)

# Scaling is an essential step in the Seurat workflow, but only on genes that will be used as input to PCA. To speed things up, the default in  ScaleData is only to perform scaling on the previously identified variable features (2,000 by default). To do this, omit the  features argument in the previous function call.
```

We can now perform PCA on the scaled data. By default, only the previously determined variable features are used as input, but can be defined using `features` argument if you wish to choose a different subset.
```{r}
PAG_seurat <- RunPCA(PAG_seurat, features = VariableFeatures(object = PAG_seurat))
```

Seurat provides several useful ways of visualizing both cells and features that define the PCA, including `VizDimReduction`, `DimPlot`, and `DimHeatmap`.
```{r}
# Examine and visualize PCA results a few different ways
print(PAG_seurat[["pca"]], dims = 1:5, nfeatures = 5)
VizDimLoadings(PAG_seurat, dims = 1:2, reduction = "pca")
DimPlot(PAG_seurat, reduction = "pca")
```

`DimHeatmap` allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. Both cells and features are ordered according to their PCA scores. Setting cells to a number plots the extreme cells on both ends of the spectrum, which dramatically speeds plotting for large datasets. Though clearly a supervised analysis, this is a valuable tool for exploring correlated feature sets.
```{r}
DimHeatmap(PAG_seurat, dims = 1, cells = 500, balanced = TRUE)
DimHeatmap(PAG_seurat, dims = 1:15, cells = 500, balanced = TRUE)
```

To overcome the extensive technical noise in any single feature for scRNA-seq data, Seurat clusters cells based on their PCA scores, with each PC essentially representing a metafeature that combines information across a correlated feature set. The top principal components therefore represent a robust compression of the dataset. However, how many componenets should we choose to include? 10? 20? 100?

In Macosko et al, they implemented a resampling test inspired by the JackStraw procedure. They randomly permuted a subset of the data (1% by default) and rerun PCA, constructing a null distribution of feature scores, and repeat this procedure. They then identify significant PCs as those with a strong enrichment of low p-value features.
```{r}
# NOTE: This process can take a long time for big datasets.
# Other techniques such as those implemented in ElbowPlot() can be used to reduce computation time
PAG_seurat <- JackStraw(PAG_seurat, num.replicate = 100)
PAG_seurat <- ScoreJackStraw(PAG_seurat, dims = 1:20)
```

The `JackStrawPlot` function provides a visualization tool for comparing the distribution of p-values for each PC with a uniform distribution (dashed line). Significant PCs will show a strong enrichment of features with low p-values (solid curve above the dashed line).
```{r}
JackStrawPlot(PAG_seurat, dims = 1:15)
```

An alternative heuristic method generates an `Elbow plot`: a ranking of principle components based on the percentage of variance explained by each one (`ElbowPlot` function). The elbow suggests a point by which the majority of true signal is captured.
```{r}
ElbowPlot(PAG_seurat)
```

Identifying the true dimensionality of a dataset can be challenging/uncertain for the user. It is therefore a good idea to try the three different methods described above. The first is more supervised, exploring PCs to determine relevant sources of heterogeneity, and could be used in conjunction with GSEA for example. The second implements a statistical test based on a random null model, but is time-consuming for large datasets, and may not return a clear PC cutoff. The third is a heuristic that is commonly used, and can be calculated instantly. It is also good practice to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). Although the results often do not differ dramatically, it is best to err on the higher side when choosing this parameter.

## Step 10.4 | Clustering
Seurat v3 applies a graph-based clustering approach. It first constructs a KNN graph based on the euclidean distance in PCA space, and refine the edge weights between any two cells based on the shared overlap in their local neighborhoods (Jaccard similarity). This step is performed using the `FindNeighbors` function, and takes as input the previously defined dimensionality of the dataset.

To cluster the cells, Seurat next applies modularity optimization techniques such as the Louvain algorithm (default) or SLM [SLM, Blondel et al., Journal of Statistical Mechanics], to iteratively group cells together, with the goal of optimizing the standard modularity function. The `FindClusters` function implements this procedure, and contains a resolution parameter that sets the "granularity" of the downstream clustering, with increased values leading to a greater number of clusters. We find that setting this parameter between 0.4-1.2 typically returns good results for single-cell datasets of around 3K cells. Optimal resolution often increases for larger datasets. The clusters can be found using the `Idents` function.
```{r}
PAG_seurat <- FindNeighbors(PAG_seurat, dims = 1:10)
PAG_seurat <- FindClusters(PAG_seurat, resolution = 0.5)
head(Idents(PAG_seurat), 5) # Look at cluster IDs of the first 5 cells
```

## Step 10.5 | Non-linear dimensionality reduction (UMAP and t-SNE)
Seurat offers several non-linear dimensional reduction techniques, such as tSNE and UMAP, to visualize and explore these datasets. The goal of these algorithms is to learn the underlying manifold of the data in order to place similar cells together in a low-dimensional space. Cells within the graph-based clusters determined above should co-localize on these dimension reduction plots. Ideally use the same PCs input to the clustering analysis as input to the UMAP and tSNE.
```{r}
PAG_seurat <- RunTSNE(PAG_seurat, dims = 1:10)
DimPlot(PAG_seurat, reduction = "tsne")
```

You can save the object at this point so that it can easily be loaded back in without having to rerun the computationally intensive steps performed above, or easily shared with collaborators.
```{r}
saveRDS(PAG_seurat, file = "../output/PAG_seurat_analysis.rds")
```

## Step 10.6 | Finding differentially expressed features (cluster biomarkers)
Seurat can help find markers that define clusters via differential expression. By default, it identifes positive and negative markers of a single cluster (specified in ident.1), compared to all other cells. `FindAllMarkers` automates this process for all clusters, but you can also test groups of clusters vs. each other, or against all cells.

The `min.pct` argument requires a feature to be detected at a minimum percentage in either of the two groups of cells, and the `thresh.test` argument requires a feature to be differentially expressed (on average) by some amount between the two groups. You can set both of these to 0, but with a dramatic increase in time - since this will test a large number of features that are unlikely to be highly discriminatory. As another option to speed up these computations, `max.cells.per.ident` can be set. This will downsample each identity class to have no more cells than whatever this is set to. While there is generally going to be a loss in power, the speed increases can be significiant and the most highly differentially expressed features will likely still rise to the top.
```{r}
# find all markers of cluster 1
cluster1.markers <- FindMarkers(PAG_seurat, ident.1 = 1, min.pct = 0.25)
head(cluster1.markers, n = 5)
```

```{r}
# find all markers distinguishing cluster 3 from clusters 0 and 2
cluster3.markers <- FindMarkers(PAG_seurat, ident.1 = 3, ident.2 = c(0, 1, 2), min.pct = 0.25)
head(cluster3.markers, n = 10)
```

```{r}
# find markers for every cluster compared to all remaining cells, report only the positive ones
PAG_seurat.markers <- FindAllMarkers(PAG_seurat, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)
PAG_seurat.markers %>% group_by(cluster) %>% top_n(n = 2, wt = avg_logFC)
```

Seurat has several tests for differential expression which can be set with the `test.use` parameter. For example, the ROC test returns the "classification power" for any individual marker (ranging from 0 - random, to 1 - perfect).
```{r}
cluster1.markers <- FindMarkers(PAG_seurat, ident.1 = 0, logfc.threshold = 0.25, test.use = "roc", only.pos = TRUE)
```

There are several tools for visualizing marker expression. `VlnPlot` (shows expression probability distributions across clusters), and `FeaturePlot` (visualizes feature expression on a tSNE or PCA plot) are the most commonly used visualizations, but additional methods are `RidgePlot`, `CellScatter`, and `DotPlot`.
```{r}
VlnPlot(PAG_seurat, features = c("Sst", "Npy"))

# you can plot raw counts as well
VlnPlot(PAG_seurat, features = c("Sst", "Npy"), slot = "counts", log = TRUE)

FeaturePlot(PAG_seurat, features = c("MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP", 
    "CD8A"))
```

`DoHeatmap` generates an expression heatmap for given cells and features.
```{r}
top10 <- PAG_seurat.markers %>% group_by(cluster) %>% top_n(n = 10, wt = avg_logFC)
DoHeatmap(PAG_seurat, features = top10$gene) + NoLegend()
```

## Step 10.7 | Assigning cell type identity to clusters
Finally, we can rename/assign an identity to the clusters we have been able to identify based on the data.
```{r}
new.cluster.ids <- c("Naive CD4 T", "Memory CD4 T", "CD14+ Mono", "B", 
                     "CD8 T", "FCGR3A+ Mono", "NK", "DC", "Platelet")
names(new.cluster.ids) <- levels(PAG_seurat)
PAG_seurat <- RenameIdents(PAG_seurat, new.cluster.ids)
DimPlot(PAG_seurat, reduction = "umap", label = TRUE, pt.size = 0.5) + NoLegend()
```

Save the complete analysis:
```{r}
saveRDS(PAG_seurat, file = "../output/PAG_seurat_analysis_final.rds")
```