---
title: "Topographic, single-cell gene expression profiling of Periaqueductal Gray neurons"
subtitle: "Code scraps"
author:
  - name: "Oriol Pavon Arocas, Sarah F. Olesen, and Tiago Branco"
    affiliation: "Sainsbury Wellcome Centre for Neural Circuits and Behaviour, University College London, UK"
    email: "oriol.pavon.16@ucl.ac.uk"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_notebook:
    highlight: pygments
    number_sections: FALSE
    theme: lumen
    toc: TRUE
    toc_float: TRUE

#output: rmdformats::readthedown:
  #highlight: pygments
---

***

This is a pipeline to analyse single-cell RNA sequencing data from Periaqueductal Gray neurons (1) isolated from acute midbrain slices of transgenic mice using visually guided aspiration via patch pipettes and (2) processed using SMART-seq2 (Picelli et al., Nature Protocols 2014).

This pipeline has been generated after attending the [EMBL-EBI RNA-Sequence Analysis Course](https://www.ebi.ac.uk/training/events/2019/rna-sequence-analysis) and [attending](https://training.csx.cam.ac.uk/bioinformatics/event/2823386) and following the online course on [Analysis of single cell RNA-seq data](https://github.com/hemberg-lab/scRNA.seq.course) by the [Hemberg Lab](https://www.sanger.ac.uk/science/groups/hemberg-group). Many other resources have been used, including the [Orchestrating Single-Cell Analysis with Bioconductor book](https://bioconductor.org/books/release/OSCA/) by Robert Amezquita, Aaron Lun, Stephanie Hicks, and Raphael Gottardo, the [simpleSingleCell workflow in Bioconductor](https://bioconductor.org/packages/3.9/workflows/html/simpleSingleCell.html) maintained by Aaron Lun, the [rnaseqGene workflow](https://bioconductor.org/packages/3.10/workflows/html/rnaseqGene.html) maintained by Michael Love, the [RNAseq123 workflow](https://bioconductor.org/packages/3.10/workflows/html/RNAseq123.html) maintained by Matthew Ritchie, and the [EGSEA123 workflow](https://bioconductor.org/packages/3.10/workflows/html/EGSEA123.html) maintained by Matthew Ritchie.

Other key resources include [Bioconductor](http://www.bioconductor.org/) (Huber et al., Nature Methods 2015), [scRNA-tools](https://www.scrna-tools.org/), `scater` (McCarthy et al., Bioinformatics 2017), `scran` (Lun et al., F1000Res 2016), `SC3` (Kiselev et al., Nature Methods 2017), `Seurat` (Butler et al., Nature Biotechnology 2018), `clusterExperiment` (Risso et al., PLOS Computational Biology 2018), `limma` (Ritchie et al., Nucleic Acids Research 2015), `DESeq2` (Love et al., Genome Biology 2014), `edgeR` (Robinson et al., Bioinformatics 2010), `MAST` (Finak, McDavid, Yajima et al., Genome Biology 2015), `iSEE` (Rue-Albrecht & Marini et al., F1000Research 2018), `t-SNE` (van der Maaten & Hinton, Journal of Machine Learning Research 2008), `UMAP` (McInnes et al., arXiv 2018), and the [Mathematical Statistics and Machine Learning for Life Sciences](https://towardsdatascience.com/tagged/stats-ml-life-sciences) column by Nikolay Oskolkov.

***

Below are some code scraps that have been taken out of the pipeline in favour of other preferred alternatives. They most likely require further adaptations before being reincorporated.

# STEP 1 | Read Data and Metadata
## Step 1.5 | Add gene-based annotations
We can add gene-based annotation by pulling the annotation from `org.Mm.eg.db` to relate the ENSEMBL identifiers to gene symbols. The `mapIds()` function ensures that only one gene symbol is returned if two symbols map to the same ENSEMBL ID.
```{r}
library(org.Mm.eg.db)
gene_symbols <- mapIds(org.Mm.eg.db, keys = rownames(PAG_data), 
                       keytype ="ENSEMBL", column = "SYMBOL")
PAG_gene_annotations <- data.frame(ENSEMBL = rownames(PAG_data), 
                                   gene_symbols = gene_symbols, 
                                   stringsAsFactors = FALSE)
head(PAG_gene_annotations)
```

We can also identify which rows correspond to mitochondrial genes. To do that we need to use extra annotation describing the genomic location of each gene. For ENSEMBL, this involves using the `TxDb.Mmusculus.UCSC.mm10.ensGene` package.
```{r}
library(TxDb.Mmusculus.UCSC.mm10.ensGene)
gene_location <- mapIds(TxDb.Mmusculus.UCSC.mm10.ensGene, keys = rownames(PAG_data), 
                        keytype = "GENEID", column = "CDSCHROM")
PAG_gene_annotations$Chromosome <- gene_location
which(gene_location=="chrM")
table(PAG_gene_annotations$Chromosome)
```

# STEP 2 | Pre-processing and data Quality Control
## Step 2.3 | Cell QC
### 2.3.5 | Xist expression
A final quality check could be to look at the expression of _Xist_, which should only appear in cells obtained from female mice, as it only expresses in the inactive X chromosome:
```{r}
# Log transform the counts for Xist
PAG_sceset$log2_total_counts_Xist <- log2(PAG_sceset$subsets_XIST_sum + 1)
summary(PAG_sceset$log2_total_counts_Xist)

# Plot Expression values of VGAT and VGluT2 in several ways:
plotExpression(PAG_sceset, "Xist", x = "mouse.id", 
               exprs_values = "logcounts_raw", # alternatively try "counts"
               colour_by = "mouse.sex",
               xlab = "Mouse ID")

plotExpression(PAG_sceset, "Xist", x = "mouse.sex", 
               exprs_values = "logcounts_raw", # alternatively try "counts"
               colour_by = "cell.type",
               xlab = "Mouse sex")

plotExpression(PAG_sceset[, PAG_sceset$mouse.sex == "M"], "Xist", x = "mouse.id", 
               exprs_values = "logcounts_raw", # alternatively try "counts"
               colour_by = "batch.processing",
               xlab = "Mouse ID")

plotExpression(PAG_sceset[, PAG_sceset$mouse.sex == "F"], "Xist", x = "mouse.id", 
               exprs_values = "logcounts_raw", # alternatively try "counts"
               colour_by = "batch.processing",
               xlab = "Mouse ID")

plotColData(PAG_sceset,
            x = "detected",
            y = "subsets_Chromosome_X_sum",
            colour_by = "mouse.id",
            other_fields = "mouse.sex") +
  facet_wrap(~mouse.sex) + 
  xlab("Number of detected genes") +
  ylab("X Chromosome counts")

plotColData(PAG_sceset,
            x = "detected",
            y = "subsets_Chromosome_Y_sum",
            colour_by = "mouse.id",
            other_fields = "mouse.sex") +
  facet_wrap(~mouse.sex) + 
  xlab("Number of detected genes") +
  ylab("Y Chromosome counts")
```

## Step 2.5 | Gene QC
### 2.5.3 | Identities of the most highly expressed genes
It can also be useful to plot gene expression frequency versus mean expression level to assess the effects of technical dropout in the dataset. The `plotExprsFreqVsMean()` function fits a non-linear least squares curve for the relationship between expression frequency and mean expression and uses this to define the number of genes above high technical dropout and the numbers of genes that are expressed (here defined as at least 4 counts) in at least 50% and at least 25% of cells. A subset of genes to be treated as feature controls can be specified, otherwise any feature controls previously defined are used. 

Plot frequency of expression (number of cells with non-zero expression) against the mean. These two metrics should be positively correlated with each other for most genes. Outliers from the trend may warrant further investigation. For example, alignment errors for pseudo-genes of highly-expressed genes will result in features with low means that are expressed in all cells. Conversely, PCR amplification biases (or the presence of rare populations) may result in genes with very high means that are expressed in very few cells. 

*Check out the "Scater Case Study" by McCarthy et al. 2016 for more information.*
```{r}
plotExprsFreqVsMean(PAG_sceset,
                    control = "is_spike") 
plotExprsFreqVsMean(PAG_sceset[ , colData(PAG_sceset)$use],
                    control = "is_spike")
```

## Step 2.5b | Cell cycle phase [OPTIONAL]
It is possible to classify cells into cell cycle phases based on the gene expression data using the function `cyclone` (Sciealdone et al. 2015). This, however, is not super useful in the brain, as many neuronal cell types are expected to lie in the G0 resting phase, which is distinct from the other phases of the cell cycle (Coller et al., 2006). We run it anyway and observe that all the cells are in G1.

Cells are classified as being in G1 phase if the G1 score is above 0.5 and greater than the G2/M score; in G2/M phase if the G2/M score is above 0.5 and greater than the G1 score; and in S phase if neither score is above 0.5.

*See McCarthy et al. F1000Research 2016 for an explanation of this step and the code.*
```{r}
library(scran)
library(AnnotationDbi)
library(org.Mm.eg.db)
set.seed(1991)

mm.pairs <- readRDS(system.file("exdata", "mouse_cycle_markers.rds", package="scran")) 
assignments <- cyclone(PAG_sceset, mm.pairs, gene.names=rowData(PAG_sceset)$ENSEMBL) 
plot(assignments$score$G1, assignments$score$G2M, xlab="G1 score", ylab="G2/M score", pch=16, xlim=c(0,1), ylim=c(0,1))
```

We could store the cell cycle phases for future use:
```{r}
PAG_sceset$cycle_phase <- assignments$phases
```

# STEP 3 | Normalization of cell-specific biases
## Step 3.1b | Compute size factors for spike-in transcripts (Bioconductor =< 3.9)
Size factors computed from the counts for endogenous genes are usually not appropriate for normalizing the counts for spike-in transcripts. The normalization we implemented in the previous step corrects for RNA content. HOwever, spike-in transcripts are not affected by RNA content, as we theoretically added the same amount to each sample. Therefore, using gene-based size factors would "over-normalize" spike-ins. To ensure spike-in normalization is performed correctly, we compute a separate set of size factors for the spike-in set. For each cell, the spike-in-specific size factor is defined as the total count across all transcripts in the spike-in set.

These size factors are stored in a separate field of the `SingleCellExperiment` object by setting `general.use=FALSE` in `computeSpikeFactors()`. This ensures that they will only be used with the spike-in transcripts and not the endogenous genes. Although we are only using spike-ins for quality control, we could use the spike-in size factors to normalize all genes by setting `general.use=TRUE`.
```{r}
PAG_sceset_qc <- computeSpikeFactors(PAG_sceset_qc, type = "ERCC", assay.type = "counts", general.use = FALSE)
# The warning "zero spike-in counts during spike-in normalization" appears because some size factors are 0, which below 1e-8 (see https://github.com/MarioniLab/scran/blob/master/R/computeSpikeFactors.R).
summary(sizeFactors(PAG_sceset_qc, "ERCC"))
length(sizeFactors(PAG_sceset_qc, "ERCC"))
which(sizeFactors(PAG_sceset_qc, "ERCC") < 1e-8)

# To overcome this inconvenience, we add 1e-7 to each spike-in size factor. This should be okay as it is a change barely above this limit and it allows us to circumvent the warning and run normalize() later on. Otherwise normalize() requires all size factors to be positive real numbers, and unlike computeSizeFactors(), computeSpikeFactors() does not have a postive=TRUE argument as input.
sizeFactors(PAG_sceset_qc, "ERCC") <- sizeFactors(PAG_sceset_qc, "ERCC") + 1e-8
summary(sizeFactors(PAG_sceset_qc, "ERCC"))
which(sizeFactors(PAG_sceset_qc, "ERCC") < 1e-8)
```

The two sets of size factors tend to agree less due to the effects of heterogeneity in total RNA content between cells - this is expected.
```{r}
plot(sizeFactors(PAG_sceset_qc, 'ERCC'), 
     sizeFactors(PAG_sceset_qc),
     log = "xy", 
     xlab = "Size factor (ERCC)", 
     ylab = "Size factor (genes)", 
     col = c(rgb(255, 119, 124, 255/2, maxColorValue = 255), rgb(0, 156, 181, 255/3, maxColorValue = 255))[factor(PAG_sceset_qc$cell.type)],
     type = "p", pch = 19, cex = 1.25)
legend("bottomleft", col = c(rgb(255, 119, 124, 255/2, maxColorValue = 255), rgb(0, 156, 181, 255/3, maxColorValue = 255)), pch = 19, cex = 1.25,  bty = "n",
       legend = levels(factor(PAG_sceset_qc$cell.type)))
```

_If we were to apply the the spike-in factors to all counts, we would set `general.use=TRUE` and use the `computeSpikeFactors` method to estimate size factors for all cells. This would compute the total count over all spike-in transcripts in each cell, and calculate size factors to equalize the total spike-in count across cells. Finally, running normalize would use the spike-in-based size factors to compute normalized log-expression values, and unlike what we actually did, we would not have to define separate size factors for the spike-in transcripts, as the relevant factors would already be used for all genes and spike-in transcripts when `general.use=TRUE`._

# STEP 4 | Modelling technical and biological variability in gene expression to identify highly variable genes
## PATH "A" to feature selection - trendVar (Bioconductor =< 3.9)
### Step 4a.1 | Modelling the technical component of variation
Variability in the observed expression values across genes can be driven by genuine biological heterogeneity or uninteresting technical noise. To distinguish between these two possibilties, we need to model the technical component of the variance of the expression values for each gene. 

We will use the `trendVar()` function to fit a mean-variance trend to the read counts.

* We can set `block=` to block on the plate/animal of origin for each cell, to ensure that technical differences between plates do not inflate the variances. This involves estimating the mean and variance of the log-expression separately in each plate, followed by fitting a single trend to the plate-specific means and variances of all spike-in transcripts. In doing so, we implicitly assume that the trend is the same between plates. The use of `block=` also assumes that the average size factor within each plate is close to unity for both endogenous genes and spike-in transcripts.
* Some tuning of trend parameters such as `span` may be required to achieve a suitable fit (default is 0.75).
* Setting `parametric=TRUE` is especially useful for modelling the expected wave-like shape of the mean-variance relationship. (This is not the default setting as it is not robust for arbitrary trend shapes.)

In addition, the `trendVar` function automatically filters out low-abundance genes prior to trend fitting. This ensures that low-abundance genes do not interfere with the fit due to discreteness, which biases the estimate of variability of the variances around the trend; or due to the frequency of low-abundance genes, which reduces the sensitivity of span-based smoothing algorithms at higher abundances. The internal choice of filtering strategy involves a number of considerations:

* Filtering uses the average of log-expression values rather than the (library size-adjusted) average count. The mean log-expression is independent of the variance estimate in a linear modelling framework (Bourgon, Gentleman, and Huber 2010), which ensures that the filter does not introduce spurious trends in the variances at the filter boundary.
* The filter threshold is specified with the `min.mean` argument in `trendVar`. We use the default threshold of 0.1 (`min.mean`) based on the appearance of discrete patterns in the variance estimates for simulated Poisson-distributed counts. Lower thresholds of 0.001-0.01 may be more suitable for very sparse data, e.g., from droplet-based protocols.
* The filter used in `trendVar` is not applied in `decomposeVar` by default. Retention of all genes ensures that weak biological signal from rare subpopulations is not discarded. To apply the filter in `decomposeVar`, users should set `subset.row=rowMeans(logcounts(sce)) > 0.1` in the function call.

__One last comment from Aaron Lun__: On occasion, users may observe a warning from `trendVar()` about the lack of centering in the size factors. Recall that the trend is fitted to the mean and variances of the spike-in transcripts, and the technical component for each endogenous gene is estimated by interpolation. This assumes that an endogenous gene is comparable to a spike-in transcript of the same abundance. In particular, we assume that variation is primarily driven by the magnitude of the counts, based on the well-known mean-variance relationships in count models. Thus, we need to ensure that similarities in the average counts are preserved in the normalized expression values. This is achieved by centering the gene- and spike-in-based size factors in `normalize()`, such that features with similar average counts will also have similar normalized abundances. However, if the `SingleCellExperiment` object was manipulated (e.g., subsetted) after `normalize()` and before `trendVar()`, centering may not be preserved - hence the warning.

#### 4a.1.1 | Using spike-ins
One way to do so is using the set of spike-in transcripts. These were in theory added in the same quantity to each cell. Thus, the spike-in transcripts should exhibit non-biological variability, i.e., any variance in their counts should be technical in origin.

We use the `trendVar()` function to fit a mean-dependent trend to the variances of the log-expression values for the spike-in transcripts. Given the mean abundance of a gene, the fitted value of the trend is then used as an estimate of the technical component for that gene. The biological component of the variance is finally calculated by subtracting the technical component from the total variance of each gene with the `decomposeVar` function.
```{r}
# Fit variance-mean trend without blocking
start_time <- Sys.time() # Takes around 1s
var_fit_spikes <- trendVar(PAG_sceset_qc_norm,
                           #method="loess",
                           use.spikes=TRUE, 
                           parametric=TRUE,
                           min.mean=0.1,
                           loess.args=list(span=0.3)
                           ) 

var_out_spikes <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes)
head(var_out_spikes)
end_time <- Sys.time()
end_time - start_time

# Fit variance-mean trend blocking on mouse.id
start_time <- Sys.time() # Takes around 2s
var_fit_spikes_mouseid <- trendVar(PAG_sceset_qc_norm,
                                   #method="loess",
                                   use.spikes=TRUE, 
                                   parametric=TRUE, 
                                   block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                   min.mean=0.1,
                                   loess.args=list(span=0.3)
                                   ) 

var_out_spikes_mouseid <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes_mouseid)
head(var_out_spikes_mouseid)
end_time <- Sys.time()
end_time - start_time

# Fit variance-mean trend blocking on batch.processing
start_time <- Sys.time() # Takes around 2s
var_fit_spikes_batch <- trendVar(PAG_sceset_qc_norm,
                                 #method="loess",
                                 use.spikes=TRUE, 
                                 parametric=TRUE, 
                                 block=PAG_sceset_qc_norm$batch.processing, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                 min.mean=0.1,
                                 loess.args=list(span=0.3)
                                 ) 

var_out_spikes_batch <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes_batch)
head(var_out_spikes_batch)
end_time <- Sys.time()
end_time - start_time
```

We visually inspect the trend to confirm that it corresponds to the spike-in variances. A wave-like shape is typical of the mean-variance trend for log-expression values. A linear increase in the variance is observed as the mean increases from zero, as larger variances are possible when the counts increase. At very high abundances, the effect of sampling noise decreases due to the law of large numbers, resulting in a decrease in the variance.
```{r}
# Plot variance-mean trend without blocking
plot(var_out_spikes$mean, 
     var_out_spikes$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression",
     ylab="Variance of log-expression")

curve(var_fit_spikes$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes$mean[current_spikes], var_out_spikes$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on mouse.id
plot(var_out_spikes_mouseid$mean, 
     var_out_spikes_mouseid$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on mouse.id",
     ylab="Variance of log-expression")

curve(var_fit_spikes_mouseid$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes_mouseid$mean[current_spikes], var_out_spikes_mouseid$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on batch.processing
plot(var_out_spikes_batch$mean, 
     var_out_spikes_batch$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on batch.processing",
     ylab="Variance of log-expression")

curve(var_fit_spikes_batch$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes_batch$mean[current_spikes], var_out_spikes_batch$total[current_spikes], col="red", pch=16)
```

***
Notes from Aaron Lun's `simpleSingleCell` workflow:

* In practice, trend fitting is complicated by the small number of spike-in transcripts and the uneven distribution of their abundances. In the absence of spike-ins or as an alternative approach, we can set `use.spikes=FALSE` to fit a trend to the variances of the endogenous genes (see 4.1.2). Another alternative would be to create a trend based on the assumption of Poisson technical noise.
* Negative biological components are often obtained from `decomposeVar`. These are intuitively meaningless as it is impossible for a gene to have total variance below technical noise. Nonetheless, such values occur due to imprecise estimation of the total variance, especially for low numbers of cells.
* `decomposeVar` also yields p-values that can be used to define highly variable genes (HVGs) at a specific threshold for the false discovery rate (FDR). We will discuss this in more detail later, as formal detection of HVGs is not necessary for feature selection during data exploration.

#### 4a.1.2 | Without spike-ins
Ideally, the technical component would be estimated by fitting a mean-variance trend to the spike-in transcripts using the `trendVar` function as we have seen before. In practice, this strategy is compromised by the small number of spike-in transcripts, the uneven distribution of their abundances, and (for low numbers of cells) the imprecision of their variance estimates. This makes it difficult to accurately fit a complex mean-dependent trend to the spike-in variances. In some datasets, spike-in RNA may not have been added in appropriate quantities (or indeed at all). It may also be inappropriate to assume Poisson technical noise with `makeTechTrend()`, especially for read count data where amplification noise is non-negligible. In such cases, an alternative approach is to fit the trend to the variance estimates of the endogenous genes, using the `use.spikes=FALSE` setting. This assumes that the majority of genes are not variably expressed, such that the technical component dominates the total variance for those genes. The fitted value of the trend is then used as an estimate of the technical component. 

__NB__: fitting the trend to the variances of the genes with `use.spikes=FALSE` probably overestimates the technical component.
```{r}
# Fit variance-mean trend without blocking
start_time <- Sys.time() # Takes around 5s
var_fit_no_spikes <- trendVar(PAG_sceset_qc_norm, 
                              method="loess", 
                              use.spikes=FALSE,
                              parametric=TRUE,
                              #block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                              min.mean=0.1,
                              loess.args=list(span=0.3)
                              ) 
var_out_no_spikes <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes)
head(var_out_no_spikes)
end_time <- Sys.time()
end_time - start_time

# Fit variance-mean trend blocking on mouse.id
start_time <- Sys.time() # Takes around 20min
var_fit_no_spikes_mouseid <- trendVar(PAG_sceset_qc_norm, 
                                      method="loess", 
                                      use.spikes=FALSE,
                                      parametric=TRUE,
                                      block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                      min.mean=0.1,
                                      loess.args=list(span=0.3)
                                      ) 
var_out_no_spikes_mouseid <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes_mouseid)
head(var_out_no_spikes_mouseid)
end_time <- Sys.time()
end_time - start_time

# Fit variance-mean trend blocking on batch.processing
start_time <- Sys.time() # Takes around 20min
var_fit_no_spikes_batch <- trendVar(PAG_sceset_qc_norm, 
                                    method="loess", 
                                    use.spikes=FALSE,
                                    parametric=TRUE,
                                    block=PAG_sceset_qc_norm$batch.processing, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                    min.mean=0.1,
                                    loess.args=list(span=0.3)
                                    ) 
var_out_no_spikes_batch  <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes_batch)
head(var_out_no_spikes_batch)
end_time <- Sys.time()
end_time - start_time
```

We assess the suitability of the trend fitted to the endogenous variances by examining whether it is consistent with the spike-in variances. If the trend passes through or close to most of the spike-in variances, this indicates that our assumption (that most genes have low levels of biological variability) is valid. This strategy exploits the large number of endogenous genes to obtain a stable trend, with the spike-in transcripts used as diagnostic features rather than in the trend fitting itself. However, if our assumption does not hold, we would instead fit the trend directly to the spike-in variances with the default use.spikes=TRUE. This sacrifices stability to reduce systematic errors in the estimate of the biological component for each gene.
```{r}
# Plot variance-mean trend without blocking
plot(var_out_no_spikes$mean, 
     var_out_no_spikes$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes$mean[current_spikes], var_out_no_spikes$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on mouse.id
plot(var_out_no_spikes_mouseid$mean, 
     var_out_no_spikes_mouseid$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on mouse.id", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes_mouseid$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes_mouseid$mean[current_spikes], var_out_no_spikes_mouseid$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on batch.processing
plot(var_out_no_spikes_batch$mean, 
     var_out_no_spikes_batch$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on batch.processing", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes_batch$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes_batch$mean[current_spikes], var_out_no_spikes_batch$total[current_spikes], col="red", pch=16)
```

### Step 4a.2 | Identifying highly variable genes
HVGs are defined as genes with biological components that are significantly greater than zero. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. Formal detection of HVGs allows us to avoid genes that are highly variable due to technical factors such as sampling noise during RNA capture and library preparation. This adds another level of statistical rigour to our previous analyses, in which we only modelled the technical component.

Identifying HVGs requires estimation of the variance in expression for each gene, followed by decomposition of the variance into biological and technical components (done in the previous step). HVGs are then identified as those genes with the largest biological components. 

We define HVGs as genes with biological components that are significantly greater than zero at a false discovery rate (FDR) of 5% or 1%. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. In addition, we could consider a gene to be a HVG only if it has a biological component greater than or equal to 0.5. For transformed expression values on the log2 scale, this would mean that the average difference in true expression between any two cells will be at least 2-fold. (This reasoning assumes that the true log-expression values are Normally distributed with variance of 0.5. The root-mean-square of the difference between two values is treated as the average log2-fold change between cells and is equal to unity.) We rank the results by the biological component to focus on genes with larger biological variability.
```{r}
# Consider playing with the $FDR threshold and the $bio threshold, try reducing/increasing them to make them more stringent or to get more genes.

# from trendVar with spikes
hvg_out_spikes <- var_out_spikes[which(var_out_spikes$FDR <= 0.05 & var_out_spikes$bio > 0.5), ]
nrow(hvg_out_spikes)

hvg_out_spikes_mouseid <- var_out_spikes_mouseid[which(var_out_spikes_mouseid$FDR <= 0.05 & var_out_spikes_mouseid$bio > 0.5), ]
nrow(hvg_out_spikes_mouseid)

hvg_out_spikes_batch <- var_out_spikes_batch[which(var_out_spikes_batch$FDR <= 0.05 & var_out_spikes_batch$bio > 0.5), ]
nrow(hvg_out_spikes_batch)


# from trendVar without spikes
hvg_out_no_spikes <- var_out_no_spikes[which(var_out_no_spikes$FDR <= 0.05 & var_out_no_spikes$bio > 0.5), ]
nrow(hvg_out_no_spikes)

hvg_out_no_spikes_mouseid <- var_out_no_spikes_mouseid[which(var_out_no_spikes_mouseid$FDR <= 0.05 & var_out_no_spikes_mouseid$bio > 0.5), ]
nrow(hvg_out_no_spikes_mouseid)

hvg_out_no_spikes_batch <- var_out_no_spikes_batch[which(var_out_no_spikes_batch$FDR <= 0.05 & var_out_no_spikes_batch$bio > 0.5), ]
nrow(hvg_out_no_spikes_batch)
```

#### 4a.2.1 | Rank and filter the results
We rank the results to focus on genes with larger biological components. This highlights an interesting aspect of the underlying hypothesis test, which is based on the ratio of the total variance to the expected technical variance. Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be interesting. This is because the ratio can be very large for HVGs that have very low total variance and do not contribute much to the cell-cell heterogeneity.
```{r}
# from trendVar with spikes
hvg_out_spikes <- hvg_out_spikes[order(hvg_out_spikes$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes)
head(hvg_out_spikes)

hvg_out_spikes_mouseid <- hvg_out_spikes_mouseid[order(hvg_out_spikes_mouseid$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes_mouseid)
head(hvg_out_spikes_mouseid)

hvg_out_spikes_batch <- hvg_out_spikes_batch[order(hvg_out_spikes_batch$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes_batch)
head(hvg_out_spikes_batch)

# from trendVar without spikes
hvg_out_no_spikes <- hvg_out_no_spikes[order(hvg_out_no_spikes$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes)
head(hvg_out_no_spikes)

hvg_out_no_spikes_mouseid <- hvg_out_no_spikes_mouseid[order(hvg_out_no_spikes_mouseid$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes_mouseid)
head(hvg_out_no_spikes_mouseid)

hvg_out_no_spikes_batch <- hvg_out_no_spikes_batch[order(hvg_out_no_spikes_batch$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes_batch)
head(hvg_out_no_spikes_batch)
```

__One important thing...__
Before moving on to downstream analysis, we consider dropping any Ribosomal, Mitochondrial, ERCC, sex-specific genes (e.g. XIST), transgenes (EYFP, tdTomato, Cre, TSO concatemers) and genes used for transgenic labeling of cells (VGAT and VGluT2) from the dataset, as some of them are not be biologically informative and others have been used to select the cells. We do so for both our `SingleCellExperiment` object and for the different HVGs:
```{r}
# Identify the genes we want to drop:
is_X_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "X") # Find which genes correspond to chromosome X 
is_Y_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "Y") # Find which genes correspond to chromosome Y
is_feature_control_qc_norm <- (rowData(PAG_sceset_qc_norm)$is_feature_control) # Find genes assigned to feature controls (ERCC, Mitochondrial, Ribosomal, tdTomato, EYFP, Cre, TSO concatemers, VGAT, VGluT2)

filter_genes_qc_norm <- !(is_X_qc_norm | is_Y_qc_norm | is_feature_control_qc_norm)

# Apply the filter to the SingleCellExperiment object:
dim(PAG_sceset_qc_norm)
PAG_sceset_qc_norm_filt <- PAG_sceset_qc_norm[filter_genes_qc_norm, ]
dim(PAG_sceset_qc_norm_filt)

# Apply the filter to the HVGs:
filt_hvg_1 <- rownames(hvg_out_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_filt <- hvg_out_spikes[filt_hvg_1, ]
nrow(hvg_out_spikes_filt)

filt_hvg_2 <- rownames(hvg_out_spikes_mouseid) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_mouseid_filt <- hvg_out_spikes_mouseid[filt_hvg_2,]
nrow(hvg_out_spikes_mouseid_filt)

filt_hvg_3 <- rownames(hvg_out_spikes_batch) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_batch_filt <- hvg_out_spikes_batch[filt_hvg_3,]
nrow(hvg_out_spikes_batch_filt)

filt_hvg_4 <- rownames(hvg_out_no_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_filt <- hvg_out_no_spikes[filt_hvg_4,]
nrow(hvg_out_no_spikes_filt)

filt_hvg_5 <- rownames(hvg_out_no_spikes_mouseid) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_mouseid_filt <- hvg_out_no_spikes_mouseid[filt_hvg_5,]
nrow(hvg_out_no_spikes_mouseid_filt)

filt_hvg_6 <- rownames(hvg_out_no_spikes_batch) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_batch_filt <- hvg_out_no_spikes_batch[filt_hvg_6,]
nrow(hvg_out_no_spikes_batch_filt)
```

#### 4a.2.2 | Visualize, store, and export HVGs
Now that we have removed the genes we are not interested in, we can use `limma` and `vennDiagram` to compare how many genes we get in each condition with or without spike-ins, and with or without blocking:
```{r}
library(limma)
#####
# Inspect the overlap between HVGs obtained with or without using spike-ins to fit a variance trend:
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_mouseid_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_spikes_batch_filt))
venn_diag_spikes_nospikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_filt),
                                              rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_filt))
                                        )

vennDiagram(venn_diag_spikes_nospikes,
            names = c("Spikes", "No_Spikes"),
            circle.col = c("#E69F00", "#56B4E9")
            )
venn_diag_spikes_nospikes_mouseid <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_mouseid_filt),
                                                      rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt))
                                                )

vennDiagram(venn_diag_spikes_nospikes_mouseid,
            names = c("Spikes_MouseID", "No_Spikes_MouseID"),
            circle.col = c("#E69F00", "#56B4E9")
            )

venn_diag_spikes_nospikes_batch <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_batch_filt),
                                                    rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_batch_filt))
                                              )
vennDiagram(venn_diag_spikes_nospikes_batch,
            names = c("Spikes_Batch", "No_Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9")
            )

#####
# Inspect the overlap between HVGs obtained using spike-ins to fit a variance trend, and with or without blocking:
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_spikes_batch_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_spikes_batch_filt))
venn_diag_spikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_filt),
                                     rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_mouseid_filt),
                                     rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_batch_filt))
                               )
vennDiagram(venn_diag_spikes,
            names = c("Spikes", "Spikes_MouseID", "Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9", "#009E73")
            )

#####
# Inspect the overlap between HVGs obtained without using spike-ins to fit a variance trend, and with or without blocking:
sum(rownames(hvg_out_no_spikes_mouseid_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt))
venn_diag_no_spikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_filt),
                                        rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt),
                                        rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_batch_filt))
                                  )
vennDiagram(venn_diag_no_spikes,
            names = c("No_Spikes", "No_Spikes_MouseID", "No_Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9", "#009E73")
            )
```

Taking into account the above Venn Diagrams, the plots of the mean-variance trend we obtained in `Step 4a.1`, and the profiles and total numbers of HVGs selected by each approach, we will proceed by using the HVGs obtained without using spike-ins and without blocking, the trend and variance-mean plot of which achieves the best compromise between spike-ins and endogenous genes.

Before we store the identified HVGs in our `SingleCellExperiment` object, we check the distribution of expression values for the genes with the largest biological components to ensure that the variance estimate is not being dominated by one or two outlier cells.
```{r}
fontsize <- theme(axis.text=element_text(size=12), axis.title=element_text(size=16))

#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_mouseid_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_batch_filt)[1:20]) + fontsize

plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_filt)[1:50]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_mouseid_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_batch_filt)[1:20]) + fontsize
```

Now that we have ordered, filtered, and inspected the identified HVGs we can add the results into the `metadata` component of the `PAG_sceset_qc_norm_filt` object. The metadata component can hold any object, as it is a list container. Any results that weâ€™d like to keep are safe to store here, and a great way to save or share intermediate results that would otherwise be kept in separate objects. Even though we have decided to use the HVGs identified by fitting `trendVar` to the endogenous genes (i.e. without using spike-ins) without any blocking factor, we store the HVGs from the different approaches we used (even without filtering) so we can go back and find them again for any future comparisons we might want to do:
```{r}
# hvg_out_spikes
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes <- rownames(hvg_out_spikes)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_filt <- rownames(hvg_out_spikes_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_filt)

# hvg_out_spikes_mouseid
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid <- rownames(hvg_out_spikes_mouseid)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid_filt <- rownames(hvg_out_spikes_mouseid_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid_filt)

# hvg_out_spikes_batch
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch <- rownames(hvg_out_spikes_batch)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch_filt <- rownames(hvg_out_spikes_batch_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch_filt)

# hvg_out_no_spikes
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes <- rownames(hvg_out_no_spikes)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt <- rownames(hvg_out_no_spikes_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt)

# hvg_out_no_spikes_mouseid
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid <- rownames(hvg_out_no_spikes_mouseid)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid_filt <- rownames(hvg_out_no_spikes_mouseid_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid_filt)

# hvg_out_no_spikes_batch
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch <- rownames(hvg_out_no_spikes_batch)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch_filt <- rownames(hvg_out_no_spikes_batch_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch_filt)
```

We can also export them into a `.tsv` file:
```{r}
# Before filtering unwanted genes:
#write.table(file="PAG_hvg_out_spikes.tsv", hvg_out_spikes, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_mouseid.tsv", hvg_out_spikes_mouseid, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_batch.tsv", hvg_out_spikes_batch, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes.tsv", hvg_out_no_spikes, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_mouseid.tsv", hvg_out_no_spikes_mouseid, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_batch.tsv", hvg_out_no_spikes_batch, sep="\t", quote=FALSE, col.names=NA)

# After filtering unwanted genes:
#write.table(file="PAG_hvg_out_spikes_filt.tsv", hvg_out_spikes_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_mouseid_filt.tsv", hvg_out_spikes_mouseid_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_batch_filt.tsv", hvg_out_spikes_batch_filt, sep="\t", quote=FALSE, col.names=NA)
write.table(file="PAG_hvg_out_no_spikes_filt.tsv", hvg_out_no_spikes_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_mouseid_filt.tsv", hvg_out_no_spikes_mouseid_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_batch_filt.tsv", hvg_out_no_spikes_batch_filt, sep="\t", quote=FALSE, col.names=NA)
```

***
Notes from Aaron Lun's `simpleSingleCell` workflow:

One reason to use the variance of the log-expression values is the fact that the log-transformation protects against genes with strong expression in only one or two cells. This reduces the risk that the set of top HVGs is not dominated by genes with (mostly uninteresting) outlier expression patterns. There are, however, many other strategies for defining HVGs, based on a variety of metrics:

* the coefficient of variation, using the `technicalCV2()` function (Brennecke et al. 2013) or the `DM()` function (Kim et al. 2015) in _scran_.
* the dispersion parameter in the negative binomial distribution, using the `estimateDisp()` function in _edgeR_ (McCarthy, Chen, and Smyth 2012).
* a proportion of total variability, using methods in the _BASiCS_ package (Vallejos, Marioni, and Richardson 2015).

### Step 4a.3 | Identifying correlated gene pairs with Spearman's rho
Another useful procedure is to identify the HVGs that are highly correlated with one another. This distinguishes between HVGs caused by random noise and those involved in driving systematic differences between subpopulations. Correlations between genes can be quantified by computing Spearman's rho, which accommodates non-linear relationships in the expression values. Gene pairs with significantly large positive or negative values of rho are identified using the `correlatePairs()` function. We only apply this function to the set of HVGs, because these genes have large biological components and are more likely to exhibit strong correlations driven by biology. In contrast, calculating correlations for all possible gene pairs would require too much computational time and increase the severity of the multiple testing correction. It may also prioritize uninteresting genes that have strong correlations but low variance, e.g., tightly co-regulated house-keeping genes.
```{r}
# Takes around 1 min
set.seed(1991)
library(scran)

start_time_c1 <- Sys.time()
var.cor <- correlatePairs(PAG_sceset_qc_norm_filt, 
                          subset.row=metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt) 
write.table(file="PAG_correlated_genes.tsv", var.cor, sep="\t", quote=FALSE, row.names=FALSE) 
head(var.cor)
end_time_c1 <- Sys.time()
end_time_c1 - start_time_c1
```

The significance of each correlation is determined using a permutation test. For each pair of genes, the null hypothesis is that the expression profiles of two genes are independent. Shuffling the profiles and recalculating the correlation yields a null distribution that is used to obtain a p-value for each observed correlation value (Phipson & Smyth, 2010). Correction for multiple testing across many gene pairs is performed by controlling the FDR at 5%. Correlated gene pairs can be directly used for experimental validation with orthogonal techniques (e.g., fluorescence-activated cell sorting, immunohistochemistry or RNA fluorescence in situ hybridization) to verify that these expression patterns are genuinely present across the cell population.
```{r}
sig.cor <- var.cor$FDR <= 0.05 
summary(sig.cor)
```

The use of `correlatePairs()` is primarily intended to identify correlated gene pairs for validation studies. Obviously, non-zero correlations do not provide evidence for a direct regulatory interaction, let alone specify causality. To construct regulatory networks involving many genes, we suggest using dedicated packages such as _[WGCNA](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/)_.

#### 4a.4.1 | Larger sets of genes
Larger sets of correlated genes are assembled by treating genes as nodes in a graph and each pair of genes with significantly large correlations as an edge. In particular, an undirected graph is constructed using methods in the `RBGL` package. Highly connected subgraphs are then identified and defined as gene sets. This provides a convenient summary of the pairwise correlations between genes.
```{r}
# Takes around 15 min
library(RBGL) 
start_time_c2 <- Sys.time()
g <- ftM2graphNEL(cbind(var.cor$gene1, var.cor$gene2)[sig.cor,], W=NULL, V=NULL, edgemode="undirected")
cl <- highlyConnSG(g)$clusters # Time consuming step
cl <- cl[order(lengths(cl), decreasing=TRUE)] 
head(cl)
end_time_c2 <- Sys.time()
end_time_c2 - start_time_c2
```

Significant correlations provide evidence for substructure in the dataset, i.e., subpopulations of cells with systematic differences in their expression profiles. The number of significantly correlated HVG pairs represents the strength of the substructure. If many pairs were significant, this would indicate that the subpopulations were clearly defined and distinct from one another. 

## PATH "B" to feature selection - M3Drop
__Based on the online course on [Analysis of single cell RNA-seq data](https://github.com/hemberg-lab/scRNA.seq.course) by the [Hemberg Lab](https://www.sanger.ac.uk/science/groups/hemberg-group).__

We continue using the `PAG_sceset_qc_norm` after normalization. We should thus have a `logcounts` slot in`assays`:
```{r}
# If starting from stored results, load saved filtered dataset from previous Step:
set.seed(1991)
options(stringsAsFactors = FALSE)
library(SingleCellExperiment)
library(scater)
library(scran)
library(matrixStats)
library(M3Drop)

PAG_sceset_qc_norm <- readRDS("PAG_sceset_qc_norm.rds") # Contains filtered cells and genes, and normalized data
assayNames(PAG_sceset_qc_norm)
PAG_sceset_qc_norm
```

scRNA-seq is capable of measuring the expression of many thousands of genes in every cell. However, in most situations only a portion of those will show a response to the biological condition of interest, e.g. differences in cell-type, drivers of differentiation, respond to an environmental stimulus. Most genes detected in a scRNA-seq experiment will only be detected at different levels due to technical noise. One consequence of this is that technical noise and batch effects can obscure the biological signal of interest. Thus, it is often advantageous to perform feature selection to remove those genes which only exhibit technical noise from downstream analysis. Not only does this generally increase the signal:noise ratio in the data; it also reduces the computational complexity of analyses, by reducing the total amount of data to be processed.

For scRNA-seq data, we will be focusing on unsupervised methods of feature selection which donâ€™t require any a priori information, such as cell-type labels or biological group, since they may not be available, or may be unreliable, for many experiments. In contrast, differential expression can be considered a form of supervised feature selection since it uses the known biological label of each sample to identify features (i.e. genes) which are expressed at different levels across groups.

__One important thing...__
Before identifying HVGs, consider dropping any Ribosomal, Mitochondrial, ERCC, sex-specific genes (e.g. XIST), transgenes (EYFP, tdTomato, Cre, TSO concatemers) and genes used for transgenic labeling of cells (VGAT and VGluT2) from the dataset before proceeding to downstream analysis, as some of them are not be biologically informative and others have been used to select the cells.
```{r}
dim(PAG_sceset_qc_norm)

is_X_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "X") # Find which genes correspond to chromosome X 
is_Y_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "Y") # Find which genes correspond to chromosome Y
is_feature_control_qc_norm <- (rowData(PAG_sceset_qc_norm)$is_feature_control) # Find which genes correspond to feature controls (Ribosomal, Mitochondrial, ERCC, transgenes)

filter_genes_qc_norm <- !(is_X_qc_norm | is_Y_qc_norm | is_feature_control_qc_norm)
PAG_sceset_qc_norm <- PAG_sceset_qc_norm[filter_genes_qc_norm, ]
dim(PAG_sceset_qc_norm)
```

Feature selection is performed after QC. `M3Drop` contains two different feature selection methods `M3DropFeatureSelection` which is based on a Michaelis-Menten curve and is designed for full-transcript single-cell RNA-seq data (such as Smart-seq2) and `NBumiFeatureSelectionCombinedDrop` which is based on a negative binomial model and is designed for UMI count data. We will use the former.

`M3Drop` feature selection runs on a normalized (but not log-transformed) expression matrix. This can be extracted from our `SingleCellExperiment` object using the command below. This function is compatible with most single-cell RNA-seq analysis packages including: `scater`, `SingleCellExperiment`, `monocle`, and `Seurat`. It can also convert an existing expression matrix to the correct form (removing undetected genes & normalizing/delogging) if you specify whether the matrix is raw counts, or log transformed. Check the manual for details.
```{r}
#expr_matrix <- M3Drop::M3DropConvertData(PAG_sceset_qc_norm) #
#expr_matrix <- M3DropConvertData(PAG_sceset_qc_norm@assays[["logcounts_raw"]], is.log=TRUE, is.counts=FALSE)
expr_matrix <- M3DropConvertData(PAG_sceset_qc_norm@assays[["counts"]], is.log=FALSE, is.counts=TRUE)
nrow(counts(PAG_sceset_qc_norm)) - nrow(expr_matrix) # check the ConvertData function has dropped undetected genes
```

### Step 4b.1 | Identifying genes vs a Null Model
There are two main approaches to unsupervised feature selection. The first is to identify genes which behave differently from a null model describing just the technical noise expected in the dataset.

If the dataset contains spike-in RNAs they can be used to directly model technical noise. However, measurements of spike-ins may not experience the same technical noise as endogenous transcripts (Svensson et al., 2017). In addition, scRNA-seq experiments often contain only a small number of spike-ins which reduces our confidence in fitted model parameters. We will not use spike-in transcripts beyond QC in our analysis.

#### 4b.1.1 | Highly Variable Genes
The first method proposed to identify features in scRNA-seq datasets was to identify highly variable genes (HVG). HVG assumes that if genes have large differences in expression across cells some of those differences are due to biological difference between the cells rather than technical noise. However, because of the nature of count data, there is a positive relationship between the mean expression of a gene and the variance in the read counts across cells. This relationship must be corrected for to properly identify HVGs.
```{r}
# Plot the relationship between mean expression and variance for all genes in this dataset:
plot(
    rowMeans(expr_matrix), 
    rowVars(expr_matrix), 
    log="xy", 
    pch=16,
    xlab="Mean Expression", 
    ylab="Variance", 
    main=""
)
```

A popular method to correct for the relationship between variance and mean expression was proposed by [Brennecke et al. 2013](http://www.nature.com/nmeth/journal/v10/n11/full/nmeth.2645.html). To use the Brennecke method, we first normalize for library size and then calculate the mean and the square coefficient of variation (variation divided by the squared mean expression). A quadratic curve is fit to the relationship between these two variables for the ERCC spike-in, and then a chi-square test is used to find genes significantly above the curve. This method is included in the `M3Drop` package as the `Brennecke_getVariableGenes(counts, spikes)` function. If the dataset does not contain spike-ins we can use the entire dataset to estimate the technical noise.

In the figure below the red curve is the fitted technical noise model and the dashed line is the 95% CI. Pink dots are the genes with significant biological variability after multiple-testing correction.
```{r}
Brennecke_HVG <- M3Drop::BrenneckeGetVariableGenes(expr_matrix,
                                                   fdr = 0.01,
                                                   minBiolDisp = 0.5
                                                   )
```

This function returns a matrix of significant genes as well as their estimated effect size (difference between observed and expected coefficient of variation), and their significance as raw p.values and FDR corrected q.values. For now we will just keep the names of the significant HVG genes.
```{r}
HVG_genes <- Brennecke_HVG$Gene
length(HVG_genes)
```

#### 4b.1.2 | High Dropout Genes
An alternative to finding HVGs is to identify genes with unexpectedly high numbers of zeros. The frequency of zeros, known as the "dropout rate", is very closely related to expression level in scRNASeq data. Zeros are the dominant feature of single-cell RNASeq data, typically accounting for over half of the entries in the final expression matrix. These zeros predominantly result from the failure of mRNAs failing to be reversed transcribed (Andrews and Hemberg, 2016). Reverse transcription is an enzyme reaction thus can be modelled using the Michaelis-Menten equation: Pdropout=1âˆ’S/(K+S) where S is the mRNA concentration in the cell (we will estimate this as average expression) and K is the Michaelis-Menten constant.

Because the Michaelis-Menten equation is a convex non-linear function, genes which are differentially expression across two or more populations of cells in our dataset will be shifted up/right of the Michaelis-Menten model. We use M3Drop to identify significant outliers to the right of the MM curve. We also apply 1% FDR multiple testing correction:
```{r}
M3Drop_genes <- M3DropFeatureSelection(expr_matrix,
                                       mt_method = "fdr",
                                       mt_threshold = 0.01
                                       )
M3Drop_genes <- M3Drop_genes$Gene
length(M3Drop_genes)
```

### Step 4b.2 | Correlated Expression
A completely different approach to feature selection is to use gene-gene correlations. This method is based on the idea that multiple genes will be differentially expressed between different cell-types or cell-states. Genes which are expressed in the same cell-population will be positively correlated with each other where as genes expressed in different cell-populations will be negatively correated with each other. Thus important genes can be identified by the magnitude of their correlation with other genes.

The limitation of this method is that it assumes technical noise is random and independent for each cell, thus shouldn't produce gene-gene correlations, but this assumption is violated by batch effects which are generally systematic between different experimental batches and will produce gene-gene correlations. As a result it is more appropriate to take the top few thousand genes as ranked by gene-gene correlation than consider the significance of the correlations.
```{r}
cor_feat <- M3Drop::corFS(expr_matrix) # Takes a long time
Cor_genes <- names(cor_feat)[1:1500]
```

### Step 4b.3 | PCA loadings
Lastly, another common method for feature selection in scRNASeq data is to use PCA loadings. Genes with high PCA loadings are likely to be highly variable and correlated with many other variable genes, thus may be relevant to the underlying biology. However, as with gene-gene correlations PCA loadings tend to be susceptible to detecting systematic variation due to batch effects; thus it is recommended to plot the PCA results to determine those components corresponding to the biological variation rather than batch effects.
```{r}
# PCA is typically performed on log-transformed expression data
pca <- prcomp(log(expr_matrix + 1) / log(2))

# plot projection
plot(
    pca$rotation[,1], 
    pca$rotation[,2], 
    pch = 16
)
```

```{r}
# calculate loadings for components 1 and 2
score <- rowSums(abs(pca$x[,c(1,2)])) 
names(score) <- rownames(expr_matrix)
score <- score[order(-score)]
PCA_genes <- names(score[1:1500])
```

```{r}
#Consider the top 5 principal components. Which ones appear to be most biologically relevant? How do the top 1,500 features change if you consider the loadings for those components?
plot(
    pca$rotation[,2], 
    pca$rotation[,3], 
    pch = 16
)

plot(
    pca$rotation[,3], 
    pca$rotation[,4], 
    pch = 16
)

# calculate loadings for components 1 and 2
score <- rowSums(abs(pca$x[,c(2, 3, 4)]))
names(score) <- rownames(expr_matrix)
score <- score[order(-score)]
PCA_genes2 = names(score[1:1500])
```

### Step 4b.4 | Comparing Methods
We can check whether the identified features really do represent genes differentially expressed between cell-types in this dataset.
```{r}
M3DropExpressionHeatmap(M3Drop_genes,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

```{r}
M3DropExpressionHeatmap(HVG_genes,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

```{r}
M3DropExpressionHeatmap(Cor_genes,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

```{r}
M3DropExpressionHeatmap(PCA_genes,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

```{r}
M3DropExpressionHeatmap(PCA_genes2,
                        expr_matrix,
                        cell_labels = PAG_sceset_qc_norm$cell.type
                        )
```

We can also consider how consistent each feature selection method is with the others using the Jaccard Index:
```{r}
list_of_features <- list(M3Drop_genes,
                         HVG_genes, 
                         Cor_genes, 
                         PCA_genes, 
                         PCA_genes2
                         )

Out <- matrix(0, 
              ncol = length(list_of_features), 
              nrow = length(list_of_features)
              )

for(i in 1:length(list_of_features) ) {
    for(j in 1:length(list_of_features) ) {
        Out[i,j] <- sum(list_of_features[[i]] %in% list_of_features[[j]])/
            length(unique(c(list_of_features[[i]], list_of_features[[j]])))
     }
}

colnames(Out) <- rownames(Out) <- c("M3Drop", "HVG", "Cor", "PCA", "PCA2")
Out
```

# STEP 5 | Identifying confounding factors, correcting batch effects, and dimensionality reduction
## Step 5.3 | Dimensionality reduction
### 5.3.3 | Denoising expression values using PCA
Another strategy is to retain all PCs until the percentage of total variation explained reaches some threshold. For example, we might retain the top set of PCs that explains 80% of the total variation in the data. Of course, it would be pointless to swap one arbitrary parameter `d` for another `T`. Instead, we derive a suitable value for `T` by calculating the proportion of variance in the data that is attributed to the biological component. This is done using the `denoisePCA()` function with the variance modelling results from `modelGeneVar()` or related functions, where `T` is defined as the ratio of the sum of the biological components to the sum of total variances. Here, explicit feature selection is not strictly necessary, as `denoisePCA()` will automatically restrict the PCA to genes with positive biological components to ensure that `T` is always a positive value. The function returns a `SingleCellExperiment` object containing the PC scores for each cell in the `reducedDims` slot, where cells are rows and PCs are columns. The aim is to eliminate technical noise and enrich for biological signal in the retained PCs. This improves resolution of the underlying biology during downstream procedures such as clustering.

Importantly, this method tends to perform best when the mean-variance trend reflects the actual technical noise, i.e., when it is estimated by `modelGeneVarByPoisson()` or `modelGeneVarWithSpikes()` instead of `modelGeneVar()`. Variance modelling results from `modelGeneVar()` tend to understate the actual biological variation, especially in highly heterogeneous datasets where secondary factors of variation inflate the fitted values of the trend. Fewer PCs are subsequently retained because `T` is artificially lowered. We will thus not be using this method, as we couldn't use spike-ins for our variance modelling.
```{r}
set.seed(1991)

# Run denoisePCA for hvg_var_out_no_spikes_filt
dim(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes)
var_model_no_spikes$filter <- rownames(var_model_no_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes)
var_model_no_spikes_filt <- var_model_no_spikes[var_model_no_spikes$filter==TRUE, ]
dim(var_model_no_spikes_filt)

PAG_var_denoised <- denoisePCA(PAG_sceset_qc_norm_filt,
                               technical = var_model_no_spikes_filt, 
                               subset.row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_filt,
                               assay.type = "logcounts")

print(ncol(reducedDim(PAG_var_denoised, "PCA")))

plotReducedDim(PAG_var_denoised,
               dimred = "PCA",
               colour_by = "cell.type",
               shape_by = "PAG.arearegistration",
               by_exprs_values = "logcounts")

# Run denoisePCA for hvg_var_out_no_spikes_block_filt
dim(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes_block)
var_model_no_spikes_block$filter <- rownames(var_model_no_spikes_block) %in% rownames(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes_block)
var_model_no_spikes_block_filt <- var_model_no_spikes_block[var_model_no_spikes_block$filter==TRUE, ]
dim(var_model_no_spikes_block_filt)

PAG_var_block_denoised <- denoisePCA(PAG_sceset_qc_norm_filt,
                                     technical = var_model_no_spikes_block_filt, 
                                     subset.row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block_filt,
                                     assay.type = "logcounts")

print(ncol(reducedDim(PAG_var_block_denoised, "PCA")))

plotReducedDim(PAG_var_block_denoised,
               dimred = "PCA",
               colour_by = "cell.type",
               shape_by = "PAG.arearegistration",
               by_exprs_values = "logcounts")
```

### 5.3.6 | Correlating PCs with experimental and QC variables of interest
`scater` allows us to identify principal components that correlate with experimental and QC variables of interest (it ranks principle components by R^2^ from a linear model regressing PC value against any variable or annotation we have associated with each cell) to see which factor is driving a particular principal component.
```{r}
plotExplanatoryPCs(PAG_sceset_qc_norm_filt,
                   dimred = "PCA_HVG_var_logcounts",
                   n_dimred = 20,
                   nvars_to_plot = 17,
                   npcs_to_plot = 20,                 
                   variables = c("mouse.id",
                                 "mouse.sex",
                                 "mouse.age",
                                 "mouse.singlehousedays",
                                 "cell.type",
                                 "cell.fluorophor",
                                 "slice.number",
                                 "slice.depth",
                                 "PAG.areacollection",
                                 "PAG.hemisphere",
                                 "PAG.APaxis",
                                 "time.sinceslicinghour",
                                 "PAGarea_celltype",
                                 "batch.processing",
                                 "batch.sequencing_round",
                                 "detected",
                                 "total"
                                 )
                   )
```

# STEP 6 | Clustering
## Step 6.1 | Graph-based clustering
### 6.1.2 | Assessing cluster separation
One useful approach is to use the ratio matrix to form another graph where the nodes are clusters rather than cells. Edges between nodes are weighted according to the ratio of observed to expected edge weights between cells in those clusters. We can then repeat our graph operations on this new cluster-level graph. For example, we could obtain clusters of clusters, or we could simply create a new cluster-based layout for visualization. This is analogous to the â€œgraph abstractionâ€ approach described by Wolf et al. 2017.
```{r}
set.seed(1991)
cluster_graph_var_rank <- igraph::graph_from_adjacency_matrix(log2(ratio_var_rank + 1), mode = "upper", weighted = TRUE, diag = FALSE)
# Increasing the weight to increase the visibility of the lines.
plot(cluster_graph_var_rank, edge.width = igraph::E(cluster_graph_var_rank)$weight*5, layout = igraph::layout_with_lgl)

cluster_graph_var_jaccard <- igraph::graph_from_adjacency_matrix(log2(ratio_var_jaccard + 1), mode = "upper", weighted = TRUE, diag = FALSE)
# Increasing the weight to increase the visibility of the lines.
plot(cluster_graph_var_jaccard, edge.width = igraph::E(cluster_graph_var_jaccard)$weight*5, layout = igraph::layout_with_lgl)

cluster_graph_cv2_rank <- igraph::graph_from_adjacency_matrix(log2(ratio_cv2_rank + 1), mode = "upper", weighted = TRUE, diag = FALSE)
plot(cluster_graph_cv2_rank, edge.width = igraph::E(cluster_graph_cv2_rank)$weight*5, layout = igraph::layout_with_lgl)

cluster_graph_cv2_jaccard <- igraph::graph_from_adjacency_matrix(log2(ratio_cv2_jaccard + 1), mode = "upper", weighted = TRUE, diag = FALSE)
plot(cluster_graph_cv2_jaccard, edge.width = igraph::E(cluster_graph_cv2_jaccard)$weight*5, layout = igraph::layout_with_lgl)
```

The graph itself can be visualized using a force-directed layout. This yields a dimensionality reduction result that is closely related to t-SNE and UMAP.
```{r}
set.seed(1991)
reducedDim(PAG_sceset_qc_norm_filt_corr, "force_var") <- igraph::layout_with_fr(graph_var)
plotReducedDim(PAG_sceset_qc_norm_filt_corr, colour_by = "SNN_clusters_var", dimred = "force_var")

reducedDim(PAG_sceset_qc_norm_filt_corr, "force_cv2") <- igraph::layout_with_fr(graph_cv2)
plotReducedDim(PAG_sceset_qc_norm_filt_corr, colour_by = "SNN_clusters_cv2", dimred = "force_cv2")
```

## Step 6.1b | Subclustering or Cell-type specific Graph-based clustering
Another approach to improving resolution is to repeat the feature selection and clustering within a single cluster. This aims to select HVGs and PCs that are more relevant to internal structure, improving resolution by avoiding noise from unnecessary features. Subsetting also encourages clustering methods to separate cells according to more modest heterogeneity in the absence of distinct subpopulations. We can repeat the steps we have done after normalization but subsetting the `SingleCellExperiment` object in excitatory and inhibitory cells:
```{r}
PAG_sceset_qc_norm_filt_corr_VGAT <- PAG_sceset_qc_norm_filt_corr[, PAG_sceset_qc_norm_filt_corr$cell.type=="VGAT"]
PAG_sceset_qc_norm_filt_corr_VGluT2 <- PAG_sceset_qc_norm_filt_corr[, PAG_sceset_qc_norm_filt_corr$cell.type=="VGluT2"]
```

### 6.1b.1 | Implementation of graph-based clustering
```{r}
library(scran)
# Using HVG from Var with rank-based weights
graph_var_rank_VGAT <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGAT,
                                     k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                     #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                     type = "rank", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                     subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_var_out_no_spikes_block_filt,
                                     assay.type = "logcounts",
                                     use.dimred = "PCA_HVG_var_logcounts")
clusters_var_rank_VGAT <- igraph::cluster_walktrap(graph_var_rank_VGAT)$membership
PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_var_rank_VGAT <- factor(clusters_var_rank_VGAT)
table(PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_var_rank_VGAT)

graph_var_rank_VGluT2 <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGluT2,
                                       k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                       #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                       type = "rank", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                       subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_var_out_no_spikes_block_filt,
                                       assay.type = "logcounts",
                                       use.dimred = "PCA_HVG_var_logcounts")
clusters_var_rank_VGluT2 <- igraph::cluster_walktrap(graph_var_rank_VGluT2)$membership
PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_var_rank_VGluT2 <- factor(clusters_var_rank_VGluT2)
table(PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_var_rank_VGluT2)

# Using HVG from Var with jaccard-based weights
graph_var_jaccard_VGAT <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGAT,
                                        k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                        #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                        type = "jaccard", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                        subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_var_out_no_spikes_block_filt,
                                        assay.type = "logcounts",
                                        use.dimred = "PCA_HVG_var_logcounts")
clusters_var_jaccard_VGAT <- igraph::cluster_louvain(graph_var_jaccard_VGAT)$membership
PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_var_jaccard_VGAT <- factor(clusters_var_jaccard_VGAT)
table(PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_var_jaccard_VGAT)

graph_var_jaccard_VGluT2 <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGluT2,
                                          k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                          #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                          type = "jaccard", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                          subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_var_out_no_spikes_block_filt,
                                          assay.type = "logcounts",
                                          use.dimred = "PCA_HVG_var_logcounts")
clusters_var_jaccard_VGluT2 <- igraph::cluster_louvain(graph_var_jaccard_VGluT2)$membership
PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_var_jaccard_VGluT2 <- factor(clusters_var_jaccard_VGluT2)
table(PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_var_jaccard_VGluT2)
 
# Using HVG from CV2 with rank-based weights
graph_cv2_rank_VGAT <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGAT,
                                     k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                     #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                     type = "rank", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                     subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_cv2_out_no_spikes_filt,
                                     assay.type = "logcounts",
                                     use.dimred = "PCA_HVG_cv2_logcounts")
clusters_cv2_rank_VGAT <- igraph::cluster_walktrap(graph_cv2_rank_VGAT)$membership
PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_cv2_rank_VGAT <- factor(clusters_cv2_rank_VGAT)
table(PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_cv2_rank_VGAT)

graph_cv2_rank_VGluT2 <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGluT2,
                                       k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                       #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                       type = "rank", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                       subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_cv2_out_no_spikes_filt,
                                       assay.type = "logcounts",
                                       use.dimred = "PCA_HVG_cv2_logcounts")
clusters_cv2_rank_VGluT2 <- igraph::cluster_walktrap(graph_cv2_rank_VGluT2)$membership
PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_cv2_rank_VGluT2 <- factor(clusters_cv2_rank_VGluT2)
table(PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_cv2_rank_VGluT2)

# Using HVG from CV2 with jaccard-based weights
graph_cv2_jaccard_VGAT <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGAT,
                                        k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                        #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                        type = "jaccard", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                        subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_cv2_out_no_spikes_filt,
                                        assay.type = "logcounts",
                                        use.dimred = "PCA_HVG_cv2_logcounts")
clusters_cv2_jaccard_VGAT <- igraph::cluster_louvain(graph_cv2_jaccard_VGAT)$membership
PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_cv2_jaccard_VGAT <- factor(clusters_cv2_jaccard_VGAT)
table(PAG_sceset_qc_norm_filt_corr_VGAT$SNN_clusters_cv2_jaccard_VGAT)

graph_cv2_jaccard_VGluT2 <- buildSNNGraph(PAG_sceset_qc_norm_filt_corr_VGluT2,
                                          k = 10, # An integer scalar specifying the number of nearest neighbors to consider during graph construction.
                                          #d = 50, # An integer scalar specifying the number of dimensions to use for the search.
                                          type = "jaccard", # A string specifying the type of weighting scheme to use for shared neighbors. Try "rank" or "jaccard"
                                          subset.row = metadata(PAG_sceset_qc_norm_filt_corr)$hvg_cv2_out_no_spikes_filt,
                                          assay.type = "logcounts",
                                          use.dimred = "PCA_HVG_cv2_logcounts")
clusters_cv2_jaccard_VGluT2 <- igraph::cluster_louvain(graph_cv2_jaccard_VGluT2)$membership
PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_cv2_jaccard_VGluT2 <- factor(clusters_cv2_jaccard_VGluT2)
table(PAG_sceset_qc_norm_filt_corr_VGluT2$SNN_clusters_cv2_jaccard_VGluT2)
```

```{r}
# VGAT cells
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "PAG.arearegistration", shape_by = "VGAT_VGluT2_expression") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from Var with rank-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_var_rank_VGAT", text_by = "SNN_clusters_var_rank_VGAT") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from Var with jaccard-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_var_jaccard_VGAT", text_by = "SNN_clusters_var_jaccard_VGAT") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from CV2 with rank-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_cv2_rank_VGAT", text_by = "SNN_clusters_cv2_rank_VGAT") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from CV2 with jaccard-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGAT, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_cv2_jaccard_VGAT", text_by = "SNN_clusters_cv2_jaccard_VGAT") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")

# VGluT2 cells
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "PAG.arearegistration", shape_by = "VGAT_VGluT2_expression") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from Var with rank-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_var_rank_VGluT2", text_by = "SNN_clusters_var_rank_VGluT2") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from Var with jaccard-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_var_jaccard_VGluT2", text_by = "SNN_clusters_var_jaccard_VGluT2") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from CV2 with rank-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_cv2_rank_VGluT2", text_by = "SNN_clusters_cv2_rank_VGluT2") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
## Using HVG from CV2 with jaccard-based weights
plotReducedDim(PAG_sceset_qc_norm_filt_corr_VGluT2, dimred = "UMAP_var_logcounts_10_neighbors_0.05_min_dist",
               colour_by = "SNN_clusters_cv2_jaccard_VGluT2", text_by = "SNN_clusters_cv2_jaccard_VGluT2") + ggtitle("UMAP_var_logcounts_10_neighbors_0.05_min_dist")
```

### 6.1b.2 | Assessing cluster separation
```{r}
# VGAT
ratio_var_rank_VGAT <- clusterModularity(graph_var_rank_VGAT, clusters_var_rank_VGAT, as.ratio = TRUE)
ratio_var_rank_VGAT

ratio_var_jaccard_VGAT <- clusterModularity(graph_var_jaccard_VGAT, clusters_var_jaccard_VGAT, as.ratio = TRUE)
ratio_var_jaccard_VGAT

ratio_cv2_rank_VGAT <- clusterModularity(graph_cv2_rank_VGAT, clusters_cv2_rank_VGAT, as.ratio = TRUE)
ratio_cv2_rank_VGAT

ratio_cv2_jaccard_VGAT <- clusterModularity(graph_cv2_jaccard_VGAT, clusters_cv2_jaccard_VGAT, as.ratio = TRUE)
ratio_cv2_jaccard_VGAT

# VGluT2
ratio_var_rank_VGluT2 <- clusterModularity(graph_var_rank_VGluT2, clusters_var_rank_VGluT2, as.ratio = TRUE)
ratio_var_rank_VGluT2

ratio_var_jaccard_VGluT2 <- clusterModularity(graph_var_jaccard_VGluT2, clusters_var_jaccard_VGluT2, as.ratio = TRUE)
ratio_var_jaccard_VGluT2

ratio_cv2_rank_VGluT2 <- clusterModularity(graph_cv2_rank_VGluT2, clusters_cv2_rank_VGluT2, as.ratio = TRUE)
ratio_cv2_rank_VGluT2

ratio_cv2_jaccard_VGluT2 <- clusterModularity(graph_cv2_jaccard_VGluT2, clusters_cv2_jaccard_VGluT2, as.ratio = TRUE)
ratio_cv2_jaccard_VGluT2
```

```{r}
library(pheatmap)
# VGAT
pheatmap(log2(ratio_var_rank_VGAT + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_var_jaccard_VGAT + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_cv2_rank_VGAT + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_cv2_jaccard_VGAT + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

# VGluT2
pheatmap(log2(ratio_var_rank_VGluT2 + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_var_jaccard_VGluT2 + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_cv2_rank_VGluT2 + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))

pheatmap(log2(ratio_cv2_jaccard_VGluT2 + 1), 
         cluster_cols = FALSE, cluster_rows = FALSE,
         color = colorRampPalette(c("white", "blue"))(100))
         #col = rev(heat.colors(100)))
```

## Step 6.4 | Consensus clustering with SC3
### 6.4.10 | SC3 Clustering using ion channels genes
Here we will run the same clustering approach but using only ion channel genes instead of all the genes. First we load the list of ion channel genes and keep the ones that are present in our dataset:
```{r}
PAG_ionchannels <- PAG_scRNAseq_gene_lists$genes.ionchannels
sum(PAG_scRNAseq_gene_lists$genes.ionchannels != "") # Ion Channel genes present in the list

# Filter any genes not present in our dataset by comparing the gene names with the rownames of our expression matrix (which contains gene names)
PAG_ionchannels <- PAG_ionchannels[(PAG_ionchannels %in% rownames(PAG_sceset_qc))]
length(PAG_ionchannels) # Ion Channel genes present in the dataset
```

Generate a SCE object with only ion channel genes:
```{r}
# Keep only the rows that correspond to ion channel genes:
PAG_sceset_qc_ionchannels <- PAG_sceset_qc[rownames(PAG_sceset_qc) %in% PAG_ionchannels, ]
dim(PAG_sceset_qc_ionchannels)
```

Estimate K:
```{r}
PAG_sceset_qc_ionchannels <- sc3_estimate_k(PAG_sceset_qc_ionchannels)
metadata(PAG_sceset_qc_ionchannels)$sc3$k_estimation
```

Run SC3:
```{r}
rowData(PAG_sceset_qc_ionchannels)$feature_symbol <- rownames(PAG_sceset_qc_ionchannels)
PAG_sceset_qc_ionchannels <- sc3(PAG_sceset_qc_ionchannels, ks = 2:6, biology = TRUE)
```

Explore results interactively:
```{r}
library(shiny)
sc3_interactive(PAG_sceset_qc_ionchannels)
```

For k=2
```{r}
# Consensus Matrix
sc3_plot_consensus(PAG_sceset_qc_ionchannels, 
                   k = 2, 
                   show_pdata = "cell.type"
)

# DE Genes
sc3_plot_de_genes(PAG_sceset_qc_ionchannels, 
                 k = 2,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )

# Marker Genes
sc3_plot_markers(PAG_sceset_qc_ionchannels, 
                 k = 2,
                 auroc = 0.65,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )

```

Figures for k=3
```{r}
# Consensus Matrix
sc3_plot_consensus(PAG_sceset_qc_ionchannels, 
                   k = 3, 
                   show_pdata = "cell.type"
                   )

# DE Genes
sc3_plot_de_genes(PAG_sceset_qc_ionchannels, 
                 k = 3,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )

# Marker Genes
sc3_plot_markers(PAG_sceset_qc_ionchannels, 
                 k = 3,
                 auroc = 0.65,
                 p.val = 0.01,
                 show_pdata = c("cell.type", "PAG.areacollection")
                 )
```

### 6.4.11 | SC3 Clustering using Neuropeptides and Neuromodulators
Here we will run the same clustering approach but using only ion channel genes instead of all the genes. First we load the list of ion channel genes and keep the ones that are present in our dataset:
```{r}
PAG_Neuromodulators_Peptides <- PAG_scRNAseq_gene_lists$genes.NeuromodulatorsPeptides
length(PAG_Neuromodulators_Peptides) # This won't be the length of the $genes.ionchannels column but the one of the longest column of the .csv file (there will be a lot of empty cells).
#typeof(PAG_ionchannels)

# Filter any genes not present in our dataset by comparing the gene names with the rownames of our expression matrix (which contains gene names)
PAG_Neuromodulators_Peptides <- PAG_Neuromodulators_Peptides[(PAG_Neuromodulators_Peptides %in% rownames(PAG_sceset_qc))]
length(PAG_Neuromodulators_Peptides)
```

Generate a SCE object with only ion channel genes:
```{r}
# Keep only the rows that correspond to ion channel genes:
PAG_sceset_qc_Neuromodulators_Peptides <- PAG_sceset_qc[rownames(PAG_sceset_qc) %in% PAG_Neuromodulators_Peptides, ]
dim(PAG_sceset_qc_Neuromodulators_Peptides)
```

Estimate K:
```{r}
PAG_sceset_qc_Neuromodulators_Peptides <- sc3_estimate_k(PAG_sceset_qc_Neuromodulators_Peptides)
metadata(PAG_sceset_qc_Neuromodulators_Peptides)$sc3$k_estimation
```

Run SC3:
```{r}
rowData(PAG_sceset_qc_Neuromodulators_Peptides)$feature_symbol <- rownames(PAG_sceset_qc_Neuromodulators_Peptides)
PAG_sceset_qc_Neuromodulators_Peptides <- sc3(PAG_sceset_qc_Neuromodulators_Peptides, ks = 2:6, biology = TRUE)
```

Explore results interactively:
```{r}
sc3_interactive(PAG_sceset_qc_Neuromodulators_Peptides)
```

```{r}
# Consensus Matrix
sc3_plot_consensus(PAG_sceset_qc_Neuromodulators_Peptides, 
                   k = 2, 
                   show_pdata = "cell.type"
                   )

# DE Genes
sc3_plot_de_genes(PAG_sceset_qc_Neuromodulators_Peptides, 
                 k = 2,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )

# Marker Genes
sc3_plot_markers(PAG_sceset_qc_Neuromodulators_Peptides, 
                 k = 2,
                 auroc = 0.65,
                 p.val = 0.01,
                 show_pdata = "cell.type"
                 )
```

### 6.4.12 | SC3 Clustering ion channels genes instead of cells (need to transpose matrix in a new SCE object)
Run STEP 5.1.2 first, so you already have the subset of genes and the SCE object. Then transpose the SCE object with only ion channel genes you generated in the previous step:
```{r}
# Keep only the rows that correspond to ion channel genes:
dim(PAG_sceset_qc_ionchannels)
# Trying to transpose a SCEset as follows doesn't work, so we have to transpose the original data prior to creating the SCEset
# PAG_sceset_qc_ionchannels_T <- PAG_sceset_qc_ionchannels[colData(PAG_sceset_qc_ionchannels), rowData(PAG_sceset_qc_ionchannels)]
dim(PAG_sceset_qc_ionchannels_T)
```

```{r}
head(PAG_data)
head(PAG_metadata)
PAG_sceset_qc
PAG_sceset_transposed <- SingleCellExperiment(
  assays = list(counts = as.matrix(PAG_data)), 
  colData = PAG_metadata, 
  rowData = PAG_gene_information
)
```

Estimate K:
```{r}
PAG_sceset_qc_ionchannels <- sc3_estimate_k(PAG_sceset_qc_ionchannels)
metadata(PAG_sceset_qc_ionchannels)$sc3$k_estimation
```

Run SC3:
```{r}
rowData(PAG_sceset_qc_ionchannels)$feature_symbol <- rownames(PAG_sceset_qc_ionchannels)
PAG_sceset_qc_ionchannels <- sc3(PAG_sceset_qc_ionchannels, ks = 2:6, biology = TRUE)
```

Explore results interactively:
```{r}
sc3_interactive(PAG_sceset_qc_ionchannels)
```

### 6.4.13 | SC3 Clustering VGAT and VGluT2 cells separately
Do this when you are ready for downstream analysis, after quality checks.  
```{r}
# Is this the best way to subset the data? Probably not, there must be a way to do it without creating a new SCE object.
VGAT_sceset <- PAG_sceset[,grep("^[I]", colnames(PAG_sceset), value=TRUE)]

VGluT2_sceset <- PAG_sceset[,grep("^[E]", colnames(PAG_sceset), value=TRUE)]
```

## Step 6.7 | clusterExperiment [not yet implemented]
## Step 6.8 | bigSCale2
bigSCale is an analytical framework for big-scale single-cell data, available [here](https://github.com/iaconogi/bigSCale2).

```{r}
library(bigscale)
PAG_sceset_qc <- bigscale(PAG_sceset_qc)
```

OR, if you want to run it step by step:
```{r}
# Pre-process the data and generate the model:
PAG_sceset_qc = preProcess(PAG_sceset_qc)
PAG_sceset_qc = storeNormalized(PAG_sceset_qc) # stores normalized data
PAG_sceset_qc = setModel(PAG_sceset_qc) # computes the numerical model
PAG_sceset_qc = storeTransformed(PAG_sceset_qc) # stores transformed expression data (needed for some plots)
viewModel(PAG_sceset_qc) # check the model

# Compute the highly variable genes and we calculate cell to cell distances. 
PAG_sceset_qc = setODgenes(PAG_sceset_qc, min_ODscore=2.33) # min_ODscore is the Z-score treshold for selecting the genes (default 2.33). Increase it to be more stringent (less genes) and viceversa.
viewODgenes(PAG_sceset_qc) # visually inspect the selected highly variable genes

# Compute the distances and the t-SNE:
PAG_sceset_qc = setDistances(PAG_sceset_qc)
PAG_sceset_qc = storeTsne(PAG_sceset_qc)

# Cluster the data
PAG_sceset_qc = setClusters(PAG_sceset_qc)

# Make pseudotime analysis:
PAG_sceset_qc = storePseudo(PAG_sceset_qc)

# Compute the differential expression for all genes. Select speed.preset='slow' for maximum accuracy with long computational time.
PAG_sceset_qc = computeMarkers(PAG_sceset_qc, speed.preset='slow')

# Organize all the genes into markers and signatures of co-expression. cutoff is a Z-score which filters the genes and retains only those with significant changes of expression. Inrease it if you want to be more stringent or viceversa.
PAG_sceset_qc = setMarkers(PAG_sceset_qc, cutoff=3)

# Restore some matrices from the virtual memory to complete the analysis.
PAG_sceset_qc = restoreData(PAG_sceset_qc)
```

Plot of the clusters and signatures of coexpressed genes and visualise:

* The dendrogram representing how the cells are phenotypically organized and clustered.
* Colored bars representing the clusters, the library size (meant as a proxy to transcriptome size/complexity) and the pseudotime of the cells. An additional color bar is displayed for any user custom `colData()` (for example, sample batches, conditions and so on ...). For custom user `colData`, the color codes are automatically chosen upoen the type of data (numeric or factor).
* The clustered signatures of coexpressed genes alogside their size. Here, all the genes differentially expressed are organized in signatures of co-expressed genes.
```{r}
viewSignatures(PAG_sceset_qc)
```

We can also inspect the markers of a specific cluster, with markers of level 1 being the most specific to a given cluster.
```{r}
viewSignatures(PAG_sceset_qc, selected.cluster=2)
```

Barplots and violin plots of selected genes to visualize gene expression at single cell level with colored clusters:
```{r}
viewGeneBarPlot(PAG_sceset_qc, gene.list = c('Aqp4','Olig1','Thy1'))
```

```{r}
viewGeneViolin(PAG_sceset_qc, 'Asic4')
```

t-SNE and UMAP plots:

* If you want to color the cell according to some custom annotation you can pass a `factor` variable in place of a gene name. If you want to visualize a UMAP plot first compute the UMAP data with `sce=storeUMAP(sce)` and then `viewReduced(sce,method = 'UMAP')`
```{r}
viewReduced(PAG_sceset_qc) # to see t-SNE with clusters
viewReduced(PAG_sceset_qc, color.by = 'Stmn2') # to see t-SNE with gene expression
```

Browsing markers:
* To have a look to the markers found by bigscale we retrive `Mlist` from the single cell object. `Mlist` is a 2 dimensional list containing for each cluster the markers of the different levels. Let's inspect the markers of level 1 (most specific) of cluster 4. We will take advantage of the package `DT` for interactive visualization. Running the next command line we will see the markers specific to cluster 4 sorted from the highest (most significant) to the lowest (less significant) Zscore.
```{r}
Mlist = getMarkers(PAG_sceset_qc)
DT::datatable(Mlist[[4,1]])
```

Browsing signatures:
* Alternatevely to a cluster-based organization of the markers (as shown before, we have clusters and we have levels) there is also a more compact organization of markers into lists of co-expressed genes. This are the same lists shown with `viewSignatures(sce)`.
```{r}
Signatures = getSignatures(PAG_sceset_qc)
DT::datatable(Signatures[[1]])
```

## Step 6.9 | BackSPIN - biclustering hierarchical [MATLAB/Python]
The BackSPIN biclustering algorithm was developed by Amit Zeisel and is described in Zeisel et al. Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq Science 2015 (PMID: 25700174, doi: 10.1126/science.aaa1934). Please cite this paper if you use the BackSPIN algorithm in your work.

Original MATLAB implementation by Amit Zeisel. This repo contains a standalone command-line version of BackSPIN, implemented in Python by Gioele La Manno (https://github.com/linnarsson-lab/BackSPIN)

## Step 6.10 | SINCERA (hierarchical clustering) [not yet implemented]
It performs a gene-level z-score transformation before doing clustering.
It can also identify k as the minimum height of the hierarchical tree that generates no more than a specified number of singleton clusters (clusters containing only 1 cell).

SINCERA: a computational pipeline for SINgle CEll RNA-seq profiling Analysis, can be used for processing scRNA-seq data from a whole organ or sorted cells. The pipeline supports the analysis for: 1) the distinction and identification of major cell types; 2) the identification of cell type specific gene signatures; and 3) the determination of driving forces of given cell types.

Website: https://research.cchmc.org/pbge/sincera.html
Paper: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004575

## Step 6.11 | pcaReduce [not yet implemented, but very stochastic and does not provide a stable result] 
pcaReduce combines PCA, k-means and iterative hierarchical clustering. Starting from a large number of clusters pcaReduce iteratively merges similar clusters; after each merging event it removes the principle component explaning the least variance in the data.

pcaReduce operates directly on the expression matrix. It is recommended to use a gene filter and log transformation before running pcaReduce. We will use the default SC3 gene filter (note that the exprs slot of a scater object is log-transformed by default).
```{r}
# use the same gene filter as in SC3
library(pcaReduce) # You need to install PCAReduce first
input <- logcounts(PAG_sceset_qc[rowData(PAG_sceset_qc)$sc3_gene_filter, ])
```

There are several parameters used by pcaReduce: nbt defines a number of pcaReduce runs (it is stochastic and may have different solutions after different runs). q defines number of dimensions to start clustering with. The output will contain partitions for all k from 2 to q+1. method defines a method used for clustering. S - to perform sampling based merging, M - to perform merging based on largest probability.
```{r}
# run pcaReduce 1 time creating hierarchies from 1 to 30 clusters
pca.red <- PCAreduce(t(input), nbt = 1, q = 30, method = 'S')[[1]]
colData(PAG_sceset_qc)$pcaReduce <- as.character(pca.red[,32 - 10])
plotPCA(PAG_sceset_qc, colour_by = "pcaReduce")
```

# STEP 7 | Differential Expression Analysis

## Step 7.2 | Differential Expression workflow with DESeq2 and edgeR (EBI-EMBL Course, Charlotte Soneson)
Count-based statistical methods such as _DESeq2_ (Love, Huber, and Anders 2014), _edgeR_ (Robinson, McCarthy, and Smyth 2009), _limma_ with the _voom_ method (Law et al. 2014), _DSS_ (Wu, Wang, and Wu 2013), _EBSeq_ (Leng et al. 2013), _BaySeq_ (Hardcastle and Kelly 2010) and _DEXSeq_ (Anders, Reyes, and Huber 2012) expect input data as obtained, e.g., from RNA-seq or another high-throughput sequencing experiment in the form of a matrix of integer values, or `counts`. The value in the i-th row and the j-th column of the matrix tells how many reads (or fragments, for paired-end RNA-seq) have been assigned to feature i in sample j. For RNA-seq, a feature is typically a gene, a transcript or an exon.

The fact that the values in the matrix are counts of sequencing reads (in the case of single-end sequencing) or fragments (for paired-end sequencing) is important for the count-based statistical models, e.g. _DESeq2_ or _edgeR_, as only the counts allow assessing the measurement precision correctly. It is important to _never_ provide counts that have been normalised for sequencing depth/library size to these packages, as the statistical model is most powerful when applied to counts, and is designed to account for library size differences internally.

The most common model of RNASeq data is the negative binomial model. However, a raw negative binomial model does not fit full-length transcript data as well due to the high dropout rates relative to the non-zero read counts. For this type of data a variety of zero-inflated negative binomial models have been proposed (e.g. MAST, SCDE). The model that makes more biological sense and with more experimental support (Kim and Marioni, 2013) is the Poisson-Beta distribution, based on a mechanistic model of transcriptional bursting.

An alternative to using actual counts of reads or fragments aligned to the genome is to use estimated counts from software that use pseudo-alignment to the transcriptome. Since these represent expected counts rather than observed counts they are not necessarily integers, and thus may need to be rounded before they are fed to the count-based pipelines. In any case, we start the DE workflow from a gene-vs-sample matrix, where raw reads have been quality controlled and gene expression quantified, and we make sure to add a new assay slot that will contain rounded raw counts, as some of the methods may only work if integers are provided:
```{r}
# Round the counts to ensure they are integers:
assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded") <- round(assay(PAG_sceset_qc_norm_filt_corr_clust, "counts")) # NOT the normalised counts
# Check you have rounded counts in your object:
assayNames(PAG_sceset_qc_norm_filt_corr_clust)
```

```{r}
# Quickly check the millions of reads that were uniquely aligned to genes (the second argument of round tells how many decimal points to keep).
round(colSums(assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded")) / 1e6, 1)
```

Once we have a gene-level count matrix and the relevant metadata we can branch out and use a variety of Bioconductor packages for exploration and DE analysis. Each of the packages we will use for differential expression has a specific class of object used to store the summarization of the RNA-seq experiment and the intermediate quantities that are calculated during the statistical analysis of the data. `DESeq2` uses a `DESeqDataSet` and `edgeR` uses a `DGEList`.

### 7.2.1 | DEseq2 and the DESeqDatSet
In _DESeq2_, the custom class is called _DESeqDataSet_. It is built on top of the _SummarizedExperiment_ class, and it is easy to convert _SummarizedExperiment_ objects into _DESeqDataSet_ objects. 

* One of the two main differences compared to a _SummarizedExperiment_ object is that the `assay` slot is instead accessed using the `counts` accessor function, and the _DESeqDataSet_ class enforces that the values in this matrix are non-negative integers.
* A second difference is that the _DESeqDataSet_ has an associated _design formula_. The experimental design is specified at the beginning of the analysis, as it will inform many of the _DESeq2_ functions how to treat the samples in the analysis (one exception is the size factor estimation, i.e., the adjustment for differing library sizes, which does not depend on the design formula). The design formula tells which columns in the sample information table (`colData`) specify the experimental design and how these factors should be used in the analysis.

We have two types of cells, _VGAT_ and _VGluT2_, across four different anatomical subdivisions.
```{r}
table(PAG_sceset_qc_norm_filt_corr_clust$cell.type, PAG_sceset_qc_norm_filt_corr_clust$PAG.area)
table(PAG_sceset_qc_norm_filt_corr_clust$celltype_PAGarea)
```

__Note__: it could be helpful for us if the first level of a factor is the reference level (e.g. control, or untreated samples). The reason is that by specifying this, functions further in the pipeline can be used and will give comparisons such as "treatment vs control", without needing to specify additional arguments. However, in our case it doesn't make much sense, as we are not comparing treatments or conditions in a time sequence, and there is no clear reference level for `cell.type` or `mouse.id`. We can, however, relevel the `PAG.area` so that `dmpag` and not `dlpag` is the top level, as we did in _Part I_ of the pipeline. We don't need to run this again.

We want to find differences in expression of genes (1) across *cell-types irrespective of PAG subdivision*, (2) across *PAG subdivisions irrespective of cell-type*, (3) across *subdivisions within a given cell-type*, and (4) across *cell-types within a given subdivision*. Importantly, each transgenic mouse (`mouse.id`) only allows us to capture one of the two cell-types, so the _mouse ID_ will be completely confounded with one or the other _cell type_. We would like to control for differences due to `mouse.sex` and `batch.processing`, as we are not interested in differences due to these. 

* An example of a design to explore differences in gene expression associated to `cell.type` but controling for differences between `mouse.sex` is obtained by writing `~ mouse.sex + cell.type`. By including `mouse.sex`, terms will be added to the model which account for differences across mice, and by adding `cell.type` we get a single term which explains the differences between cell types. Make sure the levels in the factors only contain letters, numbers, underscores and periods. The variable of interest should go at the end of the formula (the `results` function will by default pull the `cell.type` results unless `contrast` or `name` arguments are specified).
* We use R's formula notation to express any fixed-effects experimental design for _edgeR_ or _DESeq2_. If the research aim is to determine for which genes the effect of a treatment is different across groups, then interaction terms can be included and tested using a design such as `~ group + treatment + group:treatment`. See the vignettes of `DESeq2` and `edgeR` for more examples.

__Contrasts__: A contrast is a linear combination of estimated log2 fold changes, which can be used to test if differences between groups are equal to zero. The simplest use case for contrasts is an experimental design containing a factor with three levels, say A, B and C. Contrasts enable the user to generate results for all 3 possible differences: log2 fold change of B vs A, of C vs A, and of C vs B.

__Interactions__: Interaction terms can be added to the design formula, in order to test, for example, if the log2 fold change attributable to a given condition is different based on another factor, for example if the condition effect differs across genotype. However, nothe the following: Many users begin to add interaction terms to the design formula, when in fact a much simpler approach would give all the results tables that are desired. If the comparisons of interest are, for example, the effect of a condition for different sets of samples, a simpler approach than adding interaction terms explicitly to the design formula is to perform the following steps:

* combine the factors of interest into a single factor with all combinations of the original factors
* change the design to include just this factor, e.g. `~ group`

Using this design is similar to adding an interaction term, in that it models multiple condition effects which can be easily extracted with results. Suppose we have two factors `genotype` (with values I, II, and III) and `condition` (with values A and B), and we want to extract the condition effect specifically for each genotype. To adapt this to our data, we could think of this as the following: our two factors will be `cell.type` (with values _VGAT_ and _VGluT2_) and `PAG.area` (with values _dmpag_, _dlpag_, _lpag_, _vlpag_). To obtain the `PAG.area` effect for a specific `cell.type` we could merge both factors into a combined one (`celltype_PAGarea`), and use that in the model instead of the other two with an interaction term. We will do both approaches.

__A note on multi-factor designs:__
Experiments with more than one factor influencing the counts can be analyzed using design formula that include the additional variables. In fact, `DESeq2` can analyze any possible experimental design that can be expressed with fixed effects terms (multiple factors, designs with interactions, designs with continuous variables, splines, and so on are all possible).

By adding variables to the design, one can control for additional variation in the counts. For example, if the condition samples are balanced across experimental batches, by including the `batch` factor to the design, one can increase the sensitivity for finding differences due to `condition`. There are multiple ways to analyze experiments when the additional variables are of interest and not just controlling factors.

We will now generate a `DESeqDataSet` from a count matrix and a table of sample information (metadata) and specify different designs we want to test:
```{r}
library(DESeq2)
#### Design 0 ####
PAG_DESeq_set_0 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded"), 
                                          colData = colData(PAG_sceset_qc_norm_filt_corr_clust),
                                          design = ~ mouse.sex + batch.processing + batch.sequencing_round)

#### Design 1 ####
PAG_DESeq_set_1 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded"), 
                                          colData = colData(PAG_sceset_qc_norm_filt_corr_clust),
                                          design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type)

#### Design 2 ####
PAG_DESeq_set_2 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded"),
                                          colData = colData(PAG_sceset_qc_norm_filt_corr_clust),
                                          design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.area)

#### Design 1&2 ####
PAG_DESeq_set_12 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded"), 
                                           colData = colData(PAG_sceset_qc_norm_filt_corr_clust),
                                           design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.area)

#### Design 1&2 with interaction ####
PAG_DESeq_set_12a <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded"), 
                                            colData = colData(PAG_sceset_qc_norm_filt_corr_clust),
                                            design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.area + cell.type:PAG.area)

#### Design 3 ####
PAG_DESeq_set_3 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded"), 
                                          colData = colData(PAG_sceset_qc_norm_filt_corr_clust),
                                          design = ~ mouse.sex + batch.processing + batch.sequencing_round + celltype_PAGarea)
```

Initial designs we tried
```{r}
# library(DESeq2)
# #### Design 0 ####
# PAG_DESeq_set_0 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                           colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                           design = ~ mouse.sex + batch.processing + batch.sequencing_round)
# 
# #### Design 1 ####
# PAG_DESeq_set_1 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                           colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                           design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type)
# 
# #### Design 2 ####
# PAG_DESeq_set_2 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"),
#                                           colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                           design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.arearegistration)
# 
# #### Design 3 ####
# PAG_DESeq_set_3 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                           colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                           design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.APaxis)
# 
# #### Design 1&2 ####
# PAG_DESeq_set_12 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                            colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                            design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.arearegistration)
# 
# #### Design 1&2 with interaction ####
# PAG_DESeq_set_12a <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                             colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                             design = ~ mouse.sex + batch.processing + batch.sequencing_round + group)
# 
# #### Design 1&3 ####
# PAG_DESeq_set_13 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                            colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                            design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.APaxis)
# 
# #### Design 1&3 with interaction ####
# PAG_DESeq_set_13b <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                             colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                             design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.APaxis + cell.type:PAG.APaxis)
# 
# #### Design 2&3 ####
# PAG_DESeq_set_23 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                            colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                            design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.arearegistration + PAG.APaxis)
# 
# #### Design 2&3 with interaction #### --> Can't be done as PAG.APaxis:PAG.arearegistrations seems to be a linear combination of the others
# #PAG_DESeq_set_23c <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                             #colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                             #design = ~ mouse.sex + batch.processing + batch.sequencing_round + PAG.arearegistration + PAG.APaxis + PAG.APaxis:PAG.arearegistration)
# 
# #### Design 1&2&3 ####
# PAG_DESeq_set_123 <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                             colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                             design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.arearegistration + PAG.APaxis)
# 
# #### Design 1&2&3 with interaction ####
# PAG_DESeq_set_123ab <- DESeqDataSetFromMatrix(countData = assay(PAG_sceset_qc_norm_filt_corr, "counts_rounded"), 
#                                                colData = colData(PAG_sceset_qc_norm_filt_corr),
#                                                design = ~ mouse.sex + batch.processing + batch.sequencing_round + cell.type + PAG.arearegistration + PAG.APaxis + cell.type:PAG.arearegistration + cell.type:PAG.APaxis)
```

There are two separate analysis paths we can follow from here: visual exploration of sample relationships (transformation, distance calculation, plotting) and statistical testing for differences attributable to our conditions. Importantly, the statistical testing methods rely on original count data (not scaled or transformed) for calculating the precision of measurements. However, for visualization and exploratory analysis, transformed counts are typically more suitable. Thus, it is critical to separate the two workflows and use the appropriate input data for each of them.

#### Exploratory analysis and visualisation
Many common statistical methods for exploratory analysis of multidimensional data, for example clustering and principal components analysis (PCA), work best for data that generally has the same range of variance at different ranges of the mean values. When the expected amount of variance is approximately the same across different mean values, the data is said to be _homoskedastic_. For RNA-seq raw counts, however, the variance grows with the mean. For example, if one performs PCA directly on a matrix of size-factor-normalised read counts, the result typically depends only on the few most strongly expressed genes because they show the largest absolute differences between samples. A simple and often used strategy to avoid this is to take the logarithm of the normalised count values plus a small pseudocount; however, now the genes with the very lowest counts will tend to dominate the results because, due to the strong Poisson noise inherent to small count values, and the fact that the logarithm amplifies differences for the smallest values, these low count genes will show the strongest relative differences between samples.

As a solution, _DESeq2_ offers two transformations for count data that stabilize the variance across the mean: the _variance stabilizing transformation_ (`VST`) for negative binomial data with a dispersion-mean trend (Anders and Huber 2010), implemented in the `vst` function, and the _regularized-logarithm_ transformation (`rlog`) (Love, Huber, and Anders 2014). These have slightly different implementations, discussed a bit in the _DESeq2_ paper and in the vignette, but a similar goal of stabilizing the variance across the range of values. Both produce log2-like values for high counts. For genes with high counts, both the `VST` and the `rlog` will give similar results to the ordinary log2 transformation of normalised counts. For genes with lower counts, however, the values are shrunken towards a middle value. The VST or rlog-transformed data then become approximately _homoskedastic_ (more flat trend in the `meanSdPlot`), and can be used directly for computing distances between samples, making PCA plots, or as input to downstream methods which perform best with homoskedastic data. The VST transformation should be used for datasets with n > 30. We will therefore use the variance stabilizing transformation implemented with the `vst` function.
```{r}
start_time <- Sys.time() # Takes around 30min
PAG_DESeq_vsd <- DESeq2::vst(PAG_DESeq_set, blind = FALSE) 
end_time <- Sys.time()
end_time - start_time
# see ?varianceStabilizingTransformation for choice of blind

# This returns a DESeqTransform object containing the column metadata attached to the DESeqDataSet:
class(PAG_DESeq_vsd)
head(assay(PAG_DESeq_vsd), 3)
head(colData(PAG_DESeq_vsd))
```

Specifying `blind = FALSE` means that differences between `mouse.id` and `cell.type` (and the rest of the variables in the _design_) will not contribute to the expected variance-mean trend of the experiment. The experimental design is not used directly in the transformation, only in estimating the global amount of variability in the counts. For a fully unsupervised transformation, one can set `blind = TRUE` (which is the default).

***
**Sample distances:**
A useful first step in an RNA-seq analysis is often to assess overall similarity between samples: Which samples are similar to each other, which are different? Does this fit to the expectation from the experiment's design?

We can use the R function `dist` to calculate the Euclidean distance between samples. To ensure we have a roughly equal contribution from all genes, we use it on the _VST_ data. We need to transpose the matrix of values using `t`, because the `dist` function expects the different samples to be rows of its argument, and different dimensions (here, genes) to be columns.
```{r}
sampleDists <- dist(t(assay(PAG_DESeq_vsd)))
head(sampleDists)
```

We visualize the distances in a heatmap in a figure below, using the function `pheatmap` from the `pheatmap` package. In order to plot the sample distance matrix with the rows/columns arranged by the distances in our distance matrix, we manually provide `sampleDists` to the `clustering_distance` argument of the `pheatmap` function. Otherwise the `pheatmap` function would assume that the matrix contains the data values themselves, and would calculate distances between the rows/columns of the distance matrix, which is not desired. We also manually specify a blue color palette using the `colorRampPalette` function from the `RColorBrewer` package.
```{r}
library("pheatmap")
library("RColorBrewer")
sampleDistMatrix <- as.matrix(sampleDists)
rownames(sampleDistMatrix) <- paste(PAG_DESeq_vsd$group)
colnames(sampleDistMatrix) <- NULL
colors <- colorRampPalette(rev(brewer.pal(9, "Blues")))(255)
pheatmap(sampleDistMatrix,
         clustering_distance_rows = sampleDists,
         clustering_distance_cols = sampleDists,
         col = colors)
```

***
**PCA plot:**
One way to visualize sample-to-sample distances is a principal components analysis (PCA). In this ordination method, the data points (here, the samples) are projected onto the 2D plane such that they spread out in the two directions that explain most of the differences. The x-axis (the first principal component, or PC1) is the direction that separates the data points the most (i.e., the direction with the largest variance). The y-axis (the second principal component, or PC2) represents the direction with largest variance subject to the constraint that it must be orthogonal to the first direction. The percent of the total variance that is contained in the direction is printed in the axis label. Note that these percentages do not sum to 100%, because there are more dimensions that contain the remaining variance (although each of these remaining dimensions will explain less than the two that we see).
```{r}
DESeq2::plotPCA(PAG_DESeq_vsd, intgroup = "mouse.id")
DESeq2::plotPCA(PAG_DESeq_vsd, intgroup = "mouse.sex")
DESeq2::plotPCA(PAG_DESeq_vsd, intgroup = "batch.processing")
DESeq2::plotPCA(PAG_DESeq_vsd, intgroup = "batch.sequencing_round")
DESeq2::plotPCA(PAG_DESeq_vsd, intgroup = "PAG.PaxinosAPmanual")
DESeq2::plotPCA(PAG_DESeq_vsd, intgroup = "cell.type")
DESeq2::plotPCA(PAG_DESeq_vsd, intgroup = "PAG.area")
DESeq2::plotPCA(PAG_DESeq_vsd, intgroup = "group")
```

#### Differential expression testing with DESeq2
As we have already specified an experimental design when we created the _DESeqDataSet_, we can run the differential expression pipeline on the raw counts with a single call to the function `DESeq`. This function will carry out: the estimation of size factors (controlling for differences in the sequencing depth of the samples), the estimation of dispersion values for each gene, and fitting a generalized linear model. A _DESeqDataSet_ is returned that contains all the fitted parameters within it, and we can plot the estimated dispersions from it. We have several designs to test, so this will take a long time. To speed things up a bit, we will paralellise the analysis and filter lowly expressed genes. We have around 50 samples per `cell.type` per `PAG.area`, so if we are a bit conservative we keep only genes with at least 5 counts in at least 5 cells (around 10% of each condition). Recommendations for use of `DESeq2` for single-cell datasets are available in the `DESeq2` vignette, specially with regards to additional arguments to set in the `DEseq` function.
```{r}
#### Design 0 ####
keep_0 <- rowSums(counts(PAG_DESeq_set_0) >= 5) >= 5 # Identify genes with 5 copies in at least 5 cells.
summary(keep_0)
PAG_DESeq_set_0 <- PAG_DESeq_set_0[keep_0,] # Remove lowly expressed genes.

start_time <- Sys.time() # Takes around 1.40 hours
PAG_DESeq_set_0 <- DESeq2::DESeq(PAG_DESeq_set_0, test = "LRT", sfType = "poscounts", reduced = ~ 1, 
                                 minReplicatesForReplace = Inf, useT = TRUE, minmu = 1e-6, parallel = TRUE)
DESeq2::plotDispEsts(PAG_DESeq_set_0)
end_time <- Sys.time()
end_time - start_time
saveRDS(PAG_DESeq_set_0, file = "PAG_DESeq_set_0.rds")

#### Design 1 ####
keep_1 <- rowSums(counts(PAG_DESeq_set_1) >= 5) >= 5 # Identify genes with 5 copies in at least 5 cells.
summary(keep_1)
PAG_DESeq_set_1 <- PAG_DESeq_set_1[keep_1,] # Remove lowly expressed genes.

start_time <- Sys.time() # Takes around 2.07 hours
PAG_DESeq_set_1 <- DESeq2::DESeq(PAG_DESeq_set_1, test = "LRT", sfType = "poscounts", reduced = ~ mouse.sex + batch.processing + batch.sequencing_round, 
                                 minReplicatesForReplace = Inf, useT = TRUE, minmu = 1e-6, parallel = TRUE)
DESeq2::plotDispEsts(PAG_DESeq_set_1)
end_time <- Sys.time()
end_time - start_time
saveRDS(PAG_DESeq_set_1, file = "PAG_DESeq_set_1.rds")

#### Design 2 ####
keep_2 <- rowSums(counts(PAG_DESeq_set_2) >= 5) >= 5 # Identify genes with 5 copies in at least 5 cells.
summary(keep_2)
PAG_DESeq_set_2 <- PAG_DESeq_set_2[keep_2,] # Remove lowly expressed genes.

start_time <- Sys.time() # Takes around 2.33 hours
PAG_DESeq_set_2 <- DESeq2::DESeq(PAG_DESeq_set_2, test = "LRT", sfType = "poscounts", reduced = ~ mouse.sex + batch.processing + batch.sequencing_round,
                                 minReplicatesForReplace = Inf, useT = TRUE, minmu = 1e-6, parallel = TRUE)
DESeq2::plotDispEsts(PAG_DESeq_set_2)
end_time <- Sys.time()
end_time - start_time
saveRDS(PAG_DESeq_set_2, file = "PAG_DESeq_set_2.rds")

#### Design 1&2 ####
keep_12 <- rowSums(counts(PAG_DESeq_set_12) >= 5) >= 5 # Identify genes with 5 copies in at least 5 cells.
summary(keep_12)
PAG_DESeq_set_12 <- PAG_DESeq_set_12[keep_12,] # Remove lowly expressed genes.

start_time <- Sys.time() # Takes around 2.42 hours
PAG_DESeq_set_12 <- DESeq2::DESeq(PAG_DESeq_set_12, test = "LRT", sfType = "poscounts", reduced = ~ mouse.sex + batch.processing + batch.sequencing_round,
                                  minReplicatesForReplace = Inf, useT = TRUE, minmu = 1e-6, parallel = TRUE)
DESeq2::plotDispEsts(PAG_DESeq_set_12)
end_time <- Sys.time()
end_time - start_time
saveRDS(PAG_DESeq_set_12, file = "PAG_DESeq_set_12.rds")

#### Design 1&2 with interaction ####
keep_12a <- rowSums(counts(PAG_DESeq_set_12a) >= 5) >= 5 # Identify genes with 5 copies in at least 5 cells.
summary(keep_12a)
PAG_DESeq_set_12a <- PAG_DESeq_set_12a[keep_12a,] # Remove lowly expressed genes.

start_time <- Sys.time() # Takes around 2.68 hours
PAG_DESeq_set_12a <- DESeq2::DESeq(PAG_DESeq_set_12a, test = "LRT", sfType = "poscounts", reduced = ~ mouse.sex + batch.processing + batch.sequencing_round,
                                   minReplicatesForReplace = Inf, useT = TRUE, minmu = 1e-6, parallel = TRUE)
DESeq2::plotDispEsts(PAG_DESeq_set_12a)
end_time <- Sys.time()
end_time - start_time
saveRDS(PAG_DESeq_set_12a, file = "PAG_DESeq_set_12a.rds")

#### Design 3 ####
keep_3 <- rowSums(counts(PAG_DESeq_set_3) >= 5) >= 5 # Identify genes with 5 copies in at least 5 cells.
summary(keep_3)
PAG_DESeq_set_3 <- PAG_DESeq_set_3[keep_3,] # Remove lowly expressed genes.

start_time <- Sys.time() # Takes around 2.65 hours
PAG_DESeq_set_3 <- DESeq2::DESeq(PAG_DESeq_set_3, test = "LRT", sfType = "poscounts", reduced = ~ mouse.sex + batch.processing + batch.sequencing_round,
                                 minReplicatesForReplace = Inf, useT = TRUE, minmu = 1e-6, parallel = TRUE)
DESeq2::plotDispEsts(PAG_DESeq_set_3)
end_time <- Sys.time()
end_time - start_time
saveRDS(PAG_DESeq_set_3, file = "PAG_DESeq_set_3.rds")

# Once you have run DESeq, make sure you save the resulting file at every step so you don't need to run it again even if it crashes.
print("Saved!")
```

Plotting the dispersion estimates is a useful diagnostic. A typical dispersion plot can be found in the `DESeq2` vignette, with the final estimates shrunk from the gene-wise estimates towards the fitted estimates. Some gene-wise estimates are flagged as outliers and not shrunk towards the fitted value (this outlier detection is described in the manual page for `estimateDispersionsMAP`). The amount of shrinkage can be more or less than seen here, depending on the sample size, the number of coefficients, the row mean and the variability of the gene-wise estimates.

#### Examining and Plotting the results
Results tables are generated using the function results, which extracts a results table with log2 fold changes, _p_ values and adjusted _p_ values. With no additional arguments to _results_, the log2 fold change and Wald test _p value_ will be for the last variable in the design formula, and if this is a factor, the comparison will be the last level of this variable over the reference level. However, the order of the variables of the design do not matter so long as the user specifies the comparison to build a results table for, using the `name` or `contrast` arguments of results.

Next, calling `results` without any arguments will extract the estimated log2-fold changes and _p values_ for the last variable in the `design` formula. If there are more than 2 levels for this variable, `results` will extract the results table for a comparison of the last level over the first level.
```{r}
resultsNames(PAG_DESeq_set_1)
levels(PAG_sceset_qc_norm_filt_corr_clust$cell.type)
PAG_DESeq_results_1_wald <- DESeq2::results(PAG_DESeq_set_1, 
                                            contrast = c("cell.type", "VGluT2", "VGAT"), 
                                            test = "Wald",
                                            lfcThreshold = 0, alpha = 0.05, pAdjustMethod = "BH", parallel = TRUE)
head(PAG_DESeq_results_1_wald)
#mcols(PAG_DESeq_results_1, use.names = TRUE)
DESeq2::plotMA(PAG_DESeq_results_1_wald, ylim = c(-5, 5))
```

`PAG_DESeq_results_0` is a _DataFrame_ object, so it contains metadata with information:

* The first column, `baseMean`, is a just the average of the normalised count values, dividing by size factors, taken over all samples in the _DESeqDataSet_. The remaining four columns refer to a specific contrast, for example the comparison of the `VGAT` level over the `VGluT2` level for the factor variable `cell.type`.
* The column `log2FoldChange` is the effect size estimate. It tells us how much the gene's expression seems to have changed due to our specified codition. This value is reported on a logarithmic scale to base 2: for example, a log2 fold change of 1.5 means that the gene's expression is increased by a multiplicative factor of `2^1.5 ~ 2.82`.
* Of course, this estimate has an uncertainty associated with it, which is available in the column `lfcSE`, the standard error estimate for the log2 fold change estimate. 
* We can also express the uncertainty of a particular effect size estimate as the result of a statistical test. The purpose of a test for differential expression is to test whether the data provide sufficient evidence to conclude that this value is really different from zero. _DESeq2_ performs for each gene a hypothesis test to see whether evidence is sufficient to decide against the null hypothesis that there is zero effect of the treatment on the gene and that the observed difference between treatment and control was merely caused by experimental variability (i.e., the type of variability that you can expect between different samples in the same treatment group). As usual in statistics, the result of this test is reported as a _p value_, and it is found in the column `pvalue`. Remember that a _p value_ indicates the probability that an effect as strong as the observed one, or even stronger, would be seen under the situation described by the null hypothesis.

We can also summarize the results as follows:
```{r}
summary(PAG_DESeq_results_1_wald)
hist(PAG_DESeq_results_1_wald$pvalue)

## We can also add a couple of extra columns that will be useful for the interactive visualization later
PAG_DESeq_results_1_wald$log10BaseMean <- log10(PAG_DESeq_results_1_wald$baseMean)
PAG_DESeq_results_1_wald$mlog10PValue <- -log10(PAG_DESeq_results_1_wald$pvalue)
rowData(PAG_DESeq_set_1)$log10Dispersion <- log10(rowData(PAG_DESeq_set_3)$dispersion)
rowData(PAG_DESeq_set_1)$DESeq2_VGluT2_VGAT <- PAG_DESeq_results_1_wald # Or any other comparison you called using results
```

There are two ways to be more strict about which set of genes are considered significant:
* Lower the false discovery rate threshold (the threshold on `padj` in the results table)
* Raise the log2 fold change threshold from 0 using the `lfcThreshold` argument of _results_

If we lower the false discovery rate threshold, we should also tell this value to `results()` when we run it, so that the function will use an alternative threshold for the optimal independent filtering step:
```{r}
PAG_DESeq_results_3_05 <- DESeq2::results(PAG_DESeq_set_3, alpha = 0.05)
table(PAG_DESeq_results_1_wald$padj < 0.05)
```

If we want to raise the log2 fold change threshold, so that we test for genes that show more substantial changes due to our condition of interest, we simply supply a value on the log2 scale. For example, by specifying `lfcThreshold = 1`, we test for genes that show significant effects of treatment on gene counts more than doubling or less than halving, because `2^1 = 2`.
```{r}
PAG_DESeq_results_1_LFC1 <- results(PAG_DESeq_set_1, lfcThreshold = 1)
summary(PAG_DESeq_results_1_LFC1)
table(PAG_DESeq_results_1_LFC1$padj < 0.1)
```

Sometimes a subset of the p values in `DESeq_results` will be `NA` (not available). This is DESeq's way of reporting that all counts for this gene were zero, and hence no test was applied. In addition, _p values_ can be assigned `NA` if the gene was excluded from analysis because it contained an extreme count outlier. For more information, see the outlier detection section of the DESeq2 vignette.

***
__Multiple Testing__
In high-throughput biology, we are careful to not use the _p values_ directly as evidence against the null hypothesis, but to correct for multiple testing. _DESeq2_ and _edgeR_ use the Benjamini-Hochberg (BH) adjustment (Benjamini and Hochberg 1995) as implemented in the base R `p.adjust` function; in brief, this method calculates for each gene an adjusted p value that answers the following question: if one called significant all genes with an adjusted p value less than or equal to this gene's adjusted p value threshold, what would be the fraction of false positives (the false discovery rate, FDR) among them, in the sense of the calculation outlined above? These values, called the BH-adjusted p values, are given in the column `padj` of the `res` object from _DESeq2_, and in the `FDR` column in the `TopTags` object from _edgeR_.

The FDR is a useful statistic for many high-throughput experiments, as we are often interested in reporting or focusing on a set of interesting genes, and we would like to put an upper bound on the percent of false positives in this set. Hence, if we consider a fraction of 5% false positives acceptable, we can consider all genes with an adjusted p value below 5% = 0.05 as significant. 
```{r}
sum(PAG_DESeq_results_1_wald$padj < 0.05, na.rm = TRUE)
```

We subset the results table to these genes and then sort it by the log2 fold change estimate to get the significant genes:
```{r}
resSig <- subset(PAG_DESeq_results_1_wald, padj < 0.05)
head(resSig[ order(resSig$log2FoldChange), ]) #  with the strongest down-regulation
head(resSig[ order(resSig$log2FoldChange, decreasing = TRUE), ]) # with the strongest up-regulation
```

#### Plotting results
***
__Counts plot__
A quick way to visualize the counts for a particular gene is to use the `plotCounts` function that takes as arguments the `DESeqDataSet`, a gene name, and the group over which to plot the counts.
```{r}
plotCounts(PAG_DESeq_set_1, gene = "Col6a6", intgroup = "cell.type", 
           normalized = TRUE, transform = FALSE)

topGene <- rownames(PAG_DESeq_results_1_wald)[which.min(PAG_DESeq_results_1_wald$padj)]
plotCounts(PAG_DESeq_set_1, gene = topGene, intgroup=c("cell.type"))
```

We can also make custom plots using the `ggplot` function from the `ggplot2` package.
```{r}
library("ggbeeswarm")
geneCounts <- plotCounts(PAG_DESeq_set_1, gene = topGene, intgroup = c("PAG.area","cell.type"),
                         returnData = TRUE)

ggplot(geneCounts, aes(x = PAG.area, y = count, color = cell.type)) +
  scale_y_log10() +  geom_beeswarm(cex = 3)

ggplot(geneCounts, aes(x = PAG.area, y = count, color = cell.type, group = cell.type)) +
  scale_y_log10() + geom_point(size = 3) + geom_line()
```

***
__MA-plot__
An MA-plot (Dudoit et al. 2002) provides a useful overview for the distribution of the estimated coefficients in the model, e.g. the comparisons of interest, across all genes. On the y-axis, the "M" stands for "minus" - subtraction of log values is equivalent to the log of the ratio - and on the x-axis, the "A" stands for "average." You may hear this plot also referred to as a mean-difference plot, or a Bland-Altman plot. Each gene is represented with a dot. Genes with an adjusted p value below a threshold (the default with DESeq2 is 0.1) are shown in red.

Before making the MA-plot, we use the `lfcShrink` function to shrink the log2 fold changes for the comparison of choice. There are three types of shrinkage estimators in DESeq2, which are covered in the DESeq2 vignette. Here we specify the `apeglm` method for shrinking coefficients, which is good for shrinking the noisy LFC estimates while giving low bias LFC estimates for true large differences (Zhu, Ibrahim, and Love 2018). To use `apeglm` we specify a coefficient from the model to shrink, either by name or number as the coefficient appears in `resultsNames(dds)`.
```{r}
library("apeglm")
resultsNames(PAG_DESeq_set_1) # Check to see which coef to choose
```

```{r}
PAG_DESeq_results_1_LFC <- lfcShrink(PAG_DESeq_set_1, coef="cell.type_VGluT2_vs_VGAT", 
                                     type="apeglm",
                                     lfcThreshold = 0, parallel = TRUE)
PAG_DESeq_results_1_LFC
DESeq2::plotMA(PAG_DESeq_results_1_LFC, ylim = c(-2, 2))
```

We can label individual points on the MA-plot as well. Here we use the `with` R function to plot a circle and text for a selected row of the results object. Within the `with` function, only the `baseMean` and `log2FoldChange` values for the selected rows of `PAG_DESeq_results_0` are used.
```{r}
plotMA(PAG_DESeq_results_1_wald, ylim = c(-5,5))
topGene <- rownames(PAG_DESeq_results_1_wald)[which.min(PAG_DESeq_results_3$padj)]
with(PAG_DESeq_results_1_wald[topGene, ], {
  points(baseMean, log2FoldChange, col="dodgerblue", cex=2, lwd=2)
  text(baseMean, log2FoldChange, topGene, pos=2, col="dodgerblue")
})
```

Another useful diagnostic plot is the histogram of the p values. This plot is best formed by excluding genes with very small counts, which otherwise generate spikes in the histogram.
```{r}
hist(PAG_DESeq_results_1_wald$pvalue[PAG_DESeq_results_1_wald$baseMean > 1], breaks = 0:20/20,
     col = "grey50", border = "white")
```

After calling plotMA, one can use the function identify to interactively detect the row number of individual genes by clicking on the plot. One can then recover the gene identifiers by saving the resulting indices:
```{r}
plotMA(PAG_DESeq_results_1_wald, ylim = c(-5, 5))
idx <- identify(PAG_DESeq_results_1_wald$baseMean, PAG_DESeq_results_1_wald$log2FoldChange)
rownames(PAG_DESeq_results_1_wald)[idx]
```

***
__Gene clustering__
Another way of representing the results of a differential expression analysis is to construct a heatmap of the top differentially expressed genes. A heatmap is a color coded expression matrix, where the rows and columns are clustered using hierarchical clustering. Typically, it should not be applied to counts, but works better with transformed values. Here we show how it can be applied to the variance-stabilized values generated above. We choose the top 30 differentially expressed genes. There are many functions in R that can generate heatmaps, here we show the one from the `pheatmap` package.

In the sample distance heatmap made previously, the dendrogram at the side shows us a hierarchical clustering of the samples. Such a clustering can also be performed for the genes. Since the clustering is only relevant for genes that actually carry a signal, one usually would only cluster a subset of the most highly variable genes. For example, let us select the 30 genes with the highest variance across samples. We will work with the VST data. In addition, the heatmap becomes more interesting if we do not look at absolute expression strength but rather at the amount by which each gene deviates in a specific sample from the gene's average across all samples. Hence, we center each gene's values across samples, and plot a heatmap. We provide a `data.frame` that instructs the `pheatmap` function how to label the columns.
```{r}
library(pheatmap)

PAG_DESeq_mat <- assay(PAG_DESeq_vsd)[head(order(PAG_DESeq_results$padj), 30), ] # Top 30 DE genes
PAG_DESeq_mat <- PAG_DESeq_mat - rowMeans(PAG_DESeq_mat)
anno <- as.data.frame(colData(PAG_DESeq_vsd)[, c("mouse.id", "cell.type", "PAG.area")])
pheatmap(PAG_DESeq_mat, annotation_col = anno)
```

***
__Independent filtering__
The MA plot highlights an important property of RNA-seq data. For weakly expressed genes, we have no chance of seeing differential expression, because the low read counts suffer from such high Poisson noise that any biological effect is drowned in the uncertainties from the sampling at a low rate. We can also show this by examining the ratio of small p values (say, less than 0.05) for genes binned by mean normalised count. We will use the results table subjected to the threshold to show what this looks like in a case when there are few tests with small p value.

In the following code chunk, we create bins using the quantile function, bin the genes by base mean using cut, rename the levels of the bins using the middle point, calculate the ratio of p values less than 0.05 for each bin, and finally plot these ratios.
```{r}
qs <- c(0, quantile(PAG_DESeq_results_LFC1$baseMean[PAG_DESeq_results_LFC1$baseMean > 0], 0:6/6))
bins <- cut(PAG_DESeq_results_LFC1$baseMean, qs)
levels(bins) <- paste0("~", round(signif((qs[-1] + qs[-length(qs)])/2, 2)))
fractionSig <- tapply(PAG_DESeq_results_LFC1$pvalue, bins, function(p)
                          mean(p < .05, na.rm = TRUE))
barplot(fractionSig, xlab = "mean normalised count",
                     ylab = "fraction of small p values")
```

At first sight, there may seem to be little benefit in filtering out these genes. After all, the test found them to be non-significant anyway. However, these genes have an influence on the multiple testing adjustment, whose performance improves if such genes are removed. By removing the low count genes from the input to the FDR procedure, we can find more genes to be significant among those that we keep, and so improved the power of our test. This approach is known as independent filtering.

The DESeq2 software automatically performs independent filtering that maximizes the number of genes with adjusted p value less than a critical value (by default, `alpha` is set to 0.1). This automatic independent filtering is performed by, and can be controlled by, the results function.

The term independent highlights an important caveat. Such filtering is permissible only if the statistic that we filter on (here the mean of normalised counts across all samples) is independent of the actual test statistic (the p value) under the null hypothesis. Otherwise, the filtering would invalidate the test and consequently the assumptions of the BH procedure. The independent filtering software used inside DESeq2 comes from the `genefilter` package, that contains a reference to a paper describing the statistical foundation for independent filtering (Bourgon, Gentleman, and Huber 2010).

#### Annotating and exporting results
Our result table so far only contains the Ensembl gene IDs, but alternative gene names may be more informative for interpretation. Bioconductor's annotation packages help with mapping various ID schemes to each other. We load the `AnnotationDbi` package and the annotation package `org.Mm.eg.db`, the organism annotation package ("org") for Mus musculus ("Mm"), organized as an `AnnotationDbi` database package ("db"), using Entrez Gene IDs ("eg") as primary key. To get a list of all available key types, use:
```{r}
library(AnnotationDbi)
library(org.Mm.eg.db)
columns(org.Mm.eg.db)
```

We can use the `mapIds` function to add individual columns to our results table. We provide the row names of our results table as a key, and specify that `keytype=ENSEMBL`. The column argument tells the `mapIds` function which information we want, and the `multiVals` argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. To add the gene symbol and Entrez ID, we call `mapIds` twice.
```{r}
PAG_DESeq_results_1$symbol <- mapIds(org.Mm.eg.db,
                                   keys = rownames(PAG_DESeq_results_1),
                                   column = "SYMBOL",
                                   keytype = "ENSEMBL",
                                   multiVals = "first")

PAG_DESeq_results_1$entrez <- mapIds(org.Mm.eg.db,
                                   keys = rownames(PAG_DESeq_results_1),
                                   column = "ENTREZID",
                                   keytype = "ENSEMBL",
                                   multiVals = "first")
```

We can easily save the results table in a CSV file that we can then share or load with a spreadsheet program such as Excel (note, however, that Excel sometimes does funny things to gene identifiers (Zeeberg et al. 2004; Ziemann, Eren, and El-Osta 2016)). The call to `as.data.frame` is necessary to convert the `DataFrame` object (`IRanges` package) to a `data.frame` object that can be processed by `write.csv`.
```{r}
PAG_DESeq_results_1_wald_ordered <- PAG_DESeq_results_1_wald[order(PAG_DESeq_results_1_wald$padj), ]
head(PAG_DESeq_results_1_wald_ordered)

PAG_DESeq_results_1_wald_ordered <- as.data.frame(PAG_DESeq_results_1_wald_ordered)[seq_len(sum(PAG_DESeq_results_1_wald$padj < 0.05, na.rm = TRUE)), ] # Exporting the genes with padg<0.05
PAG_DESeq_results_1_wald_ordered <- PAG_DESeq_results_1_wald_ordered[order(PAG_DESeq_results_1_wald_ordered$log2FoldChange, decreasing = TRUE), ]
write.table(cbind(id = rownames(PAG_DESeq_results_1_wald_ordered), PAG_DESeq_results_1_wald_ordered), quote = FALSE, sep = "\t", row.names = FALSE, file = "PAG_DESeq_results_1_wald_ordered.txt")
write.csv(PAG_DESeq_results_1_wald_ordered, file = "PAG_DESeq_results_1_wald_ordered.csv")
```

A more sophisticated way for exporting results the Bioconductor package `ReportingTools` (Huntley et al. 2013). `ReportingTools` will automatically generate dynamic HTML documents, including links to external databases using gene identifiers and boxplots summarizing the normalised counts across groups. See the `ReportingTools` vignettes for full details. The simplest version of creating a dynamic `ReportingTools` report is performed with the following code:
```{r}
library("ReportingTools")
htmlRep <- HTMLReport(shortName="report", title="My report",
                      reportDirectory="./report")
publish(resOrderedDF, htmlRep)
url <- finish(htmlRep)
browseURL(url)
```

### 7.2.2 | edgeR: the DGEList
`edgeR` is designed for bulkRNA and is based on a negative binomial model of gene expression and uses a generalized linear model (GLM) framework, that enables us to include other factors such as `batch` to the model. The _edgeR_ package uses another type of data container, namely a _DGEList_ object. We can create a _DGEList_ object using a count matrix and a table with information about samples, and additionally add information about the genes.
```{r}
library(edgeR)

PAG_genetable <- data.frame(gene.id = rownames(PAG_sceset_qc_norm_filt_corr_clust),
                            stringsAsFactors = FALSE)

stopifnot(all(rownames(colData(PAG_sceset_qc_norm_filt_corr_clust)) == colnames(PAG_sceset_qc_norm_filt_corr_clust)))

PAG_DGE_list <- DGEList(counts = assay(PAG_sceset_qc_norm_filt_corr_clust, "counts_rounded"), # raw read counts, rounded
                        #lib.size = colSums(counts), # numeric vector giving the total count (sequence depth) for each library.
                        #norm.factors = rep(1, length(counts[1,])), # numeric vector of normalisation factors that modify the library sizes (could insert scran size factors here)
                        samples = colData(PAG_sceset_qc_norm_filt_corr_clust), # data frame containing information for each sample
                        group = NULL, # vector or factor giving the experimental group/condition for each sample/library
                        genes = PAG_genetable, # data frame containing annotation information for each gene
                        remove.zeros = FALSE) # logical, whether to remove rows that have 0 total count

names(PAG_DGE_list)
```

Just like the _SummarizedExperiment_ and the _DESeqDataSet_ the _DGEList_ contains all the information we need: the count matrix, information about the samples (the columns of the count matrix), and information about the genes (the rows of the count matrix). One difference compared to the _DESeqDataSet_ is that the experimental design is not defined when creating the _DGEList_, but later in the workflow.

Once a `DGEList` has been created, we calculate between-sample (TMM) normalisation factors, using the `calcNormFactors` function in `edgeR`.
```{r}
PAG_DGE_list <- edgeR::calcNormFactors(PAG_DGE_list, method="TMM")
PAG_DGE_list$samples
```

#### Exploratory analysis and visualization
***
**MDS plot:**
A way to reduce dimensionality, which is in many ways similar to PCA, is _multidimensional scaling_ (MDS). For MDS, we first have to calculate all pairwise distances between our objects (samples in this case), and then create a (typically) two-dimensional representation where these pre-calculated distances are represented as accurately as possible. This means that depending on how the pairwise sample distances are defined, the two-dimensional plot can be very different, and it is important to choose a distance that is suitable for the type of data at hand.

_edgeR_ contains a function `plotMDS`, which operates on a `DGEList` object and generates a two-dimensional MDS representation of the samples. The default distance between two samples can be interpreted as the â€œtypicalâ€ log fold change between the two samples, for the genes that are most different between them (by default, the top 500 genes, but this can be modified). We generate an MDS plot from the DGEList object `PAG_DGE_list`, coloring by the `PAG.area` and using different plot symbols for different cell types:
```{r}
plotMDS(PAG_DGE_list, top = 500, labels = NULL, col = as.numeric(PAG_DGE_list$samples$PAG.area), 
        pch = as.numeric(PAG_DGE_list$samples$cell.type), cex = 2, gene.selection = "common")
```

#### Differential expression testing with EdgeR
_edgeR_ is designed for bulkRNA and is based on a negative binomial model of gene expression and uses a generalized linear model (GLM) framework, the enables us to include other factors such as `batch` to the model. We have a `DGEList` object containing all the necessary information. We first define a design matrix, using the same formula syntax as we did for _DESeq2_.
```{r}
names(PAG_DGE_list)
PAG_DGE_design <- model.matrix(~ mouse.sex + batch.processing + batch.sequencing_round + celltype_PAGarea,
                               data = PAG_DGE_list$samples)
```

While _DESeq2_ performs independent filtering of lowly expressed genes internally, this is done by the user before applying _edgeR_. Here, we filter out lowly expressed genes using the `filterByExpr()` function, and then estimate the dispersion for each gene. Note that it is important that we specify the design in the dispersion calculation. Afterwards, we plot the estimated dispersions. By default, the function keeps genes with about 10 read counts or more in a minimum number of samples, where the number of samples is chosen according to the minimum group sample size.

Biological CV (BCV) is the coefficient of variation with which the (unknown) true abundance of the gene varies between replicate RNA samples. It represents the CV that would remain between biological replicates if sequencing depth could be increased indefinitely. The technical CV decreases as the size of the counts increases. BCV on the other hand does not. BCV is therefore likely to be the dominant source of uncertainty for high-count genes, so reliable estimation of BCV is crucial for realistic assessment of differential expression in RNA-Seq experiments. If the abundance of each gene varies between replicate RNA samples in such a way that the genewise standard deviations are proportional to the genewise means, a commonly occurring property of measurements on physical quantities, then it is reasonable to suppose that BCV is approximately constant across genes. We allow however for the possibility that BCV might vary between genes and might also show a systematic trend with respect to gene expression or expected count. The magnitude of BCV is more important than the exact probabilistic law followed by the true gene abundances. For mathematical convenience, we assume that the true gene abundances follow a gamma distributional law between replicate RNA samples. This implies that the read counts follow a negative binomial probability law.
```{r}
start_time <- Sys.time() # Takes around 1.61 hours
DGE_genes_keep <- edgeR::filterByExpr(PAG_DGE_list, PAG_DGE_design, min.count = 5) # Choose a specific gene filter
table(DGE_genes_keep)
PAG_DGE_list <- PAG_DGE_list[DGE_genes_keep, ]
PAG_DGE_list <- edgeR::estimateDisp(PAG_DGE_list, PAG_DGE_design)
saveRDS(PAG_DGE_list, file = "PAG_DGE_list.rds")
end_time <- Sys.time()
end_time - start_time

edgeR::plotBCV(PAG_DGE_list)
```

Finally, we fit the generalized linear model and perform the test. In the `glmQLFTest` function, we indicate which coefficient (which column in the design matrix) we would like to test for. It is possible to test more general contrasts as well, and the user guide contains many examples on how to do this. The `topTags` function extracts the top-ranked genes. You can indicate the adjusted p-value cutoff, and/or the number of genes to keep.

For general experiments, once dispersion estimates are obtained and negative binomial generalized linear models are fitted, we can proceed with testing procedures for determining differential expression using either quasi-likelihood (QL) F-test or likelihood ratio test. While the likelihood ratio test is a more obvious choice for inferences with GLMs, the QL F-test is preferred as it reflects the uncertainty in estimating the dispersion for each gene. It provides more robust and reliable error rate control when the number of replicates is small.

Given raw counts, NB dispersion(s) and a design matrix, `glmQLFit()` fits the negative binomial GLM for each tag and produces an object of class `DGEGLM` with some new components. This `DGEGLM` object can then be passed to `glmQLFTest()` to carry out the QL F-test. User can select one or more coefficients to drop from the full design matrix. This gives the null model against which the full model is compared. Tags can then be ranked in order of evidence for differential expression, based on the p-value computed for each tag.
```{r}
start_time <- Sys.time() # Takes around 40 minutes
fit <- edgeR::glmQLFit(PAG_DGE_list, PAG_DGE_design)
qlf <- edgeR::glmQLFTest(fit, coef = 30:36)
end_time <- Sys.time()
end_time - start_time

PAG_edgeR_results_all <- edgeR::topTags(qlf, n = nrow(PAG_DGE_list), sort.by = "none") # all genes
hist(PAG_edgeR_results_all$table$PValue)

PAG_edgeR_results_tt <- edgeR::topTags(qlf, n = nrow(PAG_DGE_list), p.value = 0.05) # genes with adj.p<0.05
PAG_edgeR_results_tt10 <- edgeR::topTags(qlf) # just the top 10 by default
PAG_edgeR_results_tt10

# Alternative from Hemberg Lab scRNAseq Course:
#res <- glmLRT(fit)
#pVals <- res$table[,4]
#names(pVals) <- rownames(res$table)
#pVals <- p.adjust(pVals, method = "fdr")
```

The columns in the _edgeR_ result data frame are similar to the ones output by _DESeq2_. _edgeR_ represents the overall expression level on the log-CPM scale rather than on the normalised count scale that DESeq2 uses. The `F` column contains the test statistic, and the `FDR` column contains the Benjamini-Hochberg adjusted p-values.

We can also test for significance relative to a fold-change threshold, using the function `glmTreat`. Below we set the log fold-change threshold to 1 (i.e., fold change threshold equal to 2), as for DESeq2 above.
```{r}
PAG_edgeR_results_treat <- edgeR::glmTreat(fit, coef = ncol(PAG_DGE_design), lfc = 1)
PAG_edgeR_results_treat_tt <- edgeR::topTags(PAG_edgeR_results_treat, n = nrow(PAG_DGE_list), sort.by = "none")
```

#### Plotting results
In edgeR, the MA plot is obtained via the `plotSmear` function.
```{r}
edgeR::plotSmear(qlf, de.tags = PAG_edgeR_results_tt$table$gene.id)
```

#### Gene ontology and pathway analysis
The gene ontology (GO) enrichment analysis and the KEGG pathway enrichment analysis are the common downstream procedures to interpret the differential expression results in a biological context. Given a set of genes that are up- or down-regulated under a certain contrast of interest, a GO (or pathway) enrichment analysis will find which GO terms (or pathways) are over- or under-represented using annotations for the genes in that set.

The GO analysis can be performed using the `goana()` function in edgeR. The KEGG pathway analysis can be performed using the `kegga()` function in edgeR. Both `goana()` and `kegga()` take a `DGELRT` or `DGEExact` object. They both use the NCBI RefSeq annotation. Therefore, the Entrez Gene identifier (ID) should be supplied for each gene as the row names of the input object. Also users should set `species` according to the organism being studied. The top set of most enriched GO terms can be viewed with the `topGO()` function, and the top set of most enriched KEGG pathways can be viewed with the `topKEGG()` function.
```{r}
go <- goana(qlf, species="Mm")
topGO(go, sort="up")

keg <- kegga(qlf, species="Mm")
topKEGG(keg, sort="up")
```

### 7.2.3 | Comparing DESeq2 and edgeR results:
We can compare the sets of significantly differentially expressed genes to see how the results from the two packages overlap:
```{r}
PAG_DE_shared <- intersect(rownames(PAG_DESeq_results), PAG_edgeR_results_all$table$gene.id)
table(DESeq2 = PAG_DESeq_results$padj[match(PAG_DE_shared, rownames(PAG_DESeq_results))] < 0.1, 
      edgeR = PAG_edgeR_results_all$table$FDR[match(PAG_DE_shared, PAG_edgeR_results_all$table$gene.id)] < 0.1)
```

We can also compare the two result lists by the ranks:
```{r}
plot(rank(PAG_DESeq_results$pvalue[match(PAG_DE_shared, rownames(PAG_DESeq_results))]), 
     rank(PAG_edgeR_results_all$table$PValue[match(PAG_DE_shared, PAG_edgeR_results_all$table$gene.id)]), 
     cex = 0.1, xlab = "DESeq2", ylab = "edgeR")
```

## Step 7.3 | RNAseq123 Workflow
The [RNAseq123 Workflow](https://bioconductor.org/packages/release/workflows/html/RNAseq123.html) uses the _edgeR_ package (Robinson, McCarthy, and Smyth 2010) to import, organise, filter and normalise the data, and the _limma_ package (Ritchie et al. 2015) with its _voom_ method for linear modelling and empirical Bayes moderation to assess differential expression and perform gene set testing.
```{r}
library(limma)
library(Glimma)
library(edgeR)
library(Mus.musculus)
```

### 7.3.1 | Loading the data
We start the DE workflow from a gene-vs-sample matrix, where raw reads have been quality controlled and gene expression quantified.
```{r}
# Round the counts to ensure they are integers:
PAG_DE_counts <- round(assay(PAG_sceset_qc, "counts")) # NOT the normalised counts
head(PAG_DE_counts)
```

Read the relevant metadata, converting the key annotations `factor`:
```{r}
# Try to keep only the metadata that is relevant for the analysis.
PAG_DE_metadata <- PAG_metadata # Alternatively use colData(PAG_sceset_qc)
PAG_DE_metadata
rownames(PAG_DE_metadata)

# Factorize the relevant annotations for proper usage within the DE packages:
PAG_DE_metadata$mouse.id <- factor(PAG_DE_metadata$mouse.id)
PAG_DE_metadata$cell.type <- factor(PAG_DE_metadata$cell.type)
PAG_DE_metadata$PAG.area <- factor(PAG_DE_metadata$PAG.area)
```

Create a _DGEList_ object as we would do for _edgeR_:
```{r}
PAG_genetable <- data.frame(gene.id = rownames(PAG_DE_counts),
                            stringsAsFactors = FALSE)

stopifnot(all(colnames(PAG_DE_counts) == rownames(PAG_DE_metadata)))
PAG_limma_set <- DGEList(counts = PAG_DE_counts,
                         samples = PAG_DE_metadata, 
                         genes = PAG_genetable)

names(PAG_limma_set)
```

### 7.3.2 | Data pre-processing
Probably already done before, but ideally we should exclude low-quality cells and remove genes with zero counts before proceeding. We also calculate normalisation factors:
```{r}
cpm <- cpm(PAG_limma_set)
lcpm <- cpm(PAG_limma_set, log=TRUE)

PAG_limma_set <- calcNormFactors(PAG_limma_set, method = "TMM")
PAG_limma_set$samples$norm.factors
```

### 7.3.3 | Creating a design matrix and contrasts
For a given experiment, there are usually several equivalent ways to set up an appropriate design matrix. For example, `~0+group+lane` removes the intercept from the first factor, `group`, but an intercept remains in the second factor lane. Alternatively, `~group+lane` could be used to keep the intercepts in both `group` and `lane`. Understanding how to interpret the coefficients estimated in a given model is key here. We choose the first model for our analysis, as setting up model contrasts is more straight forward in the absence of an intercept for `group`. Contrasts for pairwise comparisons between cell populations are set up in limma using the `makeContrasts` function.
```{r}
PAG_limma_design <- model.matrix(~ mouse.id + cell.type + PAG.area,
                                 data = PAG_limma_set$samples)
PAG_limma_design

PAG_limma_contrasts_matrix <- makeContrasts(VGATvsVGluT2 = VGAT-VGluT2,
                                            levels = colnames(PAG_limma_design))
PAG_limma_contrasts_matrix
```

A key strength of limma's linear modelling approach, is the ability accommodate arbitrary experimental complexity. Simple designs, such as the one in this workflow, with cell type and batch, through to more complicated factorial designs and models with interaction terms can be handled relatively easily. Where experimental or technical effects can be modelled using a random effect, another possibility in limma is to estimate correlations using `duplicateCorrelation` by specifying a `block` argument for both this function and in the `lmFit` linear modelling step.

### 7.3.4 | Removing heteroscedascity from count data and linear modelling
RNA-seq data is homoscedastic (the variance is not independent of the mean). Methods that model counts using a Negative Binomial distribution assume a quadratic mean-variance relationship. In limma, linear modelling is carried out on the log-CPM values which are assumed to be normally distributed and the mean-variance relationship is accommodated using precision weights calculated by the voom function.

When operating on a DGEList-object, `voom` converts raw counts to log-CPM values by automatically extracting library sizes and normalisation factors from `PAG_limma_set` itself. Additional normalisation to log-CPM values can be specified within `voom` using the `normalize.method `argument. Typically, the _voom-plot_ shows a decreasing trend between the means and variances resulting from a combination of technical variation in the sequencing experiment and biological variation amongst the replicate samples from different cell populations. Experiments with high biological variation usually result in flatter trends, where variance values plateau at high expression values. Experiments with low biological variation tend to result in sharp decreasing trends.
```{r}
par(mfrow=c(1,2))
PAG_limma_set_voom <- voom(PAG_limma_set, PAG_limma_design, plot=TRUE)
PAG_limma_set_voom
```

Linear modelling in limma is carried out using the `lmFit` and `contrasts.fit` functions originally written for application to microarrays. The functions can be used for both microarray and RNA-seq data and fit a separate model to the expression values for each gene. Next, empirical Bayes moderation is carried out by borrowing information across all the genes to obtain more precise estimates of gene-wise variability (Smyth 2004).
```{r}
PAG_limma_set_voom_fit <- lmFit(PAG_limma_set_voom, PAG_limma_design)
PAG_limma_set_voom_fit <- contrasts.fit(PAG_limma_set_voomfit, contrasts=PAG_limma_contrasts_matrix)
PAG_limma_set_voom_efit <- eBayes(PAG_limma_set_voomfit)
plotSA(PAG_limma_set_voom_efit, main="Final model: Mean-variance trend")
```

The figure should shwo the means (x-axis) and variances (y-axis) of each gene plotted to show the dependence between the two before `voom` is applied to the data (left panel) and how the trend is removed after `voom` precision weights are applied to the data (right panel). The plot on the left is created within the `voom` function which extracts residual variances from fitting linear models to log-CPM transformed data. Variances are then rescaled to quarter-root variances (or square-root of standard deviations) and plotted against the average log2 count for each gene. The plot on the right is created using `plotSA` which plots log2 residual standard deviations against mean log-CPM values. In both plots, each black dot represents a gene. On the left plot, the red curve shows the estimated mean-variance trend used to compute the `voom` weights. On the right plot, the average log2 residual standard deviation estimated by the empirical Bayes algorithm is marked by a horizontal blue line.

### 7.3.5 | Examining the number of DE genes
For a quick look at differential expression levels, the number of significantly up- and down-regulated genes can be summarised in a table. 
```{r}
summary(decideTests(PAG_limma_set_voom_efit))
```

For a stricter definition on significance, one may require log-fold-changes (log-FCs) to be above a minimum value. The `treat` method (McCarthy and Smyth 2009) can be used to calculate p-values from empirical Bayes moderated t-statistics with a minimum log-FC requirement.
```{r}
PAG_limma_set_treat_fit <- treat(PAG_limma_set_voom_fit, lfc=1)
PAG_limma_set_treat_fit_dt <- decideTests(PAG_limma_set_treat_fit)
summary(PAG_limma_set_treat_fit_dt)
```

Genes that are DE in multiple comparisons can be extracted using the results from `decideTests`, where 0s represent genes that are not DE, 1s represent genes that are up-regulated, and -1s represent genes that are down-regulated. The `write.fit` function can be used to extract and write results for all comparisons to a single output file.
```{r}
PAG_limma_DE_genes <- which(PAG_limma_set_treat_fit_dt[,1]!=0 & PAG_limma_set_treat_fit_dt[,2]!=0)
length(PAG_limma_DE_genes) # Number of DE genes.

head(PAG_limma_set_treat_fit$genes$SYMBOL[PAG_limma_DE_genes], n=20) # Try without $SYMBOL if it fails

# A venn diagram shows the number of DE genes in each comparisons, the number of genes that are DE in both comparisons, and the number of genes that are not DE in either comparison (bottom-right).
vennDiagram(PAG_limma_set_treat_fit_dt[,1:2], circle.col=c("turquoise", "salmon"))

# Extract and write results for all comparisons to a single output file.
write.fit(PAG_limma_set_treat_fit, PAG_limma_set_treat_fit_dt, file="PAG_limma_results.txt")
```

The top DE genes can be listed using `topTreat` for results using `treat` (or `topTable`for results using `eBayes`). By default `topTreat` arranges genes from smallest to largest adjusted p-value with associated gene information, log-FC, average log-CPM, moderated t-statistic, raw and adjusted p-value for each gene. The number of top genes displayed can be specified, where `n=Inf` includes all genes.
```{r}
VGAT_vs_VGluT2 <- topTreat(PAG_limma_set_treat_fit, coef=1, n=Inf)
head(VGAT_vs_VGluT2)
```

### 7.3.6 | Useful graphical representations of differential expression results
To summarise results for all genes visually, mean-difference plots, which display log-FCs from the linear model fit against the average log-CPM values can be generated using the `plotMD` function, with the differentially expressed genes highlighted.
```{r}
plotMD(PAG_limma_set_treat_fit, column=1, 
       status=PAG_limma_set_treat_fit_dt[,1], 
       main=colnames(PAG_limma_set_treat_fit)[1], 
       xlim=c(-8,13))
```

`Glimma` extends this functionality by providing an interactive mean-difference plot via the `glMDPlot` function. The output of this function is an html page, with summarised results in the left panel (similar to what is output by `plotMD`), and the log-CPM values from individual samples for a selected gene in the right panel, with a table of results below the plots. This interactive display allows the user to search for particular genes based on the annotation provided (e.g. Gene symbol identifier), which is not possible in a static R plot.
```{r}
glMDPlot(PAG_limma_set_treat_fit, coef=1, 
         status=PAG_limma_set_treat_fit_dt,
         main=colnames(PAG_limma_set_treat_fit)[1],
         side.main="ENTREZID", counts=lcpm, groups=group, launch=FALSE)
```

Heatmaps allow users to look at the expression of a subset of genes. This can give useful insight into the expression of individual groups and samples without losing perspective of the overall study when focusing on individual genes, or losing resolution when examining patterns averaged over thousands of genes at the same time. We can create a heatmap for the top 100 DE genes (as ranked by adjusted p-value) with the `heatmap.2` function from the _gplots_ package. 
```{r}
library(gplots)

VGAT_vs_VGluT2_topgenes <- VGAT_vs_VGluT2$ENTREZID[1:100]
i <- which(PAG_limma_set_voom$genes$ENTREZID %in% VGAT_vs_VGluT2_topgenes)

mycolors <- colorpanel(1000,"blue","white","red")

heatmap.2(lcpm[i,], scale="row",
          labRow=PAG_limma_set_voom$genes$SYMBOL[i], labCol=cell.type, 
          col=mycolors, trace="none", density.info="none", 
          margin=c(8,6), lhei=c(2,10), dendrogram="column")
```

## Step 7.4 | MAST
MAST is an R/Bioconductor package for managing and analyzing qPCR and sequencing-based single-cell gene expression data, as well as data from other types of single-cell assays. Apart from reading and storing single-cell assay data, the package also provides functionality for significance testing of differential expression using a Hurdle model, gene set enrichment, facilities for visualizing patterns in residuals indicative of differential expression, and power calculations.

MAST is based on a zero-inflated negative binomial model. It tests for differential expression using a Hurdle model to combine tests of discrete (0 vs not zero) and continuous (non-zero values) aspects of gene expression. Similar to `DESeq2` and `edgeR`, it uses a linear modelling framework to enable complex models to be considered.

We can directly convert our `SingleCellExperiment` object to the `SingleCellAssay` that `MAST` uses:
```{r}
library(MAST)
PAG_MAST_sca = SceToSingleCellAssay(PAG_sceset_qc_norm_filt_corr_clust)
# or: sca <- FromMatrix(assays(sce)$logcounts, colData(sce), rowData(sce))
options(mc.cores = 10) # For parallelisation. You can check how many cores you have available with detectCores()
```

### 7.4.1 | Significance testing under the Hurdle model
Once we are satisfied that we have high-quality expression (i.e. after QC), we will consider tests for differential expression and ways to visualize results. It is often helpful to synthesize from gene-level into module-level statements, so we will use MAST to also test for gene set enrichment.

There are two frameworks available in the package. The first framework `zlm` offers a full linear model to allow arbitrary comparisons and adjustment for covariates. The second framework `LRT` can be considered essentially performing t-tests (respecting the discrete/continuous nature of the data) between pairs of groups. `LRT` is subsumed by the first framework, but might be simpler for some users, so has been kept in the package.

For each gene, `MAST` will fit a Hurdle model with a separate intercept for each factor added to the model. An S4 object of class `ZlmFit` is returned, containing slots with the `genewise` coefficients, variance-covariance matrices, etc.
```{r}
# Model expression as function of condition & number of detected genes
start_time <- Sys.time() # Takes around 10 minutes
PAG_zlm <- zlm(~ mouse.sex + batch.processing + batch.sequencing_round + celltype_PAGarea, 
               sca = PAG_MAST_sca, parallel = TRUE)
end_time <- Sys.time()
end_time - start_time
```

We could run a likelihood ratio test here, testing for differences when we drop the `celltype_PAGarea` factor. Note that any arbitrary contrast matrix can be tested here, and specified either using a matrix or syntactically (see `Hypothesis` for details).
```{r}
#only test the condition coefficient.
PAG_zlm_summary <- summary(PAG_zlm, doLRT='conditionStim') 
#print the top 4 genes by contrast using the logFC
print(PAG_zlm_summary, n=4)
```

_But often of more general use is this delicious syntactic sugar to make a giant `data.table` containing coefficients, standard errors, etc, for the various model components. Many Bothan spies died so that we could pretty-print this summary of the top differentially expressed genes._
```{r}
PAG_zlm_summary_DT <- PAG_zlm_summary$datatable
fcHurdle <- merge(PAG_zlm_summary_DT[contrast=='conditionStim' & component=='H',.(primerid, `Pr(>Chisq)`)], #hurdle P values
                  PAG_zlm_summary_DT[contrast=='conditionStim' & component=='logFC', .(primerid, coef, ci.hi, ci.lo)], by='primerid') #logFC coefficients

fcHurdle[,fdr:=p.adjust(`Pr(>Chisq)`, 'fdr')]
fcHurdleSig <- merge(fcHurdle[fdr<.05 & abs(coef)>FCTHRESHOLD], as.data.table(mcols(PAG_MAST_sca)), by='primerid')
setorder(fcHurdleSig, fdr)
```

#### Visualisation of 50 most differentially expressed genes
```{r}
entrez_to_plot <- fcHurdleSig[1:50,primerid]
symbols_to_plot <- fcHurdleSig[1:50,symbolid]
flat_dat <- as(PAG_MAST_sca[entrez_to_plot,], 'data.table')
ggbase <- ggplot(flat_dat, aes(x=condition, y=thresh,color=condition)) + geom_jitter()+facet_wrap(~symbolid, scale='free_y')+ggtitle("DE Genes in Activated MAIT Cells")
ggbase+geom_violin() 
```

#### Heatmap based on most DE genes
```{r}
mat_to_plot <- assay(PAG_MAST_sca[entrez_to_plot,])
rownames(mat_to_plot) <- symbols_to_plot
aheatmap(mat_to_plot,annCol=colData(PAG_MAST_sca)[,"condition"],main="DE genes",col=rev(colorRampPalette(colors = brewer.pal(name="PiYG",n=10))(20)))
```

## Step 7.5 | Non-parametric tests
The main argument against using non-parametric tests is that scaling normalization does not adjust for differences in the distributions, which results in incorrect rejection of the null hypothesis in fairly innocuous cases (see Aaron Lun's github: https://github.com/LTLA/SingleCellThoughts/blob/master/workflows/de.Rmd). Also, from _Soneson et al 2018_, the main drawback is that these methods don't allow for complex designs.

### 7.5.1 | Kolmogorov-Smirnov test (non-parametric)
To compare the distributions for each gene in two individuals/groups. The KS-test quantifies the distance between the empirical cummulative distributions of the expression of each gene in each of the two populations. It is sensitive to changes in mean expression and changes in variability. However it assumes data are continuous and may perform poorly when data contains a large number of identical values (eg. zeros). Another issue with the KS-test is that it can be very sensitive for large sample sizes and thus it may end up as significant even though the magnitude of the difference is very small.
```{r}
pValues_KS <- apply(
    norm, 1, function(x) {
        ks.test(
            x[PAG_sceset_qc$cell.type == "VGluT2"], 
            x[PAG_sceset_qc$cell.type == "VGAT"]
        )$p.value
    }
)
# multiple testing correction
pValues_KS <- p.adjust(pValues_KS, method = "fdr")
```

```{r}
# How many of the significant DE genes are detected
significant_DE_genes_KS <- names(pValues_KS)[pValues_KS < 0.05]
length(significant_DE_genes_KS)
```

Often it is informative to vary the threshold and evaluate performance across a range of values. This is then plotted as a receiver-operating-characteristic curve (ROC) and a general accuracy statistic can be calculated as the area under this curve (AUC). The ROCR package facilitates this plotting.

### 7.5.2 | Wilcox/Mann-Whitney-U Test (non-parametric)
The Wilcox-rank-sum test is another non-parametric test, but tests specifically if values in one group are greater/less than the values in the other group. Thus it is often considered a test for difference in median expression between two groups; whereas the KS-test is sensitive to any change in distribution of expression values.
```{r}
pValues_W <- apply(
    norm, 1, function(x) {
        wilcox.test(
            x[PAG_sceset_qc$cell.type == "VGluT2"], 
            x[PAG_sceset_qc$cell.type == "VGAT"]
        )$p.value
    }
)
# multiple testing correction
pValues_W <- p.adjust(pValues_W, method = "fdr")
```

```{r}
#How many of the significant DE genes are detected
significant_DE_genes_W <- names(pValues_W)[pValues_W < 0.05]
length(significant_DE_genes_W)
```
## Step 7.6 | SCDE [not yet implemented]
The `scde` package implements routines for fitting individual error models for single-cell RNA-seq measurements. Briefly, the read counts observed for each gene are modeled using a mixture of a negative binomial (NB) distribution (for the amplified/detected transcripts) and low-level Poisson distribution (for the unobserved or background-level signal of genes that failed to amplify or were not detected for other reasons). These models can then be used to identify robustly differentially expressed genes between groups of cells.

Website: http://hms-dbmi.github.io/scde/

### 7.6.1 | Preparing the data
The analysis starts with a matrix of read counts (the values must be integers). We first need to define the two groups we are going to compare:
```{r}
library(scde)
# load example dataset
data(es.mef.small)

# factor determining cell types
sg <- factor(gsub("(MEF|ESC).*", "\\1", colnames(es.mef.small)), levels = c("ESC", "MEF"))
# the group factor should be named accordingly
names(sg) <- colnames(es.mef.small)  
table(sg)

# clean up the dataset
cd <- clean.counts(es.mef.small, min.lib.size=1000, min.reads = 1, min.detected = 1)
```

### 7.6.2 | Fitting error models
As a next step we fit the error models on which all subsequent calculations will rely. The fitting process relies on a subset of robust genes that are detected in multiple cross-cell comparisons. Here we supply the groups = sg argument, so that the error models for the two cell types are fit independently (using two different sets of "robust" genes). If the groups argument is omitted, the models will be fit using a common set.

Note this step takes a considerable amount of time unless multiple cores are used.
```{r}

```

## Step 7.7 | BPSC [not yet implemented]
BPSC uses the Poisson-Beta model of single-cell gene expression, which we discussed in the previous chapter, and combines it with generalized linear models which weâ€™ve already encountered when using edgeR. BPSC performs comparisons of one or more groups to a reference group (â€œcontrolâ€) and can include other factors such as batches in the model.

# STEP 10 | Seurat
[Seurat](https://satijalab.org/seurat/) is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. For more info see Butler et al. 2018 and Stuart, Butler et al. 2018. The following code has been adapted from the [Guided Clustering Tutorial](https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html).

_IMP:_ Before starting this, run "Part I: from a gene expression matrix to a SingleCellExperiment object" again to load the raw data. Seurat uses its own object class, so we need to create a new object with the raw (non-normalised) data. Setting up a Seurat object:
```{r}
library(Seurat)
library(dplyr)
PAG_seurat <- CreateSeuratObject(counts = PAG_data,
                                 project = "PAG_seurat_analysis",
                                 min.cells = 1, # keep genes expressed in more than 1 cell
                                 min.features = 2000, # keep cells with at least 2000 detected genes
                                 meta.data = PAG_metadata)
```

## Step 10.1 | Pre-processing, data QC and normalization
Selection and filtration of cells based on QC metrics, data normalization and scaling, and detection of highly variable features.
```{r}
# The [[ operator can add columns to object metadata. This is a great place to stash QC stats
PAG_seurat[["percent.mt"]] <- PercentageFeatureSet(PAG_seurat, pattern = "^MT-")
PAG_seurat[["percent.ercc"]] <- PercentageFeatureSet(PAG_seurat, pattern = "^ERCC-")
head(PAG_seurat@meta.data, 5) # Metadata for the first 5 cells
```

Visualize QC metrics using a violin plot:
```{r}
VlnPlot(PAG_seurat, features = c("nFeature_RNA", "nCount_RNA", "percent.mt", "percent.ercc"), ncol = 4)
```

```{r}
# FeatureScatter is typically used to visualize feature-feature relationships, but can be used
# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.

plot1 <- FeatureScatter(PAG_seurat, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(PAG_seurat, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
CombinePlots(plots = list(plot1, plot2))
```

Filter the dataset according to specific quality  metrics:
```{r}
PAG_seurat <- subset(PAG_seurat, subset = nFeature_RNA > 2000 & percent.mt < 5)
```

After removing unwanted cells from the dataset, the next step is to normalize the data. By default, Seurat employs a global-scaling normalization method `LogNormalize` that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. Normalized values are stored in `SeuratObject[["RNA"]]@data`.
```{r}
PAG_seurat <- NormalizeData(PAG_seurat, normalization.method = "LogNormalize", scale.factor = 10000)
PAG_seurat <- NormalizeData(PAG_seurat)
```

## Step 10.2 | Feature selection: identify highly variable features
The next step consists on calculating a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). Focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets. Seurat directly models the mean-variance relationship inherent in single-cell data and implements in in the `FindVariableFeatures` function, returning 2000 features per dataset by default.
```{r}
PAG_seurat <- FindVariableFeatures(PAG_seurat, selection.method = "vst", nfeatures = 2000)

# Identify the 10 most highly variable genes
top10 <- head(VariableFeatures(PAG_seurat), 10)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(PAG_seurat)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
CombinePlots(plots = list(plot1, plot2))
```

## Step 10.3 | Scaling the data, peforming dimensionality reduction and determining the dimensionality of the dataset
Linear transformation (scaling) is a standard pre-processing step prior to dimensionality reduction techniques like PCA. The `ScaleData` function:

* Shifts the expression of each gene, so that the mean expression across cells is 0
* Scales the expression of each gene, so that the variance across cells is and highly-expressed genes do not dominate
* Stores the resuts in `SeuratObject[["RNA"]]@scale.data`
```{r}
all.genes <- rownames(PAG_seurat)
PAG_seurat <- ScaleData(PAG_seurat, features = all.genes)

# Scaling is an essential step in the Seurat workflow, but only on genes that will be used as input to PCA. To speed things up, the default in  ScaleData is only to perform scaling on the previously identified variable features (2,000 by default). To do this, omit the  features argument in the previous function call.
```

We can now perform PCA on the scaled data. By default, only the previously determined variable features are used as input, but can be defined using `features` argument if you wish to choose a different subset.
```{r}
PAG_seurat <- RunPCA(PAG_seurat, features = VariableFeatures(object = PAG_seurat))
```

Seurat provides several useful ways of visualizing both cells and features that define the PCA, including `VizDimReduction`, `DimPlot`, and `DimHeatmap`.
```{r}
# Examine and visualize PCA results a few different ways
print(PAG_seurat[["pca"]], dims = 1:5, nfeatures = 5)
VizDimLoadings(PAG_seurat, dims = 1:2, reduction = "pca")
DimPlot(PAG_seurat, reduction = "pca")
```

`DimHeatmap` allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. Both cells and features are ordered according to their PCA scores. Setting cells to a number plots the extreme cells on both ends of the spectrum, which dramatically speeds plotting for large datasets. Though clearly a supervised analysis, this is a valuable tool for exploring correlated feature sets.
```{r}
DimHeatmap(PAG_seurat, dims = 1, cells = 500, balanced = TRUE)
DimHeatmap(PAG_seurat, dims = 1:15, cells = 500, balanced = TRUE)
```

To overcome the extensive technical noise in any single feature for scRNA-seq data, Seurat clusters cells based on their PCA scores, with each PC essentially representing a metafeature that combines information across a correlated feature set. The top principal components therefore represent a robust compression of the dataset. However, how many componenets should we choose to include? 10? 20? 100?

In Macosko et al, they implemented a resampling test inspired by the JackStraw procedure. They randomly permuted a subset of the data (1% by default) and rerun PCA, constructing a null distribution of feature scores, and repeat this procedure. They then identify significant PCs as those with a strong enrichment of low p-value features.
```{r}
# NOTE: This process can take a long time for big datasets.
# Other techniques such as those implemented in ElbowPlot() can be used to reduce computation time
PAG_seurat <- JackStraw(PAG_seurat, num.replicate = 100)
PAG_seurat <- ScoreJackStraw(PAG_seurat, dims = 1:20)
```

The `JackStrawPlot` function provides a visualization tool for comparing the distribution of p-values for each PC with a uniform distribution (dashed line). Significant PCs will show a strong enrichment of features with low p-values (solid curve above the dashed line).
```{r}
JackStrawPlot(PAG_seurat, dims = 1:15)
```

An alternative heuristic method generates an `Elbow plot`: a ranking of principle components based on the percentage of variance explained by each one (`ElbowPlot` function). The elbow suggests a point by which the majority of true signal is captured.
```{r}
ElbowPlot(PAG_seurat)
```

Identifying the true dimensionality of a dataset can be challenging/uncertain for the user. It is therefore a good idea to try the three different methods described above. The first is more supervised, exploring PCs to determine relevant sources of heterogeneity, and could be used in conjunction with GSEA for example. The second implements a statistical test based on a random null model, but is time-consuming for large datasets, and may not return a clear PC cutoff. The third is a heuristic that is commonly used, and can be calculated instantly. It is also good practice to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). Although the results often do not differ dramatically, it is best to err on the higher side when choosing this parameter.

## Step 10.4 | Clustering
Seurat v3 applies a graph-based clustering approach. It first constructs a KNN graph based on the euclidean distance in PCA space, and refine the edge weights between any two cells based on the shared overlap in their local neighborhoods (Jaccard similarity). This step is performed using the `FindNeighbors` function, and takes as input the previously defined dimensionality of the dataset.

To cluster the cells, Seurat next applies modularity optimization techniques such as the Louvain algorithm (default) or SLM [SLM, Blondel et al., Journal of Statistical Mechanics], to iteratively group cells together, with the goal of optimizing the standard modularity function. The `FindClusters` function implements this procedure, and contains a resolution parameter that sets the "granularity" of the downstream clustering, with increased values leading to a greater number of clusters. We find that setting this parameter between 0.4-1.2 typically returns good results for single-cell datasets of around 3K cells. Optimal resolution often increases for larger datasets. The clusters can be found using the `Idents` function.
```{r}
PAG_seurat <- FindNeighbors(PAG_seurat, dims = 1:10)
PAG_seurat <- FindClusters(PAG_seurat, resolution = 0.5)
head(Idents(PAG_seurat), 5) # Look at cluster IDs of the first 5 cells
```

## Step 10.5 | Non-linear dimensionality reduction (UMAP and t-SNE)
Seurat offers several non-linear dimensional reduction techniques, such as tSNE and UMAP, to visualize and explore these datasets. The goal of these algorithms is to learn the underlying manifold of the data in order to place similar cells together in a low-dimensional space. Cells within the graph-based clusters determined above should co-localize on these dimension reduction plots. Ideally use the same PCs input to the clustering analysis as input to the UMAP and tSNE.
```{r}
PAG_seurat <- RunTSNE(PAG_seurat, dims = 1:10)
DimPlot(PAG_seurat, reduction = "tsne")
```

You can save the object at this point so that it can easily be loaded back in without having to rerun the computationally intensive steps performed above, or easily shared with collaborators.
```{r}
saveRDS(PAG_seurat, file = "../output/PAG_seurat_analysis.rds")
```

## Step 10.6 | Finding differentially expressed features (cluster biomarkers)
Seurat can help find markers that define clusters via differential expression. By default, it identifes positive and negative markers of a single cluster (specified in ident.1), compared to all other cells. `FindAllMarkers` automates this process for all clusters, but you can also test groups of clusters vs. each other, or against all cells.

The `min.pct` argument requires a feature to be detected at a minimum percentage in either of the two groups of cells, and the `thresh.test` argument requires a feature to be differentially expressed (on average) by some amount between the two groups. You can set both of these to 0, but with a dramatic increase in time - since this will test a large number of features that are unlikely to be highly discriminatory. As another option to speed up these computations, `max.cells.per.ident` can be set. This will downsample each identity class to have no more cells than whatever this is set to. While there is generally going to be a loss in power, the speed increases can be significiant and the most highly differentially expressed features will likely still rise to the top.
```{r}
# find all markers of cluster 1
cluster1.markers <- FindMarkers(PAG_seurat, ident.1 = 1, min.pct = 0.25)
head(cluster1.markers, n = 5)
```

```{r}
# find all markers distinguishing cluster 3 from clusters 0 and 2
cluster3.markers <- FindMarkers(PAG_seurat, ident.1 = 3, ident.2 = c(0, 1, 2), min.pct = 0.25)
head(cluster3.markers, n = 10)
```

```{r}
# find markers for every cluster compared to all remaining cells, report only the positive ones
PAG_seurat.markers <- FindAllMarkers(PAG_seurat, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)
PAG_seurat.markers %>% group_by(cluster) %>% top_n(n = 2, wt = avg_logFC)
```

Seurat has several tests for differential expression which can be set with the `test.use` parameter. For example, the ROC test returns the "classification power" for any individual marker (ranging from 0 - random, to 1 - perfect).
```{r}
cluster1.markers <- FindMarkers(PAG_seurat, ident.1 = 0, logfc.threshold = 0.25, test.use = "roc", only.pos = TRUE)
```

There are several tools for visualizing marker expression. `VlnPlot` (shows expression probability distributions across clusters), and `FeaturePlot` (visualizes feature expression on a tSNE or PCA plot) are the most commonly used visualizations, but additional methods are `RidgePlot`, `CellScatter`, and `DotPlot`.
```{r}
VlnPlot(PAG_seurat, features = c("Sst", "Npy"))

# you can plot raw counts as well
VlnPlot(PAG_seurat, features = c("Sst", "Npy"), slot = "counts", log = TRUE)

FeaturePlot(PAG_seurat, features = c("MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP", 
    "CD8A"))
```

`DoHeatmap` generates an expression heatmap for given cells and features.
```{r}
top10 <- PAG_seurat.markers %>% group_by(cluster) %>% top_n(n = 10, wt = avg_logFC)
DoHeatmap(PAG_seurat, features = top10$gene) + NoLegend()
```

## Step 10.7 | Assigning cell type identity to clusters
Finally, we can rename/assign an identity to the clusters we have been able to identify based on the data.
```{r}
new.cluster.ids <- c("Naive CD4 T", "Memory CD4 T", "CD14+ Mono", "B", 
                     "CD8 T", "FCGR3A+ Mono", "NK", "DC", "Platelet")
names(new.cluster.ids) <- levels(PAG_seurat)
PAG_seurat <- RenameIdents(PAG_seurat, new.cluster.ids)
DimPlot(PAG_seurat, reduction = "umap", label = TRUE, pt.size = 0.5) + NoLegend()
```

Save the complete analysis:
```{r}
saveRDS(PAG_seurat, file = "../output/PAG_seurat_analysis_final.rds")
```