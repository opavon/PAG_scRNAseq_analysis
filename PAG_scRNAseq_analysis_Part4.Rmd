---
title: "Topographic, single-cell gene expression profiling of Periaqueductal Gray neurons"
subtitle: "Part IV: modelling the technical noise and removing batch effects"
author:
  - name: "Oriol Pavon Arocas, Sarah F. Olesen, and Tiago Branco"
    affiliation: "Sainsbury Wellcome Centre for Neural Circuits and Behaviour, University College London, UK"
    email: "oriol.pavon.16@ucl.ac.uk"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_notebook:
    highlight: pygments
    number_sections: FALSE
    theme: lumen
    toc: TRUE
    toc_float: TRUE

#output: rmdformats::readthedown:
  #highlight: pygments
---

***
This is a pipeline to analyse single-cell RNA sequencing data from Periaqueductal Gray neurons (1) isolated from acute midbrain slices of transgenic mice using visually guided aspiration via patch pipettes and (2) processed using SMART-seq2 (Picelli et al. Nature Protocols 2014). 

This pipeline has been generated after attending the [EMBL-EBI RNA-Sequence Analysis Course](https://www.ebi.ac.uk/training/events/2019/rna-sequence-analysis) and [attending](https://training.csx.cam.ac.uk/bioinformatics/event/2823386) and following the online course on [Analysis of single cell RNA-seq data](https://github.com/hemberg-lab/scRNA.seq.course) by the [Hemberg Lab](https://www.sanger.ac.uk/science/groups/hemberg-group). Many other resources have been used, including the [Orchestrating Single-Cell Analysis with Bioconductor book](https://osca.bioconductor.org/) by Robert Amezquita and Stephanie Hicks, the [simpleSingleCell workflow in Bioconductor](https://bioconductor.org/packages/3.9/workflows/html/simpleSingleCell.html) maintained by Aaron Lun, the [rnaseqGene workflow](https://bioconductor.org/packages/release/workflows/html/rnaseqGene.html) maintained by Michael Love, the [RNAseq123 workflow](https://bioconductor.org/packages/release/workflows/html/RNAseq123.html) maintained by Matthew Ritchie, and the [EGSEA123 workflow](https://bioconductor.org/packages/release/workflows/html/EGSEA123.html) maintained by Matthew Ritchie.

Other key resources are Bioconductor (Huber et al., Nature Methods 2015), [scRNA-tools](https://www.scrna-tools.org/), `scater` (McCarty et al., Bioinformatics 2017), `scran` (Lun et al. F1000Res 2016), `SC3` (Kiselev et al., Nature Methods 2017), `Seurat` (Butler et al., Nature Biotechnology 2018), `clusterExperiment` (Risso et al., PLOS Computational Biology 2018), `limma` (Ritchie et al., Nucleic Acids Research 2015), `DESeq2` (Love et al., Genome Biology 2014), `iSEE` (Rue-Albrecht & Marini et al., F1000Research 2018), `t-SNE` (van der Maaten & Hinton, Journal of Machine Learning Research 2008).

***

# STEP 4 | Modelling technical and biological variability in gene expression and identifying highly variable genes
__Based on the [simpleSingleCell workflow in Bioconductor](https://bioconductor.org/packages/3.9/workflows/html/simpleSingleCell.html) and [Description of the HVG machinery in scran](https://github.com/LTLA/HVGDetection2018) maintained by Aaron Lun.__

We continue using the `PAG_sceset_qc_norm` after normalization. We should thus have a `logcounts` slot in`assays`:
```{r}
# If starting from stored results, load saved filtered dataset from previous Step:
set.seed(1991)
options(stringsAsFactors = FALSE)
library(SingleCellExperiment)
library(scater)
library(scran)

PAG_sceset_qc_norm <- readRDS("PAG_sceset_qc_norm.rds") # Contains filtered cells and genes, and normalized data
assayNames(PAG_sceset_qc_norm)
PAG_sceset_qc_norm
```

## Step 4a.1 | Modelling the technical component of variation
Variability in the observed expression values across genes can be driven by genuine biological heterogeneity or uninteresting technical noise. To distinguish between these two possibilties, we need to model the technical component of the variance of the expression values for each gene. 

We will use the `trendVar()` function to fit a mean-variance trend to the read counts.

* We can set `block=` to block on the plate/animal of origin for each cell, to ensure that technical differences between plates do not inflate the variances. This involves estimating the mean and variance of the log-expression separately in each plate, followed by fitting a single trend to the plate-specific means and variances of all spike-in transcripts. In doing so, we implicitly assume that the trend is the same between plates. The use of `block=` also assumes that the average size factor within each plate is close to unity for both endogenous genes and spike-in transcripts.
* Some tuning of trend parameters such as `span` may be required to achieve a suitable fit (default is 0.75).
* Setting `parametric=TRUE` is especially useful for modelling the expected wave-like shape of the mean-variance relationship. (This is not the default setting as it is not robust for arbitrary trend shapes.)

In addition, the `trendVar` function automatically filters out low-abundance genes prior to trend fitting. This ensures that low-abundance genes do not interfere with the fit due to discreteness, which biases the estimate of variability of the variances around the trend; or due to the frequency of low-abundance genes, which reduces the sensitivity of span-based smoothing algorithms at higher abundances. The internal choice of filtering strategy involves a number of considerations:

* Filtering uses the average of log-expression values rather than the (library size-adjusted) average count. The mean log-expression is independent of the variance estimate in a linear modelling framework (Bourgon, Gentleman, and Huber 2010), which ensures that the filter does not introduce spurious trends in the variances at the filter boundary.
* The filter threshold is specified with the `min.mean` argument in `trendVar`. We use the default threshold of 0.1 (`min.mean`) based on the appearance of discrete patterns in the variance estimates for simulated Poisson-distributed counts. Lower thresholds of 0.001-0.01 may be more suitable for very sparse data, e.g., from droplet-based protocols.
* The filter used in `trendVar` is not applied in `decomposeVar` by default. Retention of all genes ensures that weak biological signal from rare subpopulations is not discarded. To apply the filter in `decomposeVar`, users should set `subset.row=rowMeans(logcounts(sce)) > 0.1` in the function call.

__One last comment from Aaron Lun__: On occasion, users may observe a warning from `trendVar()` about the lack of centering in the size factors. Recall that the trend is fitted to the mean and variances of the spike-in transcripts, and the technical component for each endogenous gene is estimated by interpolation. This assumes that an endogenous gene is comparable to a spike-in transcript of the same abundance. In particular, we assume that variation is primarily driven by the magnitude of the counts, based on the well-known mean-variance relationships in count models. Thus, we need to ensure that similarities in the average counts are preserved in the normalized expression values. This is achieved by centering the gene- and spike-in-based size factors in `normalize()`, such that features with similar average counts will also have similar normalized abundances. However, if the `SingleCellExperiment` object was manipulated (e.g., subsetted) after `normalize()` and before `trendVar()`, centering may not be preserved - hence the warning.

#### 4a.1.1 | Using spike-ins
One way to do so is using the set of spike-in transcripts. These were in theory added in the same quantity to each cell. Thus, the spike-in transcripts should exhibit non-biological variability, i.e., any variance in their counts should be technical in origin.

We use the `trendVar()` function to fit a mean-dependent trend to the variances of the log-expression values for the spike-in transcripts. Given the mean abundance of a gene, the fitted value of the trend is then used as an estimate of the technical component for that gene. The biological component of the variance is finally calculated by subtracting the technical component from the total variance of each gene with the `decomposeVar` function.
```{r}
# Fit variance-mean trend without blocking
start_time_s1 <- Sys.time() # Takes around 1s
var_fit_spikes <- trendVar(PAG_sceset_qc_norm,
                           #method="loess",
                           use.spikes=TRUE, 
                           parametric=TRUE,
                           min.mean=0.1,
                           loess.args=list(span=0.3)
                           ) 

var_out_spikes <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes)
head(var_out_spikes)
end_time_s1 <- Sys.time()
end_time_s1 - start_time_s1

# Fit variance-mean trend blocking on mouse.id
start_time_s2 <- Sys.time() # Takes around 2s
var_fit_spikes_mouseid <- trendVar(PAG_sceset_qc_norm,
                                   #method="loess",
                                   use.spikes=TRUE, 
                                   parametric=TRUE, 
                                   block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                   min.mean=0.1,
                                   loess.args=list(span=0.3)
                                   ) 

var_out_spikes_mouseid <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes_mouseid)
head(var_out_spikes_mouseid)
end_time_s2 <- Sys.time()
end_time_s2 - start_time_s2

# Fit variance-mean trend blocking on batch.processing
start_time_s3 <- Sys.time() # Takes around 2s
var_fit_spikes_batch <- trendVar(PAG_sceset_qc_norm,
                                 #method="loess",
                                 use.spikes=TRUE, 
                                 parametric=TRUE, 
                                 block=PAG_sceset_qc_norm$batch.processing, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                 min.mean=0.1,
                                 loess.args=list(span=0.3)
                                 ) 

var_out_spikes_batch <- decomposeVar(PAG_sceset_qc_norm, var_fit_spikes_batch)
head(var_out_spikes_batch)
end_time_s3 <- Sys.time()
end_time_s3 - start_time_s3
```

We visually inspect the trend to confirm that it corresponds to the spike-in variances. A wave-like shape is typical of the mean-variance trend for log-expression values. A linear increase in the variance is observed as the mean increases from zero, as larger variances are possible when the counts increase. At very high abundances, the effect of sampling noise decreases due to the law of large numbers, resulting in a decrease in the variance.
```{r}
# Plot variance-mean trend without blocking
plot(var_out_spikes$mean, 
     var_out_spikes$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression",
     ylab="Variance of log-expression")

curve(var_fit_spikes$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes$mean[current_spikes], var_out_spikes$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on mouse.id
plot(var_out_spikes_mouseid$mean, 
     var_out_spikes_mouseid$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression",
     ylab="Variance of log-expression")

curve(var_fit_spikes_mouseid$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes_mouseid$mean[current_spikes], var_out_spikes_mouseid$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on batch.processing
plot(var_out_spikes_batch$mean, 
     var_out_spikes_batch$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression",
     ylab="Variance of log-expression")

curve(var_fit_spikes_batch$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm)
points(var_out_spikes_batch$mean[current_spikes], var_out_spikes_batch$total[current_spikes], col="red", pch=16)
```

***
Notes from Aaron Lun's `simpleSingleCell` workflow:

* In practice, trend fitting is complicated by the small number of spike-in transcripts and the uneven distribution of their abundances. In the absence of spike-ins or as an alternative approach, we can set `use.spikes=FALSE` to fit a trend to the variances of the endogenous genes (see 4.1.2). Another alternative would be to create a trend based on the assumption of Poisson technical noise.
* Negative biological components are often obtained from `decomposeVar`. These are intuitively meaningless as it is impossible for a gene to have total variance below technical noise. Nonetheless, such values occur due to imprecise estimation of the total variance, especially for low numbers of cells.
* `decomposeVar` also yields p-values that can be used to define highly variable genes (HVGs) at a specific threshold for the false discovery rate (FDR). We will discuss this in more detail later, as formal detection of HVGs is not necessary for feature selection during data exploration.

#### 4a.1.2 | Without spike-ins
Ideally, the technical component would be estimated by fitting a mean-variance trend to the spike-in transcripts using the `trendVar` function as we have seen before. In practice, this strategy is compromised by the small number of spike-in transcripts, the uneven distribution of their abundances and (for low numbers of cells) the imprecision of their variance estimates. This makes it difficult to accurately fit a complex mean-dependent trend to the spike-in variances. In some datasets, spike-in RNA may not have been added in appropriate quantities (or indeed at all). It may also be inappropriate to assume Poisson technical noise with `makeTechTrend()`, especially for read count data where amplification noise is non-negligible. In such cases, an alternative approach is to fit the trend to the variance estimates of the endogenous genes, using the `use.spikes=FALSE` setting. This assumes that the majority of genes are not variably expressed, such that the technical component dominates the total variance for those genes. The fitted value of the trend is then used as an estimate of the technical component. 

__NB__: fitting the trend to the variances of the genes with `use.spikes=FALSE` probably overestimates the technical component.
```{r}
# Fit variance-mean trend without blocking
start_time1 <- Sys.time() # Takes around 5s
var_fit_no_spikes <- trendVar(PAG_sceset_qc_norm, 
                              method="loess", 
                              use.spikes=FALSE,
                              parametric=TRUE,
                              #block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                              min.mean=0.1,
                              loess.args=list(span=0.3)
                              ) 
var_out_no_spikes <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes)
head(var_out_no_spikes)
end_time1 <- Sys.time()
end_time1 - start_time1

# Fit variance-mean trend blocking on mouse.id
start_time2 <- Sys.time() # Takes around 32min
var_fit_no_spikes_mouseid <- trendVar(PAG_sceset_qc_norm, 
                                      method="loess", 
                                      use.spikes=FALSE,
                                      parametric=TRUE,
                                      block=PAG_sceset_qc_norm$mouse.id, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                      min.mean=0.1,
                                      loess.args=list(span=0.3)
                                      ) 
var_out_no_spikes_mouseid <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes_mouseid)
head(var_out_no_spikes_mouseid)
end_time2 <- Sys.time()
end_time2 - start_time2

# Fit variance-mean trend blocking on batch.processing
start_time3 <- Sys.time() # Takes around 32min
var_fit_no_spikes_batch <- trendVar(PAG_sceset_qc_norm, 
                                    method="loess", 
                                    use.spikes=FALSE,
                                    parametric=TRUE,
                                    block=PAG_sceset_qc_norm$batch.processing, # Try PAG_sceset_qc_norm$mouse.id/batch.processing
                                    min.mean=0.1,
                                    loess.args=list(span=0.3)
                                    ) 
var_out_no_spikes_batch  <- decomposeVar(PAG_sceset_qc_norm, var_fit_no_spikes_batch)
head(var_out_no_spikes_batch)
end_time3 <- Sys.time()
end_time3 - start_time3
```

We assess the suitability of the trend fitted to the endogenous variances by examining whether it is consistent with the spike-in variances. If the trend passes through or close to most of the spike-in variances, this indicates that our assumption (that most genes have low levels of biological variability) is valid. This strategy exploits the large number of endogenous genes to obtain a stable trend, with the spike-in transcripts used as diagnostic features rather than in the trend fitting itself. However, if our assumption does not hold, we would instead fit the trend directly to the spike-in variances with the default use.spikes=TRUE. This sacrifices stability to reduce systematic errors in the estimate of the biological component for each gene.
```{r}
# Plot variance-mean trend without blocking
plot(var_out_no_spikes$mean, 
     var_out_no_spikes$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes$mean[current_spikes], var_out_no_spikes$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on mouse.id
plot(var_out_no_spikes_mouseid$mean, 
     var_out_no_spikes_mouseid$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on mouse.id", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes_mouseid$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes_mouseid$mean[current_spikes], var_out_no_spikes_mouseid$total[current_spikes], col="red", pch=16)

# Plot variance-mean trend blocking on batch.processing
plot(var_out_no_spikes_batch$mean, 
     var_out_no_spikes_batch$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression - block on batch.processing", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes_batch$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc_norm) 
points(var_out_no_spikes_batch$mean[current_spikes], var_out_no_spikes_batch$total[current_spikes], col="red", pch=16)
```

## Step 4a.2 | Identifying highly variable genes
HVGs are defined as genes with biological components that are significantly greater than zero. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. Formal detection of HVGs allows us to avoid genes that are highly variable due to technical factors such as sampling noise during RNA capture and library preparation. This adds another level of statistical rigour to our previous analyses, in which we only modelled the technical component.

Identifying HVGs requires estimation of the variance in expression for each gene, followed by decomposition of the variance into biological and technical components (done in the previous step). HVGs are then identified as those genes with the largest biological components. 

We define HVGs as genes with biological components that are significantly greater than zero at a false discovery rate (FDR) of 5% or 1%. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. In addition, we only consider a gene to be a HVG if it has a biological component greater than or equal to 0.5. For transformed expression values on the log2 scale, this means that the average difference in true expression between any two cells will be at least 2-fold. (This reasoning assumes that the true log-expression values are Normally distributed with variance of 0.5. The root-mean-square of the difference between two values is treated as the average log2-fold change between cells and is equal to unity.) We rank the results by the biological component to focus on genes with larger biological variability.
```{r}
# Consider playing with the $FDR threshold and the $bio threshold, try reducing/increasing them to make them more stringent or to get more genes.

# from trendVar with spikes
hvg_out_spikes <- var_out_spikes[which(var_out_spikes$FDR <= 0.05 & var_out_spikes$bio >= 0.5), ]
nrow(hvg_out_spikes)

hvg_out_spikes_mouseid <- var_out_spikes_mouseid[which(var_out_spikes_mouseid$FDR <= 0.05 & var_out_spikes_mouseid$bio >= 0.5), ]
nrow(hvg_out_spikes_mouseid)

hvg_out_spikes_batch <- var_out_spikes_batch[which(var_out_spikes_batch$FDR <= 0.05 & var_out_spikes_batch$bio >= 0.5), ]
nrow(hvg_out_spikes_batch)


# from trendVar without spikes
hvg_out_no_spikes <- var_out_no_spikes[which(var_out_no_spikes$FDR <= 0.05 & var_out_no_spikes$bio >= 0.5), ]
nrow(hvg_out_no_spikes)

hvg_out_no_spikes_mouseid <- var_out_no_spikes_mouseid[which(var_out_no_spikes_mouseid$FDR <= 0.05 & var_out_no_spikes_mouseid$bio >= 0.5), ]
nrow(hvg_out_no_spikes_mouseid)

hvg_out_no_spikes_batch <- var_out_no_spikes_batch[which(var_out_no_spikes_batch$FDR <= 0.05 & var_out_no_spikes_batch$bio >= 0.5), ]
nrow(hvg_out_no_spikes_batch)
```

#### 4a.2.1 | Rank and filter the results
We rank the results to focus on genes with larger biological components. This highlights an interesting aspect of the underlying hypothesis test, which is based on the ratio of the total variance to the expected technical variance. Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be interesting. This is because the ratio can be very large for HVGs that have very low total variance and do not contribute much to the cell-cell heterogeneity.
```{r}
# from trendVar with spikes
hvg_out_spikes <- hvg_out_spikes[order(hvg_out_spikes$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes)
head(hvg_out_spikes)

hvg_out_spikes_mouseid <- hvg_out_spikes_mouseid[order(hvg_out_spikes_mouseid$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes_mouseid)
head(hvg_out_spikes_mouseid)

hvg_out_spikes_batch <- hvg_out_spikes_batch[order(hvg_out_spikes_batch$bio, decreasing=TRUE), ]
nrow(hvg_out_spikes_batch)
head(hvg_out_spikes_batch)

# from trendVar without spikes
hvg_out_no_spikes <- hvg_out_no_spikes[order(hvg_out_no_spikes$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes)
head(hvg_out_no_spikes)

hvg_out_no_spikes_mouseid <- hvg_out_no_spikes_mouseid[order(hvg_out_no_spikes_mouseid$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes_mouseid)
head(hvg_out_no_spikes_mouseid)

hvg_out_no_spikes_batch <- hvg_out_no_spikes_batch[order(hvg_out_no_spikes_batch$bio, decreasing=TRUE), ]
nrow(hvg_out_no_spikes_batch)
head(hvg_out_no_spikes_batch)
```

__One important thing...__
Before moving on to downstream analysis, we consider dropping any Ribosomal, Mitochondrial, ERCC, sex-specific genes (e.g. XIST), transgenes (EYFP, tdTomato, Cre, TSO concatamers) and genes used for transgenic labeling of cells (VGAT and VGluT2) from the dataset, as some of them are not be biologically informative and others have been used to select the cells. We do so for both our `SingleCellExperiment` object and for the different HVGs:
```{r}
# Identify the genes we want to drop:
is_X_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "X") # Find which genes correspond to chromosome X 
is_Y_qc_norm <- (rowData(PAG_sceset_qc_norm)$Chromosome == "Y") # Find which genes correspond to chromosome Y
is_feature_control_qc_norm <- (rowData(PAG_sceset_qc_norm)$is_feature_control) # Find which genes correspond to feature controls (Ribosomal, Mitochondrial, ERCC, transgenes)

filter_genes_qc_norm <- !(is_X_qc_norm | is_Y_qc_norm | is_feature_control_qc_norm)

# Apply the filter to the SCE object:
dim(PAG_sceset_qc_norm)
PAG_sceset_qc_norm_filt <- PAG_sceset_qc_norm[filter_genes_qc_norm, ]
dim(PAG_sceset_qc_norm_filt)

# Apply the filter to the HVGs:
filt_hvg_1 <- rownames(hvg_out_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_filt <- hvg_out_spikes[filt_hvg_1, ]

filt_hvg_2 <- rownames(hvg_out_spikes_mouseid) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_mouseid_filt <- hvg_out_spikes_mouseid[filt_hvg_2,]

filt_hvg_3 <- rownames(hvg_out_spikes_batch) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_spikes_batch_filt <- hvg_out_spikes_batch[filt_hvg_3,]

filt_hvg_4 <- rownames(hvg_out_no_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_filt <- hvg_out_no_spikes[filt_hvg_4,]

filt_hvg_5 <- rownames(hvg_out_no_spikes_mouseid) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_mouseid_filt <- hvg_out_no_spikes_mouseid[filt_hvg_5,]

filt_hvg_6 <- rownames(hvg_out_no_spikes_batch) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_out_no_spikes_batch_filt <- hvg_out_no_spikes_batch[filt_hvg_6,]
```

#### 4a.2.2 | Visualize, store, and export HVGs
Now that we have removed thegenes we are not interested in, we can use `limma` and `vennDiagram` to compare how many genes we get in each condition with or without spike-ins, and with or without blocking:
```{r}
library(limma)
#####
# Inspect the overlap between HVGs obtained with or without using spike-ins to fit a variance trend:
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_mouseid_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_spikes_batch_filt))
venn_diag_spikes_nospikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_filt),
                                              rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_filt))
                                        )

vennDiagram(venn_diag_spikes_nospikes,
            names = c("Spikes", "No_Spikes"),
            circle.col = c("#E69F00", "#56B4E9")
            )
venn_diag_spikes_nospikes_mouseid <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_mouseid_filt),
                                                      rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt))
                                                )

vennDiagram(venn_diag_spikes_nospikes_mouseid,
            names = c("Spikes_MouseID", "No_Spikes_MouseID"),
            circle.col = c("#E69F00", "#56B4E9")
            )

venn_diag_spikes_nospikes_batch <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_batch_filt),
                                                    rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_batch_filt))
                                              )
vennDiagram(venn_diag_spikes_nospikes_batch,
            names = c("Spikes_Batch", "No_Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9")
            )

#####
# Inspect the overlap between HVGs obtained using spike-ins to fit a variance trend, and with or without blocking:
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_spikes_batch_filt) %in% rownames(hvg_out_spikes_mouseid_filt))
sum(rownames(hvg_out_spikes_filt) %in% rownames(hvg_out_spikes_batch_filt))
venn_diag_spikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_filt),
                                     rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_mouseid_filt),
                                     rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_spikes_batch_filt))
                               )
vennDiagram(venn_diag_spikes,
            names = c("Spikes", "Spikes_MouseID", "Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9", "#009E73")
            )

#####
# Inspect the overlap between HVGs obtained without using spike-ins to fit a variance trend, and with or without blocking:
sum(rownames(hvg_out_no_spikes_mouseid_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_no_spikes_filt))
sum(rownames(hvg_out_no_spikes_batch_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt))
venn_diag_no_spikes <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_filt),
                                        rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_mouseid_filt),
                                        rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_out_no_spikes_batch_filt))
                                  )
vennDiagram(venn_diag_no_spikes,
            names = c("No_Spikes", "No_Spikes_MouseID", "No_Spikes_Batch"),
            circle.col = c("#E69F00", "#56B4E9", "#009E73")
            )
```

Taking into account the above Venn Diagrams, the plots of the mean-variance trend we obtained in `Step 4a.1`, and the profiles and total numbers of HVGs selected by each approach, we will proceed by using the HVGs obtained without using spike-ins and without blocking, the trend and variance-mean plot of which achieves the best compromise between spike-ins and endogenous genes.

Before we store the identified HVGs in our `SingleCellExperiment` object, we check the distribution of expression values for the genes with the largest biological components to ensure that the variance estimate is not being dominated by one or two outlier cells.
```{r}
fontsize <- theme(axis.text=element_text(size=12), axis.title=element_text(size=16))

#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_mouseid_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_spikes_batch_filt)[1:20]) + fontsize

plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_filt)[1:50]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_mouseid_filt)[1:20]) + fontsize
#plotExpression(PAG_sceset_qc_norm_filt, features=rownames(hvg_out_no_spikes_batch_filt)[1:20]) + fontsize
```

Now that we have ordered, filtered, and inspected the identified HVGs we can add the results into the `metadata` component of the `PAG_sceset_qc_norm_filt` object. The metadata component can hold any object, as it is a list container. Any results that we’d like to keep are safe to store here, and a great way to save or share intermediate results that would otherwise be kept in separate objects. Even though we have decided to use the HVGs identified by fitting `trendVar` to the endogenous genes (i.e. without using spike-ins) without any blocking factor, we store the HVGs from the different approaches we used (even without filtering) so we can go back and find them again for any future comparisons we might want to do:
```{r}
# hvg_out_spikes
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes <- rownames(hvg_out_spikes)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_filt <- rownames(hvg_out_spikes_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_filt)

# hvg_out_spikes_mouseid
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid <- rownames(hvg_out_spikes_mouseid)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid_filt <- rownames(hvg_out_spikes_mouseid_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_mouseid_filt)

# hvg_out_spikes_batch
metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch <- rownames(hvg_out_spikes_batch)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch_filt <- rownames(hvg_out_spikes_batch_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_spikes_batch_filt)

# hvg_out_no_spikes
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes <- rownames(hvg_out_no_spikes)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt <- rownames(hvg_out_no_spikes_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt)

# hvg_out_no_spikes_mouseid
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid <- rownames(hvg_out_no_spikes_mouseid)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid_filt <- rownames(hvg_out_no_spikes_mouseid_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_mouseid_filt)

# hvg_out_no_spikes_batch
metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch <- rownames(hvg_out_no_spikes_batch)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch)

metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch_filt <- rownames(hvg_out_no_spikes_batch_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_batch_filt)
```

We can also export them into a `.tsv` file:
```{r}
# Before filtering unwanted genes:
#write.table(file="PAG_hvg_out_spikes.tsv", hvg_out_spikes, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_mouseid.tsv", hvg_out_spikes_mouseid, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_batch.tsv", hvg_out_spikes_batch, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes.tsv", hvg_out_no_spikes, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_mouseid.tsv", hvg_out_no_spikes_mouseid, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_batch.tsv", hvg_out_no_spikes_batch, sep="\t", quote=FALSE, col.names=NA)

# After filtering unwanted genes:
#write.table(file="PAG_hvg_out_spikes_filt.tsv", hvg_out_spikes_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_mouseid_filt.tsv", hvg_out_spikes_mouseid_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_spikes_batch_filt.tsv", hvg_out_spikes_batch_filt, sep="\t", quote=FALSE, col.names=NA)
write.table(file="PAG_hvg_out_no_spikes_filt.tsv", hvg_out_no_spikes_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_mouseid_filt.tsv", hvg_out_no_spikes_mouseid_filt, sep="\t", quote=FALSE, col.names=NA)
#write.table(file="PAG_hvg_out_no_spikes_batch_filt.tsv", hvg_out_no_spikes_batch_filt, sep="\t", quote=FALSE, col.names=NA)
```

***
Notes from Aaron Lun's `simpleSingleCell` workflow:

One reason to use the variance of the log-expression values is the fact that the log-transformation protects against genes with strong expression in only one or two cells. This reduces the risk that the set of top HVGs is not dominated by genes with (mostly uninteresting) outlier expression patterns. There are, however, many other strategies for defining HVGs, based on a variety of metrics:

* the coefficient of variation, using the `technicalCV2()` function (Brennecke et al. 2013) or the `DM()` function (Kim et al. 2015) in _scran_.
* the dispersion parameter in the negative binomial distribution, using the `estimateDisp()` function in _edgeR_ (McCarthy, Chen, and Smyth 2012).
* a proportion of total variability, using methods in the _BASiCS_ package (Vallejos, Marioni, and Richardson 2015).

# STEP 5 | Identifying confounding factors and correcting batch effects factors
To account for technical confounders we need to identify and remove sources of variation in the expression data that are not related to the biological signal of interest. We could use spike-ins for this (in theory you add the same amount of ERCC in each cell lysate, so any variability should be technical noise), but these can turn out to be extremely variable across cells. Instead, we could use endogenous or housekeeping genes that do not vary systematically between cells. Where we have a large number of endogenous genes that, on average, do not vary systematically between cells and where we expect technical effects to affect a large number of genes (a very common and reasonable assumption), then such methods of batch correction can perform well.

The most prominent technical covariates in single-cell data are count depth and batch (Luecken & Theis, Mol Syst Biol. 2019). Correcting for batch effects between samples or cells in the same experiment is the classical scenario known as _batch correction_ from bulk RNA-seq. This is different from _data integration_ from multiple experiments or collaborative projects. Irrespective of computational methods, the best method of batch correction is pre-empting the effect and avoiding it altogether by clever experimental design (Hicks et al, 2017). Batch effects can be avoided by pooling cells across experimental conditions and samples. 

Our data have been collected from samples originated from several animals and processed on different days. Small uncontrollable differences in processing between batches (changes in operator, differences in reagent quality) can result in systematic differences in the observed expression in cells from different batches, which are refered to as "batch effects". Such differences are not interesting and can be problematic as they can be major drivers of heterogeneity in the data, masking the relevant biological differences and complicating interpretation of the results.

## Step 5.1 | Identifying confounding and technical factors
As we have mentioned, a big source of variability in scRNA-seq data, besides bad cells, are batch effects: the person processing the samples, date of processing, reagent kit, etc. We want to try to identify such effects and remove them, so our analysis picks up mainly biological effects.

### 5.1.1 | Screening for obvious biases with the table function
The `table` and `summary` functions allow us to quickly inspect if we have any obvious biases or confounds (i.e. all the cells come from male or female mice, or from a particular location or sequencing batch).
```{r}
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$PAG.hemisphere)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$PAG.areacollection)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$PAG.APaxis)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$mouse.sex)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$mouse.id)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$batch.processing)
```

```{r}
summary(PAG_sceset_qc_norm_filt$cell.type)
summary(PAG_sceset_qc_norm_filt$PAG.hemisphere)
summary(PAG_sceset_qc_norm_filt$PAG.areacollection)
summary(PAG_sceset_qc_norm_filt$mouse.sex)
```

### 5.1.2 | Checking for important technical factors or explanatory variables
We check whether there are technical factors that contribute substantially to the heterogeneity of gene expression. If so, the factor may need to be regressed out to ensure that it does not inflate the variances or introduce spurious correlations. scater can compute the marginal R^2^ for each variable when fitting a linear model regressing expression values for each gene against just that variable, and display a density plot of the gene-wise marginal R^2^ values for the variables.

For each gene, it calculates the percentage of the variance of the expression values that is explained by the variable. Small percentages (1-3%) indicate that the expression profiles of most genes are not strongly associated with this factor. If, on the other hand, we get density curves that are shifted to the right (i.e. with a peak towards the 100% end of the x-axis), this tells us that for a large proportion of the genes in our dataset this particular variable explains a large proportion of the variation.
```{r}
# How much of the variance does each variable explain? Some of these variables are calculated when we run the CalculateQC function from scater, and others are added by us in the metadata (such as batch or individual).

# After scran normalization
plotExplanatoryVariables(PAG_sceset_qc_norm_filt,
                         nvars_to_plot = 16,
                         exprs_values = "logcounts",
                         variables = c("mouse.id",
                                       "mouse.sex",
                                       "mouse.age",
                                       "mouse.singlehousedays",
                                       "cell.type",
                                       "cell.fluorophor",
                                       "slice.number",
                                       "slice.depth",
                                       "PAG.areacollection",
                                       "PAG.hemisphere",
                                       "PAG.APaxis",
                                       "time.sinceslicinghour",
                                       "batch.processing",
                                       "batch.sequencing_round",
                                       "total_features_by_counts",
                                       "total_counts"
                                       )
                         ) + ggtitle("scran")

# To check how the explanatory variables change before normalization, run the same using `exprs_values = "counts" + ggtitle("Raw counts")` or `exprs_values = "logcounts_raw" + ggtitle("Logcounts Raw")` and `multiplot(explV1, explV2, explV3, cols=1)`.
```

We can see that the two variables standing out are `batch.processing` and `mouse.id`. This makes perfect sense, as they illustrate how the cells were acquired and processed. Due to the way we isolated cells, however, `mouse.id` is confounded with `cell.type`: each transgenic mouse labeled either VGAT or VGluT2 neurons, which means that if we regress out the variability in `mouse.id` we will also be removing much of the biology linked to the `cell.type` that came from each mouse.

A better option would be to regress out the `batch.processing`, but only half of our samples were assigned to a balanced design. For the other half, all the cells obtained from the same animal were processed together, thereby confounding the `batch.processing` with the `mouse.id` (and by extension with `cell.type`) and leading to a similar scenario as above. 

## Step 5.2 | Batch Correction (dealing with confounders)
Computational correction of these effects is critical for eliminating batch-to-batch variation, allowing data across multiple batches to be combined for valid downstream analysis. One method to achieve this is `removeBatchEffect()` from the __limma__ package (Ritchie et al. 2015). `removeBatchEffect()` performs a linear regression and sets the coefficients corresponding to the blocking factors to zero. This is effective provided that the population composition within each batch is either known (and supplied as `design=`) or identical across batches. In our case we know exactly which cell type ended up in each batch. 

In most scRNA-seq applications, however, the factors of variation are not identical across batches and not known in advance. This motivates the use of more sophisticated batch correction methods such as `mnnCorrect()`, based on the detection of mutual nearest neighbours (MNNs) (Haghverdi et al. 2018). The MNN approach does not rely on pre-defined or equal population compositions across batches, only requiring that a subset of the population be shared between batches. In our case, each animal could be treated as a separate batch in their own right, reflecting (presumably uninteresting) biological differences due to genotype, age, sex or other factors that are inherent to our experimental design. `mnnCorrect()` was developed to merge different datasets obtained from the same biological system. It assumes that each batch shares at least one biological condition with each other batch (in our case it can be the same cell type across animals). Thus it works well for a variety of balanced experimental designs. However, in a confounded/replicate design biological effects will not be fit/preserved and we will only be able to remove batch effects from each individual separately in order to preserve biological (and technical) variance between individuals.

Manual batch correction is necessary for downstream procedures that are not model-based, e.g., clustering and most forms of dimensionality reduction. However, if an analysis method can accept a design matrix (such as differential expression analysis), blocking on nuisance factors in the design matrix is preferable to using `removeBatchEffect()`. This is because the latter does not account for the loss of residual degrees of freedom, nor the uncertainty of estimation of the blocking factor terms.

### 5.2.1 | Applying removeBatchEffect()
`removeBatchEffect()` performs a linear regression and sets the coefficients corresponding to the blocking factors to zero. This is effective provided that the population composition within each batch is known (and supplied as `design=`) or identical across batches. The composition of our cell poplulation is __not__ identical across batches, as each batch comes from one transgenic mouse and thus has only one out of two possible cell types. However, individually aspirating cells based on expression of a transgene has the advantage that it allows us to know exactly the composition of each batch, so we can set the design and batches as follows:
```{r}
library(limma)
start_timec3 <- Sys.time() # Takes around 32min
assay(PAG_sceset_qc_norm, "corrected") <- removeBatchEffect(logcounts(PAG_sceset_qc_norm_filt),
                                                            design = model.matrix(~PAG_sceset_qc_norm_filt$cell.type), 
                                                            batch = PAG_sceset_qc_norm_filt$batch.processing)

assayNames(PAG_sceset_qc_norm_filt)
end_timec3 <- Sys.time()
end_timec3 - start_time3
```

### 5.2.3 | Effectiveness of the correction
We can evaluate the effectiveness of the correction by inspecting the PCA plot where colour corresponds the technical replicates and shape corresponds to the individuals from which biological samples where acquired. Separation of biological samples and interspersed batches indicates that technical variation has been removed.
```{r}
for(n in assayNames(PAG_sceset_qc_norm_filt)) {
    print(
        plotPCA(
            PAG_sceset_qc_norm_filt,
            colour_by = "batch.processing",
            shape_by = "mouse.id",
            size_by = "total_features_by_counts",
            exprs_values = n
        ) +
        ggtitle(n)
    )
}
```

## Step 5.3 | Save the filtered and corrected SCE object
```{r}
# Save the normalized data:
saveRDS(PAG_sceset_qc_norm_filt, file = "PAG_sceset_qc_norm_filt.rds")
print("Parts 4 and 5 - Done!")
```

# STEP 6 | Denoising expression values using PCA
Once the technical noise is modelled and we have corrected our batch effects, we can use principal component analysis (PCA) to remove random technical noise. Consider that each cell represents a point in the high-dimensional expression space, where the spread of points represents the total variance. PCA identifies axes in this space that capture as much of this variance as possible. Each axis is a principal component (PC), where any early PC will explain more of the variance than a later PC.

We assume that biological processes involving co-regulated groups of genes will account for the most variance in the data. If this is the case, this process should be represented by one or more of the earlier PCs. In contrast, random technical noise affects each gene independently and will be represented by later PCs. The `denoisePCA()` function removes later PCs until the total discarded variance is equal to the sum of technical components for all genes used in the PCA.

We use all genes with a positive biological component in `denoisePCA` and perform PCA on the expression profiles, choosing the number of PCs to retain based on the total technical noise in the data set. The idea is to discard later PCs that contain random technical noise, thus enriching for early biological signal (and also reducing work in downstream steps).

* `denoisePCA()` will only use genes that have positive biological components, i.e., variances greater than the fitted trend. This guarantees that the total technical variance to be discarded will not be greater than the total variance in the data.
* For the `technical=` argument, the function will also accept the trend function directly (i.e., `var.fit$trend`) or a vector of technical components per gene. Here, we supply the  DataFrame from decomposeVar() to allow the function to adjust for the loss of residual degrees of freedom after batch correction. Specifically, the variance in the batch-corrected matrix is slightly understated, requiring some rescaling of the technical components to compensate.
* No filtering is performed on abundance here, which ensures that PCs corresponding to rare subpopulations can still be detected. Discreteness is less of an issue as low-abundance genes also have lower variance, thus reducing their contribution to the PCA.
* It is also possible to obtain a low-rank approximation of the original expression matrix, capturing the variance equivalent to the retained PCs. This is useful for denoising prior to downstream procedures that require gene-wise expression values.
```{r}
PAG_sceset_qc_norm <- denoisePCA(PAG_sceset_qc_norm, technical=var_out_no_spikes, assay.type="corrected")
#technical=var_out_spikes or var_fit_spikes$trend
dim(reducedDim(PAG_sceset_qc_norm, "PCA")) # Cells are rows, PCs are columns
```

The function returns a `SingleCellExperiment` object containing the PC scores for each cell in the `reducedDims` slot, where cells are rows and PCs are columns. The aim is to eliminate technical noise and enrich for biological signal in the retained PCs. This improves resolution of the underlying biology during downstream procedures such as clustering.
## Step 4a.3 | Using HVGs for further data exploration
### 4a.3.1 | Heatmaps
We visualize the expression profiles of the correlated HVGs with a heatmap. All expression values are mean-centred for each gene to highlight the relative differences in expression between cells. If any subpopulations were present, they would manifest as rectangular "blocks" in the heatmap, corresponding to sets of genes that are systematically up- or down-regulated in specific groups of cells. 
```{r}
chosen <- unique(c(var.cor$gene1[sig.cor], var.cor$gene2[sig.cor])) 
norm.exprs <- exprs(PAG_sceset_qc_norm_filt)[chosen,,drop=FALSE] 
heat.vals <- norm.exprs - rowMeans(norm.exprs) 
library(gplots) 
heat.out <- heatmap.2(heat.vals, col=bluered, symbreak=TRUE, trace='none', cexRow=0.6)
```

#### 4a.3.2 | PCA
We also apply dimensionality reduction techniques to visualize the relationships between cells. This is done by constructing a PCA plot from the normalized log-expression values of the correlated HVGs. Cells with similar expression profiles should be located close together in the plot, while dissimilar cells should be far apart. We only use the correlated HVGs in plotPCA because any substructure should be most pronounced in the expression profiles of these genes.
```{r}
out_pca <- plotPCA(PAG_sceset_qc_norm_filt,
                   rerun = TRUE,
                   run_args = list(feature_set = chosen),
                   colour_by = "mouse.id",
                   size_by = "total_features_by_counts"
                   )
plot(out_pca)
```

Additional components can be visualized by increasing the ncomponents argument in plotPCA to construct pairwise plots. The percentage of variance explained by each component can also be obtained by running plotPCA with return_SCESet=TRUE, and then calling reducedDimension on the returned object. This information may be useful for selecting high-variance components (possibly corresponding to interesting underlying factors) for further examination.

#### 4a.3.3 | tSNE
Another widely used approach is the t-stochastic neighbour embedding (t-SNE) method (Van der Maaten & Hinton, 2008). t-SNE tends to work better than PCA for separating cells in more diverse populations. This is because the former can directly capture non-linear relationships in high-dimensional space, whereas the latter must represent them (suboptimally) as linear components. However, this improvement comes at the cost of more computational effort and complexity. In particular, t-SNE is a stochastic method, so users should run the algorithm several times to ensure that the results are representative, and then set a seed to ensure that the chosen results are reproducible. It is also advisable to test different settings of the "perplexity" parameter as this will affect the distribution of points in the low-dimensional space.
```{r}
set.seed(1991) 
out5 <- plotTSNE(PAG_sceset_qc_norm_filt,
                 rerun = TRUE, 
                 run_args = list(perplexity=5, feature_set=chosen),
                 colour_by="cell.type"
                 ) + ggtitle("Perplexity = 5")

out10 <- plotTSNE(PAG_sceset_qc_norm_filt,
                  rerun = TRUE, 
                  run_args = list(perplexity=10, feature_set=chosen),
                  colour_by="mouse.id"
                  ) + ggtitle("Perplexity = 10")

out20 <- plotTSNE(PAG_sceset_qc_norm_filt,
                  rerun = TRUE, 
                  run_args = list(perplexity=20, feature_set=chosen),
                  colour_by="cell.type"
                  ) + ggtitle("Perplexity = 20")

out50 <- plotTSNE(PAG_sceset_qc_norm_filt,
                  rerun = TRUE, 
                  run_args = list(perplexity=50, feature_set=chosen),
                  colour_by="mouse.id"
                  ) + ggtitle("Perplexity = 50")

multiplot(out5, out10, out20, out50, cols=2)
```

#### 4a.3.1 | Dimensionality reduction with HVGs
We now can perform dimensionality reduction using our HVGs subset. To do this, we first calculate the PCA representation via the `runPCA()` function from the `scater` package. We will calculate 50 components on our highly variable genes. The results of these calculations will be stored in the `reducedDims` slot. This method saves the percent variance explained per component as an attribute, which can be accessed as follows, and subsequently plot using the “elbow plot”:
```{r}
PAG_sceset_qc_norm_filt <- runPCA(PAG_sceset_qc_norm_filt, 
                                  ncomponents = 50,
                                  ntop = 2743, 
                                  exprs_values = "logcounts",
                                  feature_set = metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt)
pct_var_explained <- attr(reducedDim(PAG_sceset_qc_norm_filt, "PCA"), "percentVar")

plot(pct_var_explained) # elbow plot
```

To calculate a 2-dimensional representation of the data, we will use the top 20 components of our PCA result to compute the UMAP representation.
```{r}
PAG_sceset_qc_norm_filt <- runUMAP(PAG_sceset_qc_norm_filt, use_dimred = 'PCA', n_dimred = 20)

plotUMAP(PAG_sceset_qc_norm_filt, 
         colour_by = "batch.processing")
```

#### 4a.3.1 | Clustering
```{r}
set.seed(1991)
snng <- buildSNNGraph(PAG_sceset_qc_norm_filt, k = 50, d = 20)
snng_clusters <- igraph::cluster_louvain(snng)
table(snng_clusters$membership)
colData(PAG_sceset_qc_norm_filt)$clusters <- as.factor(snng_clusters$membership)
plotUMAP(PAG_sceset_qc_norm_filt, colour_by = "clusters")
```

#### 4a.3.1 | Differential Expression
```{r}
markers <- findMarkers(PAG_sceset_qc_norm_filt, 
                       clusters = colData(PAG_sceset_qc_norm_filt)$clusters,
                       subset.row = metadata(PAG_sceset_qc_norm_filt)$hvg_out_no_spikes_filt[1:250],
                       lfc = 1.5, 
                       direction = 'up', 
                       log.p = TRUE)
markers[[1]][1:5, ]
plotExpression(PAG_sceset_qc_norm_filt, 'Gad1', x = 'clusters')
```


## 5.1.3 | Correlations with Principal Components
`scater` allows us to identify principal components that correlate with experimental and QC variables of interest (it ranks principle components by R^2^ from a linear model regressing PC value against any variable or annotation we have associated with each cell) to see which factor is driving a particular principal component.
```{r}
plotExplanatoryPCs(PAG_sceset_qc_norm_filt,
                   nvars_to_plot = 16,
                   npcs_to_plot = 10,                 
                   variables = c("mouse.id",
                                 "mouse.sex",
                                 "mouse.age",
                                 "mouse.singlehousedays",
                                 "cell.type",
                                 "cell.fluorophor",
                                 "slice.number",
                                 "slice.depth",
                                 "PAG.areacollection",
                                 "PAG.hemisphere",
                                 "PAG.APaxis",
                                 "time.sinceslicinghour",
                                 "batch.processing",
                                 "batch.sequencing_round",
                                 "total_features_by_counts",
                                 "total_counts"
                                 )
                   )
```

## Step 6.1 | Visualizing data in low-dimensional space with PCA
We can visualize the relationships between cells by constructing pairwise PCA plots for the first components. Cells with similar expression profiles should be located close together in the plot, while dissimilar cells should be far apart.
```{r}
plotReducedDim(PAG_sceset_qc_norm, use_dimred="PCA", ncomponents=4, colour_by="cell.type")
```

By comparison, we should not observe a clear separation of cells by batch (mouse.id), which would indicate that our batch correction step using `removeBatchEffect()` was successful.
```{r}
plotReducedDim(PAG_sceset_qc_norm, use_dimred="PCA", ncomponents=4, colour_by="mouse.id")
```
Note that `plotReducedDim()` will use the PCA results that were already stored in sce by `denoisePCA()`. This allows us to rapidly generate new plots with different aesthetics, without repeating the entire PCA computation. Similarly, `plotPCA()` will use existing results if they are available in the `SingleCellExperiment`, and will recalculate them otherwise. Users should set `rerun=TRUE` to forcibly recalculate the PCs in the presence of existing results. Additional cell-specific information can be incorporated into the size or shape of each point (e.g. `size_by=` and `shape_by=` arguments).

We can also look at the proportion of variance explained by each PC:
```{r}
# Is this the same as plotExplanatoryPCs?
principal_components <- reducedDim(PAG_sceset_qc_norm, "PCA")
proportion_var <- attr(principal_components, "percentVar")
plot(proportion_var, xlab="PC", ylab="Proportion of variance explained")
```

## Step 6.2 | Visualizing data in low-dimensional space with t-SNE
Another widely used approach for dimensionality reduction is the t-stochastic neighbour embedding (t-SNE) method (Van der Maaten and Hinton 2008). t-SNE tends to work better than PCA for separating cells in more diverse populations. This is because the former can directly capture non-linear relationships in high-dimensional space, whereas the latter must represent them on linear axes. However, this improvement comes at the cost of more computational effort and requires the user to consider parameters such as the random seed and perplexity.

We can use the `plotTSNE()` function and set `use_dimred="PCA"` to perform the t-SNE on the low-rank approximation of the data, allowing the algorithm to take advantage of the previous denoising step.
```{r}
set.seed(1991)

tsne5 <- plotTSNE(PAG_sceset_qc_norm, run_args=list(use_dimred="PCA", perplexity=5),
    colour_by="mouse.id") + fontsize + ggtitle("Perplexity = 5")

tsne10 <- plotTSNE(PAG_sceset_qc_norm, run_args=list(use_dimred="PCA", perplexity=10),
    colour_by="mouse.id") + fontsize + ggtitle("Perplexity = 10")

tsne20 <- plotTSNE(PAG_sceset_qc_norm, run_args=list(use_dimred="PCA", perplexity=20),
    colour_by="mouse.id") + fontsize + ggtitle("Perplexity = 20")

multiplot(tsne5, tsne10, tsne20, cols=3)
```

t-SNE is a stochastic method, so we should run the algorithm several times to ensure that the results are representative. We should set a seed with the `set.seed()` command to ensure that the chosen results are reproducible, and test different settings of the "perplexity" parameter as this will affect the distribution of points in the low-dimensional space.

We can also use `runTSNE()` to store the t-SNE results inside our `SingleCellExperiment` object. This avoids repeating the calculations whenever we want to create a new plot with `plotTSNE()`, as the stored results will be used instead. Again, users can set `rerun=TRUE` to force recalculation in the presence of stored results.
```{r}
set.seed(1991)
PAG_sceset_qc_norm <- runTSNE(PAG_sceset_qc_norm, use_dimred="PCA", perplexity=20)
reducedDimNames(PAG_sceset_qc_norm)
```