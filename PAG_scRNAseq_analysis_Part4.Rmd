---
title: "Topographic, single-cell gene expression profiling of Periaqueductal Gray neurons"
subtitle: "Part IV: modelling the technical noise and removing batch effects"
author:
  - name: "Oriol Pavon Arocas, Sarah F. Olesen and Tiago Branco"
    affiliation: "Sainsbury Wellcome Centre for Neural Circuits and Behaviour, University College London, UK"
    email: "oriol.pavon.16@ucl.ac.uk"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_notebook:
    highlight: pygments
    number_sections: FALSE
    theme: lumen
    toc: TRUE
    toc_float: TRUE

#output: rmdformats::readthedown:
  #highlight: pygments
---

***
This is a pipeline to analyse single-cell RNA sequencing data from Periaqueductal Gray neurons (1) isolated from acute midbrain slices of transgenic mice using visually guided aspiration via patch pipettes and (2) processed using SMART-seq2 (Picelli et al. Nature Protocols 2014). 

This pipeline has been generated following the online course on [Analysis of single cell RNA-seq data](https://github.com/hemberg-lab/scRNA.seq.course) by the [Hemberg Lab](http://hemberg-lab.github.io/scRNA.seq.course), the [Orchestrating Single-Cell Analysis with Bioconductor book](https://osca.bioconductor.org/) by Robert Amezquita and Stephanie Hicks, the [simpleSingleCell workflow in Bioconductor](https://bioconductor.org/packages/3.8/workflows/html/simpleSingleCell.html) maintained by Aaron Lun, and by attending the [EMBL-EBI RNA-Sequence Analysis Course](https://www.ebi.ac.uk/training/events/2019/rna-sequence-analysis).

Other key resources are Bioconductor (Huber et al. Nature Methods 2015), scater (McCarty et al. Bioinformatics 2017), scran (Lun et al. F1000Res 2016), and SC3 (Kiselev et al. Nature Methods 2017).

***

# STEP 4 | Modelling technical and biological variability in gene expression
_Based on the [simpleSingleCell workflow in Bioconductor](https://bioconductor.org/packages/3.8/workflows/html/simpleSingleCell.html) maintained by Aaron Lun_.

We continue using the `PAG_sceset_qc` after normalization. We should thus have a `logcounts` slot in`assays`:
```{r}
set.seed(1991)
options(stringsAsFactors = FALSE)
library(SingleCellExperiment)
library(scater)
library(scran)

assayNames(PAG_sceset_qc)
PAG_sceset_qc
```

## Step 4.1 | Modelling the technical noise
Variability in the observed expression values across genes can be driven by genuine biological heterogeneity or uninteresting technical noise. To distinguish between these two possibilties, we need to model the technical component of the variance of the expression values for each gene. 

We will use the `trendVar()` function to fit a mean-variance trend to the read counts.

* We can set `block=` to block on the plate/animal of origin for each cell, to ensure that technical differences between plates do not inflate the variances. This involves estimating the mean and variance of the log-expression separately in each plate, followed by fitting a single trend to the plate-specific means and variances of all spike-in transcripts. In doing so, we implicitly assume that the trend is the same between plates. The use of `block=` also assumes that the average size factor within each plate is close to unity for both endogenous genes and spike-in transcripts.
* Some tuning of trend parameters such as `span` may be required to achieve a suitable fit (default is 0.75).
* Setting `parametric=TRUE` is especially useful for modelling the expected wave-like shape of the mean-variance relationship. (This is not the default setting as it is not robust for arbitrary trend shapes.)

In addition, the `trendVar` function automatically filters out low-abundance genes prior to trend fitting. This ensures that low-abundance genes do not interfere with the fit due to discreteness, which biases the estimate of variability of the variances around the trend; or due to the frequency of low-abundance genes, which reduces the sensitivity of span-based smoothing algorithms at higher abundances. The internal choice of filtering strategy involves a number of considerations:

* Filtering uses the average of log-expression values rather than the (library size-adjusted) average count. The mean log-expression is independent of the variance estimate in a linear modelling framework (Bourgon, Gentleman, and Huber 2010), which ensures that the filter does not introduce spurious trends in the variances at the filter boundary.
* The filter threshold is specified with the `min.mean` argument in `trendVar`. We use the default threshold of 0.1 (`min.mean`) based on the appearance of discrete patterns in the variance estimates for simulated Poisson-distributed counts. Lower thresholds of 0.001-0.01 may be more suitable for very sparse data, e.g., from droplet-based protocols.
* The filter used in `trendVar` is not applied in `decomposeVar` by default. Retention of all genes ensures that weak biological signal from rare subpopulations is not discarded. To apply the filter in `decomposeVar`, users should set `subset.row=rowMeans(logcounts(sce)) > 0.1` in the function call.

__One last comment from Aaron Lun__: On occasion, users may observe a warning from `trendVar()` about the lack of centering in the size factors. Recall that the trend is fitted to the mean and variances of the spike-in transcripts, and the technical component for each endogenous gene is estimated by interpolation. This assumes that an endogenous gene is comparable to a spike-in transcript of the same abundance. In particular, we assume that variation is primarily driven by the magnitude of the counts, based on the well-known mean-variance relationships in count models. Thus, we need to ensure that similarities in the average counts are preserved in the normalized expression values. This is achieved by centering the gene- and spike-in-based size factors in `normalize()`, such that features with similar average counts will also have similar normalized abundances. However, if the `SingleCellExperiment` object was manipulated (e.g., subsetted) after `normalize()` and before `trendVar()`, centering may not be preserved - hence the warning.

### 4.1.1 | Using spike-ins
One way to do so is using the set of spike-in transcripts. These were in theory added in the same quantity to each cell. Thus, the spike-in transcripts should exhibit non-biological variability, i.e., any variance in their counts should be technical in origin.

We use the `trendVar()` function to fit a mean-dependent trend to the variances of the log-expression values for the spike-in transcripts. Given the mean abundance of a gene, the fitted value of the trend is then used as an estimate of the technical component for that gene. The biological component of the variance is finally calculated by subtracting the technical component from the total variance of each gene with the `decomposeVar` function.
```{r}
var_fit_spikes <- trendVar(PAG_sceset_qc,
                           #method="loess",
                           use.spikes=TRUE, 
                           parametric=TRUE, 
                           block=PAG_sceset_qc$mouse.id,
                           min.mean=0.1,
                           loess.args=list(span=0.3)
                           ) 

var_out_spikes <- decomposeVar(PAG_sceset_qc, var_fit_spikes)
head(var_out_spikes)
```

We visually inspect the trend to confirm that it corresponds to the spike-in variances. The wave-like shape is typical of the mean-variance trend for log-expression values. A linear increase in the variance is observed as the mean increases from zero, as larger variances are possible when the counts increase. At very high abundances, the effect of sampling noise decreases due to the law of large numbers, resulting in a decrease in the variance.
```{r}
plot(var_out_spikes$mean, 
     var_out_spikes$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression",
     ylab="Variance of log-expression")

curve(var_fit_spikes$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc)
points(var_out_spikes$mean[current_spikes], var_out_spikes$total[current_spikes], col="red", pch=16)
```

***
Notes from Aaron Lun's `simpleSingleCell` workflow:

* In practice, trend fitting is complicated by the small number of spike-in transcripts and the uneven distribution of their abundances. In the absence of spike-ins or as an alternative approach, we can set `use.spikes=FALSE` to fit a trend to the variances of the endogenous genes (see 4.1.2). Another alternative would be to create a trend based on the assumption of Poisson technical noise.
* Negative biological components are often obtained from `decomposeVar`. These are intuitively meaningless as it is impossible for a gene to have total variance below technical noise. Nonetheless, such values occur due to imprecise estimation of the total variance, especially for low numbers of cells.
* `decomposeVar` also yields p-values that can be used to define highly variable genes (HVGs) at a specific threshold for the false discovery rate (FDR). We will discuss this in more detail later, as formal detection of HVGs is not necessary for feature selection during data exploration.

### 4.1.2 | Without spike-ins
Ideally, the technical component would be estimated by fitting a mean-variance trend to the spike-in transcripts using the `trendVar` function as we have seen before. In practice, this strategy is compromised by the small number of spike-in transcripts, the uneven distribution of their abundances and (for low numbers of cells) the imprecision of their variance estimates. This makes it difficult to accurately fit a complex mean-dependent trend to the spike-in variances. In some datasets, spike-in RNA may not have been added in appropriate quantities (or indeed at all). It may also be inappropriate to assume Poisson technical noise with `makeTechTrend()`, especially for read count data where amplification noise is non-negligible. In such cases, an alternative approach is to fit the trend to the variance estimates of the endogenous genes, using the `use.spikes=FALSE` setting. This assumes that the majority of genes are not variably expressed, such that the technical component dominates the total variance for those genes. The fitted value of the trend is then used as an estimate of the technical component. 

__NB__: fitting the trend to the variances of the genes with `use.spikes=FALSE` probably overestimates the technical component.
```{r}
var_fit_no_spikes <- trendVar(PAG_sceset_qc, 
                              method="loess", 
                              use.spikes=FALSE,
                              parametric=TRUE,
                              block=PAG_sceset_qc$mouse.id, 
                              min.mean=0.1,
                              loess.args=list(span=0.3)
                              ) 

var_out_no_spikes <- decomposeVar(PAG_sceset_qc, var_fit_no_spikes)
head(var_out_no_spikes)
```

We assess the suitability of the trend fitted to the endogenous variances by examining whether it is consistent with the spike-in variances. If the trend passes through or close to most of the spike-in variances, this indicates that our assumption (that most genes have low levels of biological variability) is valid. This strategy exploits the large number of endogenous genes to obtain a stable trend, with the spike-in transcripts used as diagnostic features rather than in the trend fitting itself. However, if our assumption does not hold, we would instead fit the trend directly to the spike-in variances with the default use.spikes=TRUE. This sacrifices stability to reduce systematic errors in the estimate of the biological component for each gene.
```{r}
plot(var_out_no_spikes$mean, 
     var_out_no_spikes$total, 
     pch=16, cex=0.6, 
     xlab="Mean log-expression", 
     ylab="Variance of log-expression")

curve(var_fit_no_spikes$trend(x), add=TRUE, col="dodgerblue", lwd=2)
current_spikes <- isSpike(PAG_sceset_qc) 
points(var_out_no_spikes$mean[current_spikes], var_out_no_spikes$total[current_spikes], col="red", pch=16)
```

## Step 4.2 | Identifying highly variable genes
HVGs are defined as genes with biological components that are significantly greater than zero. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. Formal detection of HVGs allows us to avoid genes that are highly variable due to technical factors such as sampling noise during RNA capture and library preparation. This adds another level of statistical rigour to our previous analyses, in which we only modelled the technical component.

Identifying HVGs requires estimation of the variance in expression for each gene, followed by decomposition of the variance into biological and technical components (done in the previous step). HVGs are then identified as those genes with the largest biological components. 

We define HVGs as genes with biological components that are significantly greater than zero at a false discovery rate (FDR) of 5%. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. In addition, we only consider a gene to be a HVG if it has a biological component greater than or equal to 0.5. For transformed expression values on the log2 scale, this means that the average difference in true expression between any two cells will be at least 2-fold. (This reasoning assumes that the true log-expression values are Normally distributed with variance of 0.5. The root-mean-square of the difference between two values is treated as the average log2-fold change between cells and is equal to unity.) We rank the results by the biological component to focus on genes with larger biological variability.
```{r}
#hvg_out_spikes <- var_out_spikes[which(var_out_spikes$FDR <= 0.05 & var_out_spikes$bio >= 0.5),]
hvg_out_no_spikes <- var_out_no_spikes[which(var_out_no_spikes$FDR <= 0.05 & var_out_no_spikes$bio >= 0.5),]
# Consider removing the $bio >= 0.5 filter
#nrow(hvg_out_spikes)
nrow(hvg_out_no_spikes)
```

We rank the results to focus on genes with larger biological components. This highlights an interesting aspect of the underlying hypothesis test, which is based on the ratio of the total variance to the expected technical variance. Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be interesting. This is because the ratio can be very large for HVGs that have very low total variance and do not contribute much to the cell-cell heterogeneity.
```{r}
#hvg_out_spikes <- hvg_out_spikes[order(hvg_out_spikes$bio, decreasing=TRUE),] 
hvg_out_no_spikes <- hvg_out_no_spikes[order(hvg_out_no_spikes$bio, decreasing=TRUE),] 
#nrow(hvg_out_spikes)
nrow(hvg_out_no_spikes)

# Export the results
#write.table(file="PAG_hvg_spikes.tsv", hvg_out_spikes, sep="\t", quote=FALSE, col.names=NA)
write.table(file="PAG_hvg_no_spikes.tsv", hvg_out_no_spikes, sep="\t", quote=FALSE, col.names=NA) 
head(hvg_out_no_spikes)
```

We finally check the distribution of expression values for the genes with the largest biological components to ensure that the variance estimate is not being dominated by one or two outlier cells.
```{r}
fontsize <- theme(axis.text=element_text(size=12), axis.title=element_text(size=16))
plotExpression(PAG_sceset_qc, features=rownames(hvg_out_no_spikes)[1:20]) + fontsize
```

***
Notes from Aaron Lun's `simpleSingleCell` workflow:

One reason to use the variance of the log-expression values is the fact that the log-transformation protects against genes with strong expression in only one or two cells. This reduces the risk that the set of top HVGs is not dominated by genes with (mostly uninteresting) outlier expression patterns. There are, however, many other strategies for defining HVGs, based on a variety of metrics:

* the coefficient of variation, using the `technicalCV2()` function (Brennecke et al. 2013) or the `DM()` function (Kim et al. 2015) in _scran_.
* the dispersion parameter in the negative binomial distribution, using the `estimateDisp()` function in _edgeR_ (McCarthy, Chen, and Smyth 2012).
* a proportion of total variability, using methods in the _BASiCS_ package (Vallejos, Marioni, and Richardson 2015).

## Step 4.3 | Identifying correlated gene pairs with Spearman's rho
Another useful procedure is to identify the HVGs that are highly correlated with one another. This distinguishes between HVGs caused by random noise and those involved in driving systematic differences between subpopulations. Correlations between genes are quantified by computing Spearman's rho, which accommodates non-linear relationships in the expression values. Gene pairs with significantly large positive or negative values of rho are identified using the correlatePairs function. We only apply this function to the set of HVGs, because these genes have large biological components and are more likely to exhibit strong correlations driven by biology. In contrast, calculating correlations for all possible gene pairs would require too much computational time and increase the severity of the multiple testing correction. It may also prioritize uninteresting genes that have strong correlations but low variance, e.g., tightly co-regulated house-keeping genes.
```{r}
set.seed(1991)
var.cor <- correlatePairs(PAG_sceset_qc, subset.row=rownames(hvg.out)) 
write.table(file="hsc_cor.tsv", var.cor, sep="\t", quote=FALSE, row.names=FALSE) 
head(var.cor)
```

The significance of each correlation is determined using a permutation test. For each pair of genes, the null hypothesis is that the expression profiles of two genes are independent. Shuffling the profiles and recalculating the correlation yields a null distribution that is used to obtain a p-value for each observed correlation value (Phipson & Smyth, 2010). Correction for multiple testing across many gene pairs is performed by controlling the FDR at 5%. Correlated gene pairs can be directly used for experimental validation with orthogonal techniques (e.g., fluorescence-activated cell sorting, immunohistochemistry or RNA fluorescence in situ hybridization) to verify that these expression patterns are genuinely present across the cell population.
```{r}
sig.cor <- var.cor$FDR <= 0.05 
summary(sig.cor)
```

Larger sets of correlated genes are assembled by treating genes as nodes in a graph and each pair of genes with significantly large correlations as an edge. In particular, an undirected graph is constructed using methods in the RBGL package. Highly connected subgraphs are then identified and defined as gene sets. This provides a convenient summary of the pairwise correlations between genes.
```{r}
library(RBGL) 
g <- ftM2graphNEL(cbind(var.cor$gene1, var.cor$gene2)[sig.cor,], W=NULL, V=NULL, edgemode="undirected")
cl <- highlyConnSG(g)$clusters 
cl <- cl[order(lengths(cl), decreasing=TRUE)] 
head(cl)
```

Significant correlations provide evidence for substructure in the dataset, i.e., subpopulations of cells with systematic differences in their expression profiles. The number of significantly correlated HVG pairs represents the strength of the substructure. If many pairs were significant, this would indicate that the subpopulations were clearly defined and distinct from one another. 

## Step 4.4 | Using correlated HVGs for further data exploration
#### Heatmaps
We visualize the expression profiles of the correlated HVGs with a heatmap. All expression values are mean-centred for each gene to highlight the relative differences in expression between cells. If any subpopulations were present, they would manifest as rectangular "blocks" in the heatmap, corresponding to sets of genes that are systematically up- or down-regulated in specific groups of cells. 
```{r}
chosen <- unique(c(var.cor$gene1[sig.cor], var.cor$gene2[sig.cor])) 
norm.exprs <- exprs(PAG_sceset_qc)[chosen,,drop=FALSE] 
heat.vals <- norm.exprs - rowMeans(norm.exprs) 
library(gplots) 
heat.out <- heatmap.2(heat.vals, col=bluered, symbreak=TRUE, trace='none', cexRow=0.6)
```

#### PCA
We also apply dimensionality reduction techniques to visualize the relationships between cells. This is done by constructing a PCA plot from the normalized log-expression values of the correlated HVGs. Cells with similar expression profiles should be located close together in the plot, while dissimilar cells should be far apart. We only use the correlated HVGs in plotPCA because any substructure should be most pronounced in the expression profiles of these genes.
```{r}
plotPCA(
    PAG_sceset_qc[endogenous_genes, ],
    rerun = TRUE,
    run_args = list(feature_set = chosen),
    colour_by = "PAG.areacollection",
    size_by = "total_features_by_counts"
)
```

Additional components can be visualized by increasing the ncomponents argument in plotPCA to construct pairwise plots. The percentage of variance explained by each component can also be obtained by running plotPCA with return_SCESet=TRUE, and then calling reducedDimension on the returned object. This information may be useful for selecting high-variance components (possibly corresponding to interesting underlying factors) for further examination.

#### tSNE
Another widely used approach is the t-stochastic neighbour embedding (t-SNE) method (Van der Maaten & Hinton, 2008). t-SNE tends to work better than PCA for separating cells in more diverse populations. This is because the former can directly capture non-linear relationships in high-dimensional space, whereas the latter must represent them (suboptimally) as linear components. However, this improvement comes at the cost of more computational effort and complexity. In particular, t-SNE is a stochastic method, so users should run the algorithm several times to ensure that the results are representative, and then set a seed to ensure that the chosen results are reproducible. It is also advisable to test different settings of the "perplexity" parameter as this will affect the distribution of points in the low-dimensional space.
```{r}
set.seed(1991) 
out5 <- plotTSNE(
  PAG_sceset_qc[endogenous_genes, ],
  rerun = TRUE, 
  run_args = list(perplexity=5, feature_set=chosen),
  colour_by="Slc17a6"
) + ggtitle("Perplexity = 5")

out10 <- plotTSNE(
  PAG_sceset_qc[endogenous_genes, ],
  rerun = TRUE, 
  run_args = list(perplexity=10, feature_set=chosen),
  colour_by="Slc17a6"
) + ggtitle("Perplexity = 10")

out20 <- plotTSNE(
  PAG_sceset_qc[endogenous_genes, ],
  rerun = TRUE, 
  run_args = list(perplexity=20, feature_set=chosen),
  colour_by="Slc17a6"
) + ggtitle("Perplexity = 20")

out50 <- plotTSNE(
  PAG_sceset_qc[endogenous_genes, ],
  rerun = TRUE, 
  run_args = list(perplexity=50, feature_set=chosen),
  colour_by="Drd1",
  shape_by="PAG.areacollection"
) + ggtitle("Perplexity = 50")

multiplot(out5, out10, out20, out50, cols=2)
```

# STEP 5 | Batch correction
Our data have been collected from samples originated from several animals and processed on different days. Small uncontrollable differences in processing between batches (changes in operator, differences in reagent quality) can result in systematic differences in the observed expression in cells from different batches, which are refered to as "batch effects". Such differences are not interesting and can be problematic as they can be major drivers of heterogeneity in the data, masking the relevant biological differences and complicating interpretation of the results.

Computational correction of these effects is critical for eliminating batch-to-batch variation, allowing data across multiple batches to be combined for valid downstream analysis. However, existing methods such as `removeBatchEffect()` from the __limma__ package (Ritchie et al. 2015) assume that the composition of cell populations are either known or the same across batches. An alternative strategy for batch correction is based on the detection of mutual nearest neighbours (MNNs) (Haghverdi et al. 2018). The MNN approach does not rely on pre-defined or equal population compositions across batches, only requiring that a subset of the population be shared between batches.

We perform batch correction after modelling the technical variance and performing feature selection.

## Step 5.1 | Identifying confounding and technical factors
As we have mentioned, a big source of variability in scRNA-seq data, besides bad cells, are batch effects: the person processing the samples, date of processing, reagent kit, etc. We want to try to identify such effects and remove them, so our dataset picks up mainly biological effects.

### 5.1.1 | Checking for important technical factors or explanatory variables
We check whether there are technical factors that contribute substantially to the heterogeneity of gene expression. If so, the factor may need to be regressed out to ensure that it does not inflate the variances or introduce spurious correlations. scater can compute the marginal R^2^ for each variable when fitting a linear model regressing expression values for each gene against just that variable, and display a density plot of the gene-wise marginal R^2^ values for the variables.

For each gene, it calculates the percentage of the variance of the expression values that is explained by the variable. Small percentages (1-3%) indicate that the expression profiles of most genes are not strongly associated with this factor. If, on the other hand, we get density curves that are shifted to the right (i.e. with a peak towards the 100% end of the x-axis), this tells us that for a large proportion of the genes in our dataset this particular variable explains a large proportion of the variation.
```{r}
# How much of the variance does each variable explain? Some of these variables are calculated when we run the CalculateQC function from scater, and others are added by us (such as batch or individual).

# Without log-transformation
explV1 <- plotExplanatoryVariables(
    PAG_sceset_qc,
    nvars_to_plot = 10,
    exprs_values = "counts",
    variables = c(
        "total_features_by_counts",
        "total_counts",
        #"pct_counts_ERCC",
        "pct_counts_Mitochondrial",
        "pct_counts_Ribosomal",
        "mouse.id",
        "mouse.sex",
        "mouse.age",
        "cell.type",
        "PAG.hemisphere",
        "PAG.areacollection"
    )
) + ggtitle("Raw counts")

# With log-transformation
explV2 <- plotExplanatoryVariables(
    PAG_sceset_qc,
    nvars_to_plot = 10,
    exprs_values = "logcounts_raw",
    variables = c(
        "total_features_by_counts",
        "total_counts",
        #"pct_counts_ERCC",
        "pct_counts_Mitochondrial",
        "pct_counts_Ribosomal",
        "mouse.id",
        "mouse.sex",
        "mouse.age",
        "cell.type",
        "PAG.hemisphere",
        "PAG.areacollection"
    )
) + ggtitle("Logcounts Raw")

# After scran normalization
explV3 <- plotExplanatoryVariables(
    PAG_sceset_qc,
    nvars_to_plot = 10,
    exprs_values = "logcounts",
    variables = c(
        "total_features_by_counts",
        "total_counts",
        #"pct_counts_ERCC",
        "pct_counts_Mitochondrial",
        "pct_counts_Ribosomal",
        "mouse.id",
        "mouse.sex",
        "mouse.age",
        "cell.type",
        "PAG.hemisphere",
        "PAG.areacollection"
    )
) + ggtitle("scran")

multiplot(explV1, explV2, explV3, cols=2)
```

### 5.1.2 | Correlations with Principal Components
`scater` allows us to identify principal components that correlate with experimental and QC variables of interest (it ranks principle components by R^2^ from a linear model regressing PC value against any variable or annotation we have associated with each cell) to see which factor is driving a particular principal component.
```{r}
plotExplanatoryPCs(PAG_sceset_qc,
                   nvars_to_plot = 10,
                   npcs_to_plot = 10,
                   exprs_values = "logcounts", 
                   variables = c(
                     "total_features_by_counts",
                     "total_counts",
                     #"pct_counts_ERCC",
                     "pct_counts_Mitochondrial",
                     "pct_counts_Ribosomal",
                     "mouse.id",
                     "mouse.sex",
                     "mouse.age",
                     "cell.type",
                     "PAG.hemisphere",
                     "PAG.areacollection"
                   )
)
```

## Step 5.2 | Dealing with confounders (batch correction) 
To account for technical confounders we need to identify and remove sources of variation in the expression data that are not related to the biological signal of interest. We could use spike-ins for this (in theory you add the same amount of ERCC in each cell lysate, so any variability should be technical noise), but these can turn out to be extremely variable across cells. Instead, we could use endogenous or housekeeping genes that do not vary systematically between cells. Where we have a large number of endogenous genes that, on average, do not vary systematically between cells and where we expect technical effects to affect a large number of genes (a very common and reasonable assumption), then such methods (for example, the RUVs method) can perform well.

NOTE: you can't really use any of the methods, as you don't have a balanced design (for the first samples each animal was processed in a different batch for library preparation). You could try it with the last samples, as we managed to get a balanced desing (samples from each animal were processed together).

You can't really use RUVseq (remove unwanted variation) as it depends on ERCCs having a constant expression across samples (which we know is not the case in our data, as they are pretty variable). You could try GLM (generalised linear model) or RUVs.

Try different methods and evaluate their effectiveness.

See: http://hemberg-lab.github.io/scRNA.seq.course/cleaning-the-expression-matrix.html#dealing-with-confounders

### Exploring obvious biases with the table function
The table function allows us to quickly inspect if we have any obvious biases or confounds (i.e. all the cells come from male or female mice, or from a particular location or sequencing batch).
```{r}
# Check if any technical factors are confounded
table(PAG_sceset$cell.type, PAG_sceset$PAG.hemisphere)
table(PAG_sceset$cell.type, PAG_sceset$mouse.sex)
#table(PAG_sceset$cell.type, PAG_sceset$reads.mitochondrial)
```

```{r}
# You can also check the distribution of each of these metadata classifications:
summary(factor(PAG_sceset$cell.type))
summary(factor(PAG_sceset$PAG.hemisphere))
summary(factor(PAG_sceset$PAG.areacollection))
summary(factor(PAG_sceset$mouse.sex))
```

### 5.2.1 | removeBatchEffect()
`removeBatchEffect()` performs a linear regression and sets the coefficients corresponding to the blocking factors to zero. This is effective provided that the population composition within each batch is known (and supplied as `design=`) or identical across batches. The composition of our cell poplulation is __not__ identical across batches, as each batch comes from one transgenic mouse and thus has only one out of two possible cell types. However, individually aspirating cells based on expression of a transgene has the advantage that it allows us to know exactly the composition of each batch.
```{r}
library(limma)
assay(PAG_sceset_qc, "corrected") <- removeBatchEffect(logcounts(PAG_sceset_qc),
                                                       design=model.matrix(~PAG_sceset_qc$cell.type), 
                                                       batch=PAG_sceset_qc$mouse.id,
                                                       batch2=PAG_sceset_qc$batch.processing)
assayNames(PAG_sceset_qc)
```

Manual batch correction is necessary for downstream procedures that are not model-based, e.g., clustering and most forms of dimensionality reduction. However, if an analysis method can accept a design matrix, blocking on nuisance factors in the design matrix is preferable to using `removeBatchEffect()`. This is because the latter does not account for the loss of residual degrees of freedom, nor the uncertainty of estimation of the blocking factor terms.

### 5.2.2 | Mutual Nearest Neighbours (mnnCorrect)
In our case, each animal could be treated as a separate batch in their own right, reflecting (presumably uninteresting) biological differences due to genotype, age, sex or other factors that are inherent to our experimental design. We will apply the detection of mutual nearest neighbours (MNNs) (Haghverdi et al. 2018) to correct for batch effects.

`mnnCorrect()` was developed to merge different datasets obtained from the same biological system. It assumes that each batch shares at least one biological condition with each other batch (in our case it can be the same cell type across animals). Thus it works well for a variety of balanced experimental designs. However, in a confounded/replicate design biological effects will not be fit/preserved and we will only be able to remove batch effects from each individual separately in order to preserve biological (and technical) variance between individuals.

### 5.2.3 | Effectiveness of the methods
We can evaluate the effectiveness of the normalization by inspecting the PCA plot where colour corresponds the technical replicates and shape corresponds to different biological samples (individuals). Separation of biological samples and interspersed batches indicates that technical variation has been removed. We always use log2-cpm normalized data to match the assumptions of PCA.
```{r}
for(n in assayNames(PAG_sceset_qc)) {
    print(
        plotPCA(
            PAG_sceset_qc[endogenous_genes, ],
            colour_by = "batch.processing",
            shape_by = "mouse.id",
            size_by = "total_features_by_counts",
            exprs_values = n
        ) +
        ggtitle(n)
    )
}
```

We can also examine the effectiveness of correction using the relative log expression (RLE) across cells to confirm technical noise has been removed from the dataset. Note RLE only evaluates whether the number of genes higher and lower than average are equal for each cell - i.e. systemic technical effects. Random technical noise between batches may not be detected by RLE.
```{r}
res <- list()
for(n in assayNames(PAG_sceset_qc)) {
    res[[n]] <- suppressWarnings(calc_cell_RLE(assay(PAG_sceset_qc, n), erccs))
}
par(mar=c(6,4,1,1))
boxplot(res, las=2)
```

```{r}
for(n in assayNames(PAG_sceset_qc)) {
    print(
        plotQC(
            PAG_sceset_qc[endogenous_genes, ],
            type = "expl",
            exprs_values = n,
            variables = c(
              "total_features_by_counts",
              "total_counts",
              "pct_counts_ERCC",
              "pct_counts_MT",
              "mouse.id",
              "mouse.sex",
              "mouse.age",
              "cell.id",
              "cell.type",
              "cell.fluorophor",
              "cell.number",
              "PAG.hemisphere",
              "PAG.areacollection",
              "slice.depth", 
              "slice.number",
              "time.aspiration",
              "time.sinceslicing",
              "sequencing.round"
            )
        ) +
        ggtitle(n)
    )
}
```

# STEP 6 | Denoising expression values using PCA
Once the technical noise is modelled and we have corrected our batch effects, we can use principal component analysis (PCA) to remove random technical noise. Consider that each cell represents a point in the high-dimensional expression space, where the spread of points represents the total variance. PCA identifies axes in this space that capture as much of this variance as possible. Each axis is a principal component (PC), where any early PC will explain more of the variance than a later PC.

We assume that biological processes involving co-regulated groups of genes will account for the most variance in the data. If this is the case, this process should be represented by one or more of the earlier PCs. In contrast, random technical noise affects each gene independently and will be represented by later PCs. The `denoisePCA()` function removes later PCs until the total discarded variance is equal to the sum of technical components for all genes used in the PCA.

We use all genes with a positive biological component in `denoisePCA` and perform PCA on the expression profiles, choosing the number of PCs to retain based on the total technical noise in the data set. The idea is to discard later PCs that contain random technical noise, thus enriching for early biological signal (and also reducing work in downstream steps).

* `denoisePCA()` will only use genes that have positive biological components, i.e., variances greater than the fitted trend. This guarantees that the total technical variance to be discarded will not be greater than the total variance in the data.
* For the `technical=` argument, the function will also accept the trend function directly (i.e., `var.fit$trend`) or a vector of technical components per gene. Here, we supply the  DataFrame from decomposeVar() to allow the function to adjust for the loss of residual degrees of freedom after batch correction. Specifically, the variance in the batch-corrected matrix is slightly understated, requiring some rescaling of the technical components to compensate.
* No filtering is performed on abundance here, which ensures that PCs corresponding to rare subpopulations can still be detected. Discreteness is less of an issue as low-abundance genes also have lower variance, thus reducing their contribution to the PCA.
* It is also possible to obtain a low-rank approximation of the original expression matrix, capturing the variance equivalent to the retained PCs. This is useful for denoising prior to downstream procedures that require gene-wise expression values.
```{r}
PAG_sceset_qc <- denoisePCA(PAG_sceset_qc, technical=var_out_no_spikes, assay.type="corrected")
#technical=var_out_spikes or var_fit_spikes$trend
dim(reducedDim(PAG_sceset_qc, "PCA")) # Cells are rows, PCs are columns
```

The function returns a `SingleCellExperiment` object containing the PC scores for each cell in the `reducedDims` slot, where cells are rows and PCs are columns. The aim is to eliminate technical noise and enrich for biological signal in the retained PCs. This improves resolution of the underlying biology during downstream procedures such as clustering.

## Step 6.1 | Visualizing data in low-dimensional space with PCA
We can visualize the relationships between cells by constructing pairwise PCA plots for the first components. Cells with similar expression profiles should be located close together in the plot, while dissimilar cells should be far apart.
```{r}
plotReducedDim(PAG_sceset_qc, use_dimred="PCA", ncomponents=4, colour_by="cell.type")
```

By comparison, we should not observe a clear separation of cells by batch (mouse.id), which would indicate that our batch correction step using `removeBatchEffect()` was successful.
```{r}
plotReducedDim(PAG_sceset_qc, use_dimred="PCA", ncomponents=4, colour_by="mouse.id")
```
Note that `plotReducedDim()` will use the PCA results that were already stored in sce by `denoisePCA()`. This allows us to rapidly generate new plots with different aesthetics, without repeating the entire PCA computation. Similarly, `plotPCA()` will use existing results if they are available in the `SingleCellExperiment`, and will recalculate them otherwise. Users should set `rerun=TRUE` to forcibly recalculate the PCs in the presence of existing results. Additional cell-specific information can be incorporated into the size or shape of each point (e.g. `size_by=` and `shape_by=` arguments).

We can also look at the proportion of variance explained by each PC:
```{r}
# Is this the same as plotExplanatoryPCs?
principal_components <- reducedDim(PAG_sceset_qc, "PCA")
proportion_var <- attr(principal_components, "percentVar")
plot(proportion_var, xlab="PC", ylab="Proportion of variance explained")
```

## Step 6.2 | Visualizing data in low-dimensional space with t-SNE
Another widely used approach for dimensionality reduction is the t-stochastic neighbour embedding (t-SNE) method (Van der Maaten and Hinton 2008). t-SNE tends to work better than PCA for separating cells in more diverse populations. This is because the former can directly capture non-linear relationships in high-dimensional space, whereas the latter must represent them on linear axes. However, this improvement comes at the cost of more computational effort and requires the user to consider parameters such as the random seed and perplexity.

We can use the `plotTSNE()` function and set `use_dimred="PCA"` to perform the t-SNE on the low-rank approximation of the data, allowing the algorithm to take advantage of the previous denoising step.
```{r}
set.seed(1991)

tsne5 <- plotTSNE(PAG_sceset_qc, run_args=list(use_dimred="PCA", perplexity=5),
    colour_by="mouse.id") + fontsize + ggtitle("Perplexity = 5")

tsne10 <- plotTSNE(PAG_sceset_qc, run_args=list(use_dimred="PCA", perplexity=10),
    colour_by="mouse.id") + fontsize + ggtitle("Perplexity = 10")

tsne20 <- plotTSNE(PAG_sceset_qc, run_args=list(use_dimred="PCA", perplexity=20),
    colour_by="mouse.id") + fontsize + ggtitle("Perplexity = 20")

multiplot(tsne5, tsne10, tsne20, cols=3)
```

t-SNE is a stochastic method, so we should run the algorithm several times to ensure that the results are representative. We should set a seed with the `set.seed()` command to ensure that the chosen results are reproducible, and test different settings of the "perplexity" parameter as this will affect the distribution of points in the low-dimensional space.

We can also use `runTSNE()` to store the t-SNE results inside our `SingleCellExperiment` object. This avoids repeating the calculations whenever we want to create a new plot with `plotTSNE()`, as the stored results will be used instead. Again, users can set `rerun=TRUE` to force recalculation in the presence of stored results.
```{r}
set.seed(1991)
PAG_sceset_qc <- runTSNE(PAG_sceset_qc, use_dimred="PCA", perplexity=20)
reducedDimNames(PAG_sceset_qc)
```

# STEP 7 | Clustering
## Step 7.1 | Hierarchical clustering on Euclidean distances
A quick and dirty approach with hierarchical clustering on Euclidean distances:
```{r}
PAG_dist <- dist(principal_components) # Making a distance matrix
PAG_tree <- hclust(PAG_dist, method="ward.D2") # Building a tree
plot(PAG_tree)
```

We can next cut the tree into different clusters and see how these correspond to the known sample conditions:
```{r}
PAG_clusters <- cutree(PAG_tree, k=3) 
table(PAG_clusters, PAG_sceset_qc$PAG.area)
```

Color the t-SNE according to the cluster identity:
```{r}
PAG_sceset_qc$Cluster <- factor(PAG_clusters)
plotTSNE(PAG_sceset_qc, colour_by='Cluster')
```

We next use a heatmap to visualize the expression profiles of the top 100 genes with the largest biological components. If there is structure, we should see "blocks" in expression that correspond nicely to known sample features. We possibly could have subclustered further, in which case we would subset and repeat the above process.
```{r}
library(pheatmap)
plotHeatmap(PAG_sceset_qc, 
            features=rownames(var_out_no_spikes)[order(var_out_no_spikes$bio, decreasing=TRUE)[1:100]],
            cluster_cols=PAG_tree, colour_columns_by="PAG.area",
            center=TRUE, symmetric=TRUE)
```

When clustering, it is often useful to look at silhouette plots to assess cluster separatedness. Each bar corresponds to a cell, and is proportional to the difference in the average distances to all other cells in the same cluster versus cells in the nearest neighbouring cluster. A good gauge for the number of clusters is that which maximizes the average silhouette width.
```{r}
library(cluster)
par(mfrow=c(2,2))
for (k in 2:5) { 
    example_clusters <- cutree(PAG_tree, k=k)
    sil <- silhouette(example_clusters, dist=PAG_dist)
    plot(sil, col=rainbow(k)[sort(sil[,1])])
}
```

Other options for clustering are:

* Use the `cutreeDynamic()` function in the _dynamicTreeCut_ package, for topology-aware cutting of the tree (doesn't require specifying the number of clusters).
* Use graph-based methods such as `buildSNNGraph()` or `buildKNNGraph()`, followed by clustering methods from _igraph_.
* Use methods with pre-specified number of clusters, e.g., k-means with `kmeans()` and _SC3_, self-organizing maps in _flowSOM_.

## Step 7.2 | Identifying marker genes between subpopulations
We use the `findMarkers` function to detect differences between clusters. This will perform pairwise DE analyses between clusters, and consolidate the results into a single table of marker genes per cluster. The tricky part is how to summarize results from many pairwise comparisons into a single ranking of genes.
```{r}
marker_genes <- findMarkers(PAG_sceset_qc, clusters=PAG_clusters)
```

Let's consider the results of one of the clusters:
```{r}
marker_set_2 <- marker_genes[[2]] # Marker set for cluster 2
head(marker_set_2)

# Check what Top represents:
marker_set_2[marker_set_2$Top <= 1,]
```

Visualize the results with a heatmap:
```{r}
plotHeatmap(PAG_sceset_qc, 
            features=rownames(marker_set_2)[marker_set_2$Top <= 10],
            cluster_cols=PAG_tree, colour_columns_by="Cluster",
            center=TRUE, symmetric=TRUE)
```

A valid alternative strategy is to detect marker genes that are uniquely up-regulated or down-regulated in each cluster (set `pval.type="all"`). However, be aware that no such genes may exist. For example, in a mixed population of T cells, you could have CD4+ T cells, CD8+ T cells, double negative and double positive cells. If each of these formed a cluster, and we only looked for unique genes, neither CD4 or CD8 would be detected!

# 2.7.1 | One last thing...
Consider dropping any Ribosomal, Mitochondrial, ERCC, sex-related genes (e.g. XIST), transgenes (EYFP, tdTomato, Cre) and genes used for transgenic labeling of cells (VGAT and VGluT2) from the dataset before proceeding to downstream analysis, as they will not be biologically informative.
```{r}
# sceset <- sceset[!fData(sceset)$is_feature_control_Mitochondrial, ]
```