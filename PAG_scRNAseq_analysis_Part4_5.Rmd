---
title: "Topographic, single-cell gene expression profiling of Periaqueductal Gray neurons"
subtitle: "Parts IV & V: modelling the technical noise, feature selection, and dimensionality reduction"
author:
  - name: "Oriol Pavon Arocas, Sarah F. Olesen, and Tiago Branco"
    affiliation: "Sainsbury Wellcome Centre for Neural Circuits and Behaviour, University College London, UK"
    email: "oriol.pavon.16@ucl.ac.uk"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_notebook:
    highlight: pygments
    number_sections: FALSE
    theme: lumen
    toc: TRUE
    toc_float: TRUE

#output: rmdformats::readthedown:
  #highlight: pygments
---

***

This is a pipeline to analyse single-cell RNA sequencing data from Periaqueductal Gray neurons (1) isolated from acute midbrain slices of transgenic mice using visually guided aspiration via patch pipettes and (2) processed using SMART-seq2 (Picelli et al., Nature Protocols 2014). 

This pipeline has been generated after attending the [EMBL-EBI RNA-Sequence Analysis Course](https://www.ebi.ac.uk/training/events/2019/rna-sequence-analysis) and [attending](https://training.csx.cam.ac.uk/bioinformatics/event/2823386) and following the online course on [Analysis of single cell RNA-seq data](https://github.com/hemberg-lab/scRNA.seq.course) by the [Hemberg Lab](https://www.sanger.ac.uk/science/groups/hemberg-group). Many other resources have been used, including the [Orchestrating Single-Cell Analysis with Bioconductor book](https://osca.bioconductor.org/) by Robert Amezquita and Stephanie Hicks, the [simpleSingleCell workflow in Bioconductor](https://bioconductor.org/packages/3.9/workflows/html/simpleSingleCell.html) maintained by Aaron Lun, the [rnaseqGene workflow](https://bioconductor.org/packages/3.10/workflows/html/rnaseqGene.html) maintained by Michael Love, the [RNAseq123 workflow](https://bioconductor.org/packages/3.10/workflows/html/RNAseq123.html) maintained by Matthew Ritchie, and the [EGSEA123 workflow](https://bioconductor.org/packages/3.10/workflows/html/EGSEA123.html) maintained by Matthew Ritchie.

Other key resources include [Bioconductor](http://www.bioconductor.org/) (Huber et al., Nature Methods 2015), [scRNA-tools](https://www.scrna-tools.org/), `scater` (McCarthy et al., Bioinformatics 2017), `scran` (Lun et al., F1000Res 2016), `SC3` (Kiselev et al., Nature Methods 2017), `Seurat` (Butler et al., Nature Biotechnology 2018), `clusterExperiment` (Risso et al., PLOS Computational Biology 2018), `limma` (Ritchie et al., Nucleic Acids Research 2015), `DESeq2` (Love et al., Genome Biology 2014), `edgeR` (Robinson et al., Bioinformatics 2010), `MAST` (Finak, McDavid, Yajima et al., Genome Biology 2015), `iSEE` (Rue-Albrecht & Marini et al., F1000Research 2018), `t-SNE` (van der Maaten & Hinton, Journal of Machine Learning Research 2008), `UMAP` (McInnes et al., arXiv 2018), and the [Mathematical Statistics and Machine Learning for Life Sciences](https://towardsdatascience.com/tagged/stats-ml-life-sciences) column by Nikolay Oskolkov.

***

# STEP 4 | Modelling technical and biological variability in gene expression to identify highly variable genes
__Based on the [OSCA book](https://osca.bioconductor.org/feature-selection.html), the [simpleSingleCell workflow in Bioconductor](https://bioconductor.org/packages/3.9/workflows/html/simpleSingleCell.html), and [Description of the HVG machinery in scran](https://github.com/LTLA/HVGDetection2018) maintained by Aaron Lun.__

We often use scRNA-seq data in exploratory analyses to characterize heterogeneity across cells. Procedures like clustering and dimensionality reduction compare cells based on their gene expression profiles, which involves aggregating per-gene differences into a single (dis)similarity metric between a pair of cells. The choice of genes to use in this calculation has a major impact on the behavior of the metric and the performance of downstream methods. We want to select genes that contain useful information about the biology of the system while removing genes that contain random noise. This aims to preserve interesting biological structure without the variance that obscures that structure, and to reduce the size of the data to improve computational efficiency of later steps.

The simplest approach to feature selection is to select the most variable genes based on their expression across the population. This assumes that genuine biological differences will manifest as increased variation in the affected genes, compared to other genes that are only affected by technical noise or a baseline level of “uninteresting” biological variation (e.g., from transcriptional bursting). Several methods are available to quantify the variation per gene and to select an appropriate set of highly variable genes (HVGs).

We continue using the `PAG_sceset_qc_norm` after normalization. We should thus have a `logcounts` slot in `assays`:
```{r}
# If starting from stored results, load saved filtered dataset from previous Step:
set.seed(1991)
options(stringsAsFactors = FALSE)
library(SingleCellExperiment)
library(scater)
library(scran)

PAG_sceset_qc_norm <- readRDS("PAG_sceset_qc_norm.rds") # Contains filtered cells and genes, and log-normalized data
assayNames(PAG_sceset_qc_norm)
```

## Step 4.1 | Quantifying per-gene variation
Variability in the observed expression values across genes can be driven by genuine biological heterogeneity or uninteresting technical noise. To distinguish between these two possibilties, we need to model the technical component of the variance of the expression values for each gene.

### 4.1.1 | Variance of the log-counts
The simplest approach to quantifying per-gene variation is to simply compute the variance of the log-normalized expression values (referred to as “log-counts” for simplicity) for each gene across all cells in the population (A. T. L. Lun, McCarthy, and Marioni 2016). This has an advantage in that the feature selection is based on the same log-values that are used for later downstream steps. In particular, genes with the largest variances in log-values will contribute the most to the Euclidean distances between cells. By using log-values here, we ensure that our quantitative definition of heterogeneity is consistent throughout the entire analysis.

Calculation of the per-gene variance is simple but feature selection requires modelling of the mean-variance relationship. The log-transformation does not achieve perfect variance stabilization, which means that the variance of a gene is driven more by its abundance than its underlying biological heterogeneity. To account for this effect, we use the `modelGeneVar()` function to fit a trend to the variance with respect to abundance across all genes.
```{r}
library(scran)
start_time <- Sys.time() # Takes around 2s
var_model_no_spikes <- modelGeneVar(PAG_sceset_qc_norm,
                                    min.mean = 0.1, parametric = TRUE)
# If block is not specified, the metadata of the DataFrame contains the output of running `fitTrendVar` on the specified features, along with the mean and var used to fit the trend.
var_fit_no_spikes <- metadata(var_model_no_spikes) 
end_time <- Sys.time()
end_time - start_time
```

We visually inspect the trend to confirm that it corresponds to the spike-in variances. A wave-like shape is typical of the mean-variance trend for log-expression values. A linear increase in the variance is observed as the mean increases from zero, as larger variances are obviously possible when the counts are not all equal to zero. In contrast, the relative contribution of sampling noise decreases at high abundances, resulting in a downward trend. The peak represents the point at which these two competing effects cancel each other out.
```{r}
# Plot variance-mean trend without blocking
plot(var_fit_no_spikes$mean, 
     var_fit_no_spikes$var, 
     pch = 16, cex = 0.6,
     xlab = "Mean of log-expression", 
     ylab = "Variance of log-expression")

curve(var_fit_no_spikes$trend(x), col = "dodgerblue", add = TRUE, lwd = 2)
```

At any given abundance, we assume that the expression profiles of most genes are dominated by random technical noise. Under this assumption, our trend represents an estimate of the technical noise as a function of abundance. We then break down the total variance of each gene into the technical component, i.e., the fitted value of the trend at that gene’s abundance; and the biological component, defined as the difference between the total variance and the technical component. This biological component represents the “interesting” variation for each gene and can be used as the metric for HVG selection. Some genes will have negative biological components, which have no obvious interpretation and can be ignored in most applications. They are inevitable when fitting a trend to the per-gene variances as approximately half of the genes will lie below the trend.
```{r}
head(var_model_no_spikes[order(var_model_no_spikes$bio, decreasing = TRUE),])
```

### 4.1.2 | Coefficient of variation
An alternative approach to quantification uses the squared coefficient of variation (CV2) of the normalized expression values prior to log-transformation. The CV2 is a widely used metric for describing variation in non-negative data and is closely related to the dispersion parameter of the negative binomial distribution in packages like _edgeR_ and _DESeq2_. We can compute the CV2 for each gene in the dataset using the `modelGeneCV2()` function, which provides a robust implementation of the approach described by Brennecke et al. (2013).
```{r}
start_time <- Sys.time() # Takes around 2s
CV2_model_no_spikes <- modelGeneCV2(PAG_sceset_qc_norm)
CV2_fit_no_spikes <- metadata(CV2_model_no_spikes)
end_time <- Sys.time()
end_time - start_time
```

This allows us to model the mean-variance relationship when considering the relevance of each gene. Again, our assumption is that most genes contain random noise and that the trend captures mostly technical variation. Large CV2 values that deviate strongly from the trend are likely to represent genes affected by biological structure.
```{r}
# Plot CV2 as a function of the mean without blocking
plot(CV2_fit_no_spikes$mean,
     CV2_fit_no_spikes$cv2, 
     log = "xy",
     pch = 16, cex = 0.6,
     xlab = "Mean of expression", 
     ylab = "CV2 of expression", 
     xlim = c(1e-05, 1e+06))
curve(CV2_fit_no_spikes$trend(x), col = "dodgerblue", add = TRUE, lwd = 2)
```

For each gene, we quantify the deviation from the trend in terms of the ratio of its CV2 to the fitted value of trend at its abundance. This is more appropriate than the directly subtracting the trend from the CV2, as the magnitude of the ratio is not affected by the mean.
```{r}
head(CV2_model_no_spikes[order(CV2_model_no_spikes$ratio, decreasing = TRUE),])
```

Both the CV2 and the variance of log-counts are effective metrics for quantifying variation in gene expression. The CV2 tends to give higher rank to low-abundance HVGs driven by upregulation in rare subpopulations, for which the increase in variance on the raw scale is stronger than that on the log-scale. However, the variation described by the CV2 is less directly relevant to downstream procedures operating on the log-counts, and the reliance on the ratio can assign high rank to uninteresting genes with low absolute variance. We generally prefer the use of the variance of log-counts and will use it in the following sections, though the many of the same principles apply to procedures based on the CV2.

### 4.1.3 | Quantifying technical noise using spike-ins [not used]
Strictly speaking, the use of a trend fitted to endogenous genes assumes that the expression profiles of most genes are dominated by random technical noise. In practice, all expressed genes will exhibit some non-zero level of biological variability due to events like transcriptional bursting. This suggests that our estimates of the technical component are likely to be inflated. It would be more appropriate to consider these estimates as technical noise plus “uninteresting” biological variation, under the assumption that most genes are unaffected by the relevant heterogeneity in the population.

This revised assumption is generally reasonable but may be problematic in some scenarios where many genes at a particular abundance are affected by a biological process. For example, strong upregulation of cell type-specific genes may result in an enrichment of HVGs at high abundances. This would inflate the fitted trend in that abundance interval and compromise the detection of the relevant genes. We can avoid this problem by fitting a mean-dependent trend to the variance of the spike-in transcripts, if they are available. The premise here is that spike-ins were in theory added in the same quantity to each cell and thus should not be affected by biological variation (i.e., any variance in their counts should be technical in origin), so the fitted value of the spike-in trend should represent a better estimate of the technical component for each gene.
```{r}
#start_time <- Sys.time() # Takes around 5s
#var_model_spikes <- modelGeneVarWithSpikes(PAG_sceset_qc_norm, "ERCC")
#var_fit_spikes <- metadata(var_model_spikes)
#head(var_model_spikes[order(var_model_spikes$bio, decreasing = TRUE),])
#end_time <- Sys.time()
#end_time - start_time

# Plot variance-mean trend using spike-ins
#plot(var_model_spikes$mean,
     #var_model_spikes$total, 
     #xlab = "Mean of log-expression",
     #ylab = "Variance of log-expression")
#points(var_fit_spikes$mean, var_fit_spikes$var, col = "red", pch = 16)
#curve(var_fit_spikes$trend(x), col = "dodgerblue", add = TRUE, lwd = 2)
```

Trends based purely on technical noise tend to yield large biological components for highly-expressed genes. This often includes so-called “house-keeping” genes coding for essential cellular components such as ribosomal proteins, which are considered uninteresting for characterizing cellular heterogeneity. These observations suggest that a more accurate noise model does not necessarily yield a better ranking of HVGs, though one should keep an open mind - house-keeping genes are regularly DE in a variety of conditions (Glare et al. 2002; Nazari, Parham, and Maleki 2015; Guimaraes and Zavolan 2016), and the fact that they have large biological components indicates that there is strong variation across cells that may not be completely irrelevant.

Ideally, the technical component would be estimated by fitting a mean-variance trend to the spike-in transcripts. In practice, this strategy is compromised by the small number of spike-in transcripts, the uneven distribution of their abundances, and (for low numbers of cells) the imprecision of their variance estimates. This makes it difficult to accurately fit a complex mean-dependent trend to the spike-in variances. In some datasets, spike-in RNA may not have been added in appropriate quantities (or indeed at all). In our case, we don't detect all ERCCs in all the cells, so using them to model the variance in our dataset will yield poor results. It may also be inappropriate to assume Poisson technical noise, especially for read count data where amplification noise is non-negligible. In such cases, an alternative approach is to fit the trend to the variance estimates of the endogenous genes. This assumes that the majority of genes are not variably expressed, such that the technical component dominates the total variance for those genes. The fitted value of the trend is then used as an estimate of the technical component. 

### 4.1.4 | Accounting for blocking factors
Data containing multiple batches will often exhibit batch effects. We are usually not interested in HVGs that are driven by batch effects. Rather, we want to focus on genes that are highly variable within each batch. This is naturally achieved by performing trend fitting and variance decomposition separately for each batch.
```{r}
library(scran)
start_time <- Sys.time() # Takes around 20s
var_model_no_spikes_block <- modelGeneVar(PAG_sceset_qc_norm,
                                          block = PAG_sceset_qc_norm$batch.processing,
                                          min.mean = 0.1, parametric = TRUE)
head(var_model_no_spikes_block[order(var_model_no_spikes_block$bio, decreasing = TRUE),1:6])
end_time <- Sys.time()
end_time - start_time
```

The use of a batch-specific trend fit is useful as it accommodates differences in the mean-variance trends between batches. This is especially important if batches exhibit systematic technical differences, e.g., differences in coverage or in the amount of spike-in RNA added. 
```{r}
par(mfrow = c(4,6))
blocked.stats <- var_model_no_spikes_block$per.block
for (i in colnames(blocked.stats)) {
    current <- blocked.stats[[i]]
    plot(current$mean, 
         current$total,
         main = i, pch = 16, cex = 0.6,
         xlab = "Mean of log-expression", 
         ylab = "Variance of log-expression",
         xlim = c(0,20), ylim = c(0,30))
    curfit <- metadata(current)
    curve(curfit$trend(x), col = 'dodgerblue', add = TRUE, lwd = 2) 
}
```

## Step 4.2 | Strategies to identify highly variable genes
Once we have quantified the per-gene variation, the next step is to select the subset of HVGs to use in downstream analyses. Formal detection of HVGs allows us to avoid genes that are highly variable due to technical factors such as sampling noise during RNA capture and library preparation. A large subset will reduce the risk of discarding interesting biological signal by retaining more potentially relevant genes, at the cost of increasing noise from irrelevant genes that might obscure said signal. It is thus difficult to determine the optimal trade-off for any given application as noise in one context may be useful signal in another.

There are different ways to define HVGs. We could select the genes with biological components that are significantly greater than zero at a false discovery rate (FDR) of 5% or 1%. These genes are interesting as they drive differences in the expression profiles between cells, and should be prioritized for further investigation. In addition, we could consider a gene to be a HVG only if it has a biological component greater than or equal to 0.5. For transformed expression values on the log2 scale, this would mean that the average difference in true expression between any two cells will be at least 2-fold. We could also rank the results by the biological component to focus on genes with larger biological variability, and then choose a certain proportion of genes from the total.

### 4.2.1 | Based on the largest metrics
The simplest HVG selection strategy is to take the top X genes with the largest values for the relevant variance metric. The main advantage of this approach is that the user can directly control the number of genes retained, which ensures that the computational complexity of downstream calculations is easily predicted. 

The choice of X has a fairly straightforward biological interpretation. Recall our trend-fitting assumption that most genes do not exhibit biological heterogeneity; this implies that they are not differentially expressed between cell types or states in our population. If we quantify this assumption into a statement that, e.g., no more than 5% of genes are differentially expressed, we can naturally set X to 5% of the number of genes. In practice, we usually do not know the proportion of DE genes beforehand so this interpretation just exchanges one unknown for another. Nonetheless, it is still useful as it implies that we should lower X for less heterogeneous datasets, retaining most of the biological signal without unnecessary noise from irrelevant genes. Conversely, more heterogeneous datasets should use larger values of X to preserve secondary factors of variation beyond those driving the most obvious HVGs.

The main disadvantage of this approach is that it turns HVG selection into a competition between genes, whereby a subset of very highly variable genes can push other informative genes out of the top set. This can be problematic for analyses of highly heterogeneous populations if the loss of important markers prevents the resolution of certain subpopulations. In the most extreme example, consider a situation where a single subpopulation is very different from the others. In such cases, the top set will be dominated by differentially expressed genes involving that distinct subpopulation, compromising resolution of heterogeneity between the other populations. 

Another possible concern with this approach is the fact that the choice of X is fairly arbitrary, with any value from 500 to 5000 considered “reasonable”. The recommendation is to simply pick an arbitrary X and proceed with the rest of the analysis, with the intention of testing other choices later, rather than spending much time worrying about obtaining the “optimal” value.

### 4.2.2 | Based on significance
Another approach to feature selection is to set a fixed threshold of one of the metrics. This is most commonly done with the (adjusted) p-value reported by each of the above methods. The p-value for each gene is generated by testing against the null hypothesis that the variance is equal to the trend. For example, we might define our HVGs as all genes that have adjusted p-values below 0.05.

This approach is simple to implement and - if the test holds its size - it controls the false discovery rate (FDR). That is, it returns a subset of genes where the proportion of false positives is expected to be below the specified threshold. This can occasionally be useful in applications where the HVGs themselves are of interest. For example, if we were to use the list of HVGs in further experiments to verify the existence of heterogeneous expression for some of the genes, we would want to control the FDR in that list.

The downside of this approach is that it is less predictable than the top X strategy. The number of genes returned depends on the type II error rate of the test and the severity of the multiple testing correction. One might obtain no genes or every gene at a given FDR threshold, depending on the circumstances. Moreover, control of the FDR is usually not helpful at this stage of the analysis. We are not interpreting the individual HVGs themselves but are only using them for feature selection prior to downstream steps. There is no reason to think that a 5% threshold on the FDR yields a more suitable compromise between bias and noise compared to the top X selection.

As an aside, we might consider ranking genes by the p-value instead of the biological component for use in a top X approach. This results in some counterintuitive behavior due to the nature of the underlying hypothesis test, which is based on the ratio of the total variance to the expected technical variance. Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be biologically interesting. Many of the largest ratios are observed in high-abundance genes and are driven by very low technical variance; the total variance is typically modest for such genes, and they do not contribute much to population heterogeneity in absolute terms. (Note that the same can be said of the ratio of CV2 values.)
 
### 4.2.3 | Keeping all genes above the trend
We can also only remove the obviously uninteresting genes with variances below the trend. By doing so, we avoid the need to make any judgement calls regarding what level of variation is interesting enough to retain. This approach represents one extreme of the bias-variance trade-off where bias is minimized at the cost of maximizing noise. For `modelGeneVar()`, it equates to keeping all positive biological components, whereas for `modelGeneCV2()`, this involves keeping all ratios above 1.

By retaining all potential biological signal, we give secondary population structure the chance to manifest. This is most useful for rare subpopulations where the relevant markers will not exhibit strong overdispersion owing to the small number of affected cells. It will also preserve a weak but consistent effect across many genes with small biological components; admittedly, though, this is not of major interest in most scRNA-seq studies given the difficulty of experimentally validating population structure in the absence of strong marker genes.

The obvious cost is that more noise is also captured, which can reduce the resolution of otherwise well-separated populations and mask the secondary signal that we were trying to preserve. The use of more genes also introduces more computational work in each downstream step. This strategy is thus best suited to very heterogeneous populations containing many different cell types (possibly across many datasets that are to be merged, as in Chapter 13) where there is a justified fear of ignoring marker genes for low-abundance subpopulations under a competitive top X approach.

### 4.2.4 | Based on a priori genes of interest
A blunt yet effective feature selection strategy is to use pre-defined sets of interesting genes. The aim is to focus on specific aspects of biological heterogeneity that may be masked by other factors when using unsupervised methods for HVG selection. One example application lies in the dissection of transcriptional changes during the earliest stages of cell fate commitment (Messmer et al. 2019), which may be modest relative to activity in other pathways (e.g., cell cycle, metabolism). Indeed, if our aim is to show that there is no meaningful heterogeneity in a given pathway, we would - at the very least - be obliged to repeat our analysis using only the genes in that pathway to maximize power for detecting such heterogeneity.

Using scRNA-seq data in this manner is conceptually equivalent to a fluorescence activated cell sorting (FACS) experiment, with the convenience of being able to (re)define the features of interest at any time. For example, in the PBMC dataset, we might use some of the C7 immunologic signatures from MSigDB (Godec et al. 2016) to improve resolution of the various T cell subtypes. We stress that there is no shame in leveraging prior biological knowledge to address specific hypotheses in this manner. We say this because a common refrain in genomics is that the data analysis should be “unbiased”, i.e., free from any biological preconceptions. Attempting to derive biological insight ab initio is admirable but such “biases” are already present at every stage, starting from experimental design (why are we interested in this cell population in the first place?) and continuing through to interpretation of marker genes.

Of course, the downside of focusing on pre-defined genes is that it will limit our capacity to detect novel or unexpected aspects of variation. Thus, this kind of focused analysis should be complementary to (rather than a replacement for) the unsupervised feature selection strategies discussed previously.

Alternatively, we can also invert this reasoning to remove genes that are unlikely to be of interest prior to downstream analyses, thus avoiding unwanted variation that interferes with downstream interpretation. Common candidates for removal include ribosomal protein genes or mitochondrial genes; for immune cell subsets, we might also be inclined to remove immunoglobulin genes and T cell receptor genes, where clonal expression introduces (possibly irrelevant) population structure. In practice, we tend to err on the side of caution and abstain from preemptive filtering on biological function until these genes are demonstrably problematic in downstream analyses.

## Step 4.3 | Selecting HVGs in our dataset
We extract and rank the results to focus on genes with larger biological components. This highlights an interesting aspect of the underlying hypothesis test, which is based on the ratio of the total variance to the expected technical variance. Ranking based on p-value tends to prioritize HVGs that are more likely to be true positives but, at the same time, less likely to be interesting. This is because the ratio can be very large for HVGs that have very low total variance and do not contribute much to the cell-cell heterogeneity.

For `modelGeneVar()` and `modelGeneVarWithSpikes()`, we would select the genes with the largest biological components:
```{r}
# The `getTopHVGs()` function dentifies all genes where the relevant metric of variation is greater than var.threshold
hvg_var_no_spikes <- getTopHVGs(var_model_no_spikes,
                                var.field = "bio", var.threshold = 1, # Select all genes with positive biological components above a certain threshold
                                n = round(nrow(var_model_no_spikes)*0.15) # Select the top `n` number or proportion of genes
                                #fdr.field = "FDR", fdr.threshold = 0.05 # Select those genes with an adjusted p-value below 5% FDR
                                )
str(hvg_var_no_spikes)

# To export not just the names but also the output from the `modelGeneVar() we can do as follows:
hvg_var_out_no_spikes <- var_model_no_spikes[order(var_model_no_spikes$bio, decreasing = TRUE),]
hvg_var_out_no_spikes <- hvg_var_out_no_spikes[hvg_var_no_spikes,]
head(hvg_var_out_no_spikes)
dim(hvg_var_out_no_spikes)
```

We do the same for the variance modeling blocking by `batch.processing`
```{r}
# The `getTopHVGs()` function dentifies all genes where the relevant metric of variation is greater than var.threshold
hvg_var_no_spikes_block <- getTopHVGs(var_model_no_spikes_block,
                                      var.field = "bio", var.threshold = 1, # Select all genes with positive biological components above a certain threshold
                                      n = round(nrow(var_model_no_spikes_block)*0.15) # Select the top `n` number or proportion of genes
                                      #fdr.field = "FDR", fdr.threshold = 0.05 # Select those genes with an adjusted p-value below 5% FDR
                                      )
str(hvg_var_no_spikes_block)

# To export not just the names but also the output from the `modelGeneVar() we can do as follows:
hvg_var_out_no_spikes_block <- var_model_no_spikes_block[order(var_model_no_spikes_block$bio, decreasing = TRUE),]
hvg_var_out_no_spikes_block <- hvg_var_out_no_spikes_block[hvg_var_no_spikes_block,]
head(hvg_var_out_no_spikes_block[,1:6])
dim(hvg_var_out_no_spikes_block)
```

For `modelGeneCV2()` (and its relative, `modelGeneCV2WithSpikes()`), this would instead be the genes with the largest ratios:
```{r}
hvg_cv2_no_spikes <- getTopHVGs(CV2_model_no_spikes, 
                                var.field = "ratio", var.threshold = 1,
                                n = round(nrow(CV2_model_no_spikes)*0.15) # Select the top `n` number or proportion of genes
                                #fdr.field = "FDR", fdr.threshold = 0.05 # Select those genes with an adjusted p-value below 5% FDR
                                )
str(hvg_cv2_no_spikes)

# To export not just the names but also the output from the `modelGeneVar() we can do as follows:
hvg_cv2_out_no_spikes <- CV2_model_no_spikes[order(CV2_model_no_spikes$ratio, decreasing = TRUE),]
hvg_cv2_out_no_spikes <- hvg_cv2_out_no_spikes[hvg_cv2_no_spikes,]
head(hvg_cv2_out_no_spikes)
dim(hvg_cv2_out_no_spikes)
```

### 4.3.1 | Filter uninteresting genes from the dataset
__One important thing...__
Before moving on to downstream analysis, we consider dropping any Ribosomal, Mitochondrial, ERCC, sex-specific genes (e.g. XIST), transgenes (EYFP, tdTomato, Cre, TSO concatamers) and genes used for transgenic labeling of cells (VGAT and VGluT2) from the dataset, as some of them are not be biologically informative and others have been used to select the cells. We do so for both our `SingleCellExperiment` object and for the different HVGs:
```{r}
##### 
##Identify the genes we want to drop:
# ERCCs
rowData(PAG_sceset_qc_norm)$is_spike <- grepl("^ERCC-", rownames(PAG_sceset_qc_norm))
# Mitochondrial genes
rowData(PAG_sceset_qc_norm)$is_mitochondrial <- (rowData(PAG_sceset_qc_norm)$Chromosome == "M") # Select the mitochondrial chromosome
# Ribosomal genes, if you somehow marked the ribosomal gene IDs.
rowData(PAG_sceset_qc_norm)$is_ribosomal <- (rowData(PAG_sceset_qc_norm)$gene_biotype == "rRNA")
# Genes from sex chromosomes
rowData(PAG_sceset_qc_norm)$is_X <- (rowData(PAG_sceset_qc_norm)$Chromosome == "X")
rowData(PAG_sceset_qc_norm)$is_Y <- (rowData(PAG_sceset_qc_norm)$Chromosome == "Y")
# EYFP, tdTomato, Cre, and TSO concatamers
rowData(PAG_sceset_qc_norm)$is_tdTomato <- (rowData(PAG_sceset_qc_norm)$Chromosome == "A") # tdTomato fluorophor
rowData(PAG_sceset_qc_norm)$is_EYFP <- (rowData(PAG_sceset_qc_norm)$Chromosome == "B") # EYFP fluorophor
rowData(PAG_sceset_qc_norm)$is_Cre <- (rowData(PAG_sceset_qc_norm)$Chromosome == "C") # Cre transgene to express fluorophors
rowData(PAG_sceset_qc_norm)$is_SmartSeqTSO <- (rowData(PAG_sceset_qc_norm)$Chromosome == "D") # TSO concatamers
# VGAT and VGluT2
rowData(PAG_sceset_qc_norm)$is_VGAT <- (rowData(PAG_sceset_qc_norm)$gene_name == "Slc32a1")
rowData(PAG_sceset_qc_norm)$is_VGluT2 <- (rowData(PAG_sceset_qc_norm)$gene_name == "Slc17a6") 

###### 
## Merge all the genes to exclude:
rowData(PAG_sceset_qc_norm)$filter_genes_qc_norm <- !(rowData(PAG_sceset_qc_norm)$is_spike | 
                                                        rowData(PAG_sceset_qc_norm)$is_mitochondrial | 
                                                        rowData(PAG_sceset_qc_norm)$is_ribosomal | 
                                                        rowData(PAG_sceset_qc_norm)$is_X | 
                                                        rowData(PAG_sceset_qc_norm)$is_Y |
                                                        rowData(PAG_sceset_qc_norm)$is_tdTomato | 
                                                        rowData(PAG_sceset_qc_norm)$is_EYFP | 
                                                        rowData(PAG_sceset_qc_norm)$is_Cre | 
                                                        rowData(PAG_sceset_qc_norm)$is_SmartSeqTSO | 
                                                        rowData(PAG_sceset_qc_norm)$is_VGAT | 
                                                        rowData(PAG_sceset_qc_norm)$is_VGluT2)
sum(!rowData(PAG_sceset_qc_norm)$filter_genes_qc_norm)

#####
## Apply the filter to the SingleCellExperiment object:
dim(PAG_sceset_qc_norm)
PAG_sceset_qc_norm_filt <- PAG_sceset_qc_norm[rowData(PAG_sceset_qc_norm)$filter_genes_qc_norm, ]
dim(PAG_sceset_qc_norm_filt)

#####
## Apply the filter to the HVGs:
filt_hvg_var <- rownames(hvg_var_out_no_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_var_out_no_spikes_filt <- hvg_var_out_no_spikes[filt_hvg_var, ]
nrow(hvg_var_out_no_spikes_filt)

filt_hvg_var_block <- rownames(hvg_var_out_no_spikes_block) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_var_out_no_spikes_block_filt <- hvg_var_out_no_spikes_block[filt_hvg_var_block, ]
nrow(hvg_var_out_no_spikes_block_filt)

filt_hvg_cv2 <- rownames(hvg_cv2_out_no_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
hvg_cv2_out_no_spikes_filt <- hvg_cv2_out_no_spikes[filt_hvg_cv2, ]
nrow(hvg_cv2_out_no_spikes_filt)
```

### 4.3.2 | Visualize, store, and export HVGs
Now that we have removed the genes we are not interested in, we can use `limma` and `vennDiagram` to compare how many genes we get in each condition:
```{r}
library(limma)
# Inspect the overlap between HVGs obtained with the different approaches:
sum(rownames(hvg_var_out_no_spikes_filt) %in% rownames(hvg_var_out_no_spikes_block_filt))
sum(rownames(hvg_var_out_no_spikes_filt) %in% rownames(hvg_cv2_out_no_spikes_filt))
sum(rownames(hvg_cv2_out_no_spikes_filt) %in% rownames(hvg_var_out_no_spikes_block_filt))
venn_diag <- vennCounts(cbind(rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_var_out_no_spikes_filt),
                              rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_var_out_no_spikes_block_filt),
                              rownames(PAG_sceset_qc_norm_filt) %in% rownames(hvg_cv2_out_no_spikes_filt))
                        )
vennDiagram(venn_diag,
            names = c("Variance", "Variance $ Block", "CV2"),
            circle.col = c("#E69F00", "#56B4E9", "#009E73")
            )
```

Taking into account the above Venn Diagrams, it seems that modeling the variance with blocking by `batch.processing` captures almost all the genes captured without blocking. There list of HVGs for the variance and the CV2 modeling are quite different. We will thus keep all groups and compare them in the downstream analysis.

Before we store the identified HVGs in our `SingleCellExperiment` object, we check the distribution of expression values for the genes with the largest biological components to ensure that the variance estimate is not being dominated by one or two outlier cells.
```{r}
fontsize <- theme(axis.text = element_text(size = 12), axis.title = element_text(size = 16))

plotExpression(PAG_sceset_qc_norm_filt, features = rownames(hvg_var_out_no_spikes_filt)[1:50]) + fontsize
plotExpression(PAG_sceset_qc_norm_filt, features = rownames(hvg_var_out_no_spikes_block_filt)[1:50]) + fontsize
plotExpression(PAG_sceset_qc_norm_filt, features = rownames(hvg_cv2_out_no_spikes_filt)[1:50]) + fontsize
```

Now that we have ordered, filtered, and inspected the identified HVGs we can add the results into the `metadata` component of the `PAG_sceset_qc_norm_filt` object. The metadata component can hold any object, as it is a list container. Any results that we’d like to keep are safe to store here, and a great way to save or share intermediate results that would otherwise be kept in separate objects. We thus store the HVGs from the different approaches we used (even without filtering) so we can go back and find them again for any future comparisons we might want to do:
```{r}
# hvg_var_out_no_spikes_filt
metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes <- rownames(hvg_var_out_no_spikes)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes)

metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_filt <- rownames(hvg_var_out_no_spikes_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_filt)

# hvg_var_out_no_spikes_block_filt
metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block <- rownames(hvg_var_out_no_spikes_block)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block)

metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block_filt <- rownames(hvg_var_out_no_spikes_block_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block_filt)

# hvg_cv2_out_no_spikes_filt
metadata(PAG_sceset_qc_norm_filt)$hvg_cv2_out_no_spikes <- rownames(hvg_cv2_out_no_spikes)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_cv2_out_no_spikes)

metadata(PAG_sceset_qc_norm_filt)$hvg_cv2_out_no_spikes_filt <- rownames(hvg_cv2_out_no_spikes_filt)
length(metadata(PAG_sceset_qc_norm_filt)$hvg_cv2_out_no_spikes_filt)
```

We can also export them into a `.tsv` file:
```{r}
# Before filtering unwanted genes:
#write.table(file = "PAG_hvg_var_out_no_spikes.tsv", hvg_var_out_no_spikes, sep = "\t", quote = FALSE, col.names = NA)
#write.table(file = "PAG_hvg_var_out_no_spikes_block.tsv", hvg_var_out_no_spikes_block, sep = "\t", quote = FALSE, col.names = NA)
#write.table(file = "PAG_hvg_cv2_out_no_spikes.tsv", hvg_cv2_out_no_spikes, sep = "\t", quote = FALSE, col.names = NA)

# After filtering unwanted genes:
write.table(file = "PAG_hvg_var_out_no_spikes_filt.tsv", hvg_var_out_no_spikes_filt, sep = "\t", quote = FALSE, col.names = NA)
write.table(file = "PAG_hvg_var_out_no_spikes_block_filt.tsv", hvg_var_out_no_spikes_block_filt, sep = "\t", quote = FALSE, col.names = NA)
write.table(file = "PAG_hvg_cv2_out_no_spikes_filt.tsv", hvg_cv2_out_no_spikes_filt, sep = "\t", quote = FALSE, col.names = NA)
```

## Step 4.4 | Save the filtered SingleCellExperiment object
```{r}
# Save the filtered data:
saveRDS(PAG_sceset_qc_norm_filt, file = "PAG_sceset_qc_norm_filt.rds")
print("Part 4 - Done!")
```

# STEP 5 | Identifying confounding factors and correcting batch effects
Our data have been collected from samples originated from several animals and processed on different days. The processing of different batches is often subject to uncontrollable differences, e.g., changes in operator, differences in reagent quality. This results in systematic differences in the observed expression in cells from different batches, which we refer to as “batch effects”. Batch effects are problematic as they can be major drivers of heterogeneity in the data, masking the relevant biological differences and complicating interpretation of the results.

Computational correction of these effects is critical for eliminating batch-to-batch variation, allowing data across multiple batches to be combined for common downstream analysis. However, existing methods based on linear models (Ritchie et al. 2015; Leek et al. 2012) assume that the composition of cell populations are either known or the same across batches. To overcome these limitations, bespoke methods have been developed for batch correction of single-cell data (Haghverdi et al. 2018; Butler et al. 2018; Lin et al. 2019) that do not require a priori knowledge about the composition of the population. This allows them to be used in workflows for exploratory analyses of scRNA-seq data where such knowledge is usually unavailable.

To account for technical confounders we need to identify and remove sources of variation in the expression data that are not related to the biological signal of interest. We could use spike-ins for this (in theory you add the same amount of ERCC in each cell lysate, so any variability should be technical noise), but these can turn out to be extremely variable across cells. Instead, we could use endogenous or housekeeping genes that do not vary systematically between cells. Where we have a large number of endogenous genes that, on average, do not vary systematically between cells and where we expect technical effects to affect a large number of genes (a very common and reasonable assumption), then such methods of batch correction can perform well.

The most prominent technical covariates in single-cell data are _count depth_ and _processing batch_ (Luecken & Theis, Mol Syst Biol. 2019). Correcting for batch effects between samples or cells in the same experiment is the classical scenario known as _batch correction_ from bulk RNA-seq. This is different from _data integration_ from multiple experiments or collaborative projects. Irrespective of computational methods, the best method of batch correction is pre-empting the effect and avoiding it altogether by clever experimental design (Hicks et al, 2017). Batch effects can be avoided by pooling cells across experimental conditions and samples. 

## Step 5.1 | Identifying confounding and technical factors
As we have mentioned, a big source of variability in scRNA-seq data, besides bad cells, are batch effects. We want to try to identify such effects and remove them, so our analysis picks up mainly biological effects.

We continue using the `PAG_sceset_qc_norm_filt` after normalization. We should thus have a `logcounts` slot in `assays`:
```{r}
# If starting from stored results, load saved filtered dataset from previous Step:
set.seed(1991)
options(stringsAsFactors = FALSE)
library(SingleCellExperiment)
library(scater)
library(scran)

PAG_sceset_qc_norm_filt <- readRDS("PAG_sceset_qc_norm_filt.rds") # Contains filtered cells and genes, and log-normalized data
assayNames(PAG_sceset_qc_norm_filt)
```

### 5.1.1 | Screening for obvious biases with the table function
The `table` and `summary` functions allow us to quickly inspect if we have any obvious biases or confounds (i.e. all the cells come from male or female mice, or from a particular location or sequencing batch).
```{r}
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$PAG.hemisphere)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$PAG.arearegistration)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$PAG.APaxis)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$mouse.sex)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$mouse.id)
table(PAG_sceset_qc_norm_filt$cell.type, PAG_sceset_qc_norm_filt$batch.processing)
```

```{r}
summary(PAG_sceset_qc_norm_filt$cell.type)
summary(PAG_sceset_qc_norm_filt$PAG.hemisphere)
summary(PAG_sceset_qc_norm_filt$PAG.areacollection)
summary(PAG_sceset_qc_norm_filt$mouse.sex)
summary(PAG_sceset_qc_norm_filt$PAG.arearegistration)
```

### 5.1.2 | Checking for important technical factors or explanatory variables
We check whether there are technical factors that contribute substantially to the heterogeneity of gene expression. If so, the factor may need to be regressed out to ensure that it does not inflate the variances or introduce spurious correlations. `scater` can compute the marginal R^2^ for each variable when fitting a linear model regressing expression values for each gene against just that variable, and display a density plot of the gene-wise marginal R^2^ values for the variables.

For each gene, it calculates the percentage of the variance of the expression values that is explained by the variable. Small percentages (1-3%) indicate that the expression profiles of most genes are not strongly associated with this factor. If, on the other hand, we get density curves that are shifted to the right (i.e. with a peak towards the 100% end of the x-axis), this tells us that for a large proportion of the genes in our dataset this particular variable explains a large proportion of the variation.
```{r}
# How much of the variance does each variable explain?
plotExplanatoryVariables(PAG_sceset_qc_norm_filt,
                         nvars_to_plot = 17,
                         exprs_values = "logcounts",
                         variables = c("mouse.id",
                                       "mouse.sex",
                                       "mouse.age",
                                       "mouse.singlehousedays",
                                       "cell.type",
                                       "cell.fluorophor",
                                       "slice.number",
                                       "slice.depth",
                                       "PAG.areacollection",
                                       "PAG.hemisphere",
                                       "PAG.APaxis",
                                       "time.sinceslicinghour",
                                       "PAGarea_celltype",
                                       "batch.processing",
                                       "batch.sequencing_round",
                                       "detected",
                                       "total"
                                       )
                         ) + ggtitle("scran")
```

We can see that the two variables standing out are `batch.processing` and `mouse.id`. This makes perfect sense, as they illustrate how the cells were acquired and processed. Due to the way we isolated cells, however, `mouse.id` is confounded with `cell.type`: each transgenic mouse labeled either VGAT or VGluT2 neurons, which means that if we regress out the variability in `mouse.id` we will also be removing much of the biology linked to the `cell.type` that came from each mouse.

A better option would be to regress out the `batch.processing`, but only half of our samples were assigned to a balanced design. For the other half, all the cells obtained from the same animal were processed together, thereby confounding the `batch.processing` with the `mouse.id` (and by extension with `cell.type`) and leading to a similar scenario as above. However, this might be the best option we have, so we will regress out `batch.processing` and compare the results without the correction.

## Step 5.2 | Batch Correction
Small uncontrollable differences in processing between plates can result in a batch effect, i.e., systematic differences in expression between cells on different plates. Computational correction of these effects is critical for eliminating batch-to-batch variation, allowing data across multiple batches to be combined for valid downstream analysis. Batch effects in bulk RNA sequencing studies are commonly removed with linear regression. This involves fitting a linear model to each gene’s expression profile, setting the undesirable batch term to zero and recomputing the observations sans the batch effect, yielding a set of corrected expression values for downstream analyses. Linear modelling is the basis of the `removeBatchEffect()` function from the _limma_ package (Ritchie et al. 2015) as well the `comBat()` function from the _sva_ package (Leek et al. 2012).

To use this approach in a scRNA-seq context, we assume that the composition of cell subpopulations is the same across batches. We also assume that the batch effect is additive, i.e., any batch-induced fold-change in expression is the same across different cell subpopulations for any given gene. These are strong assumptions as batches derived from different individuals will naturally exhibit variation in cell type abundances and expression. Nonetheless, they may be acceptable when dealing with batches that are technical replicates generated from the same population of cells. (In fact, when its assumptions hold, linear regression is the most statistically efficient as it uses information from all cells to compute the common batch vector.) Linear modelling can also accommodate situations where the composition is known a priori by including the cell type as a factor in the linear model. `removeBatchEffect()` performs a linear regression and sets the coefficients corresponding to the blocking factors to zero. This is effective provided that the population composition within each batch is either known (and supplied as `design=`) or identical across batches. This is actually the closest to our situation, as we know exactly which cell type we are collecting and processing in each batch, so we can make use of that. Unfortunately, only half of our batches followed a balanced design, whereas for the other half each batch of cells comes from the same animal.

In most scRNA-seq applications, however, the factors of variation are not identical across batches and not known in advance. This motivates the use of more sophisticated batch correction methods such as `mnnCorrect()`, based on the detection of mutual nearest neighbours (MNNs) (Haghverdi et al. 2018). The MNN approach does not rely on pre-defined or equal population compositions across batches, only requiring that a subset of the population be shared between batches. In our case, each animal could be treated as a separate batch in their own right, reflecting (presumably uninteresting) biological differences due to genotype, age, sex or other factors that are inherent to our experimental design. `mnnCorrect()` was developed to merge different datasets obtained from the same biological system. It assumes that each batch shares at least one biological condition with each other batch, thus it works well for a variety of balanced experimental designs. However, in a confounded/replicate design biological effects will not be fit/preserved and we will only be able to remove batch effects from each individual separately in order to preserve biological (and technical) variance between individuals.

Batch correction is necessary for downstream procedures that are not model-based, e.g., clustering and most forms of dimensionality reduction. However, if an analysis method can accept a design matrix (such as differential expression analysis), blocking on nuisance factors in the design matrix is preferable to using `removeBatchEffect()`. This is because the latter does not account for the loss of residual degrees of freedom, nor the uncertainty of estimation of the blocking factor terms. For our experimental design, `removeBatchEffect()` seems more appropriate than `mnnCorrect()`. Whereas we know the exact composition of each batch and can provide in the `batch` argument of `removeBatchEffect()`, the main requirement for `mnnCorrect()` is unfortunately not fulfilled, as each batch contains only one of two possible cell types. We will apply `removeBatchEffect()` in our dataset and store the results in a new slot so that we can compare them in our clustering analysis. 

*Important:* The OSCA book suggests limiting the use of per-gene corrected values to visualization, e.g., when coloring points on a t-SNE plot by per-cell expression. This can be more aesthetically pleasing than uncorrected expression values that may contain large shifts on the colour scale between cells in different batches. Use of the corrected values in any quantitative procedure should be treated with caution, and should be backed up by similar results from an analysis on the uncorrected values. For gene-based procedures like differential expression (DE) analyses or gene network construction, it is desirable to use the original log-expression values or counts with batch effects being handled explicitly using blocking terms or via a meta-analysis across batches. In the dimensionality reduction steps, we will check whether our batches actually segregate and whether batch correction improves the obtained results.

### 5.2.1 | Applying removeBatchEffect()
`removeBatchEffect()` performs a linear regression and sets the coefficients corresponding to the blocking factors to zero. This function is useful for removing batch effects associated with technical variables prior to clustering or unsupervised analysis such as PCA, MDS or heatmaps. The design matrix is used to describe comparisons between the samples, for example treatment effects, which should not be removed. The function fits a linear model to the data, including both batches and regular treatments, then removes the component due to the batch effects.

This method is effective provided that the population composition within each batch is known or identical across batches. In our case, the composition of our cell population is __not__ identical across batches, as each batch comes from one transgenic mouse and thus has only one out of two possible cell types. However, individually aspirating cells based on expression of a transgene has the advantage that it allows us to know exactly the composition of each batch, so we can set the `design` and `batch` arguments as follows:
```{r}
library(limma)
assay(PAG_sceset_qc_norm_filt, "corrected") <- removeBatchEffect(logcounts(PAG_sceset_qc_norm_filt),
                                                                 batch = PAG_sceset_qc_norm_filt$batch.processing,
                                                                 design = model.matrix(~PAG_sceset_qc_norm_filt$PAGarea_celltype)
                                                                 )
assayNames(PAG_sceset_qc_norm_filt)
```

If the batch effect is successfully corrected, clusters corresponding to shared cell types or states should contain cells from multiple batches. The general recommendation is to use measured raw data for statistical testing (e.g. Differential Expression Analysis), corrected data for visual comparison of data, and reduced/feature selected data for other downstream analysis based on finding the underlying biological data manifold.

### 5.2.2 | Effectiveness of the correction
We can also use PCA to compare the effects of our normalisation and batch correction. To evaluate the effectiveness of the correction we can plot the PCA output where colour corresponds the technical replicates and shape corresponds to the individuals from which biological samples where acquired. Separation of biological samples and interspersed batches indicates that technical variation has been removed. In addition, if we plot the PCAs using the `size_by = total_features_by_counts` we can assess whether the total features correlates with the PCs: if there is a correlation with the total features, we will see that the cells are distributed in a particular way (e.g. with small bubbles on the left and large bubbles on the right). We should not observe a clear separation of cells by batch (`batch.processing` or `mouse.id`), indicating that our batch correction step using `removeBatchEffect()` was successful.

*[IMP]*: If `feature_set=NULL`, the `ntop` features with the largest variances are used instead. This literally computes the variances from the expression values without considering any mean-variance trend. Note that the value of `ntop` is ignored if `feature_set` is specified. Given that we have fitted a mean-variance trend and obtained highly variable genes from it, we will use our HVG instead of using the `ntop` option.
```{r}
# Compute PCA for each of the assay slots with the HVGs obtained with variance modeling:
for(n in assayNames(PAG_sceset_qc_norm_filt)) {
  PAG_sceset_qc_norm_filt <- runPCA(PAG_sceset_qc_norm_filt,
                                    exprs_values = n,
                                    subset_row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block_filt,
                                    name = paste0("PCA_HVG_var_", n)
  )
}

PCA_HVG_var_counts <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                     dimred = "PCA_HVG_var_counts",
                                     colour_by = "batch.processing", # "batch.processing", "PAG.arearegistration"
                                     shape_by = "cell.type"#, # "mouse.id"
                                     #size_by = "detected"
                                     ) + ggtitle("PCA_HVG_var_counts") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

PCA_HVG_var_logcounts_raw <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "PCA_HVG_var_logcounts_raw",
                                            colour_by = "batch.processing", # "batch.processing", "PAG.arearegistration"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("PCA_HVG_var_logcounts_raw") 

PCA_HVG_var_logcounts <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                        dimred = "PCA_HVG_var_logcounts",
                                        colour_by = "batch.processing", # "batch.processing", "PAG.arearegistration"
                                        shape_by = "cell.type"#, # "mouse.id"
                                        #size_by = "detected"
                                        ) + ggtitle("PCA_HVG_var_logcounts") 

PCA_HVG_var_corrected <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                        dimred = "PCA_HVG_var_corrected",
                                        colour_by = "batch.processing", # "batch.processing", "PAG.arearegistration"
                                        shape_by = "cell.type"#, # "mouse.id"
                                        #size_by = "detected"
                                        ) + ggtitle("PCA_HVG_var_corrected") 

multiplot(PCA_HVG_var_counts, PCA_HVG_var_logcounts_raw, PCA_HVG_var_logcounts, PCA_HVG_var_corrected, cols = 2)
```

```{r}
# Compute PCA for each of the assay slots with the HVGs obtained with CV2 modeling:
for(n in assayNames(PAG_sceset_qc_norm_filt)) {
  PAG_sceset_qc_norm_filt <- runPCA(PAG_sceset_qc_norm_filt,
                                    exprs_values = n,
                                    subset_row = metadata(PAG_sceset_qc_norm_filt)$hvg_cv2_out_no_spikes_filt,
                                    name = paste0("PCA_HVG_cv2_", n)
  )
}

PCA_HVG_cv2_counts <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                     dimred = "PCA_HVG_cv2_counts",
                                     colour_by = "batch.processing", # "batch.processing", "PAG.arearegistration"
                                     shape_by = "cell.type"#, # "mouse.id"
                                     #size_by = "detected"
                                     ) + ggtitle("PCA_HVG_cv2_counts") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

PCA_HVG_cv2_logcounts_raw <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "PCA_HVG_cv2_logcounts_raw",
                                            colour_by = "batch.processing", # "batch.processing", "PAG.arearegistration"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("PCA_HVG_cv2_logcounts_raw") 

PCA_HVG_cv2_logcounts <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                        dimred = "PCA_HVG_cv2_logcounts",
                                        colour_by = "batch.processing", # "batch.processing", "PAG.arearegistration"
                                        shape_by = "cell.type"#, # "mouse.id"
                                        #size_by = "detected"
                                        ) + ggtitle("PCA_HVG_cv2_logcounts") 

PCA_HVG_cv2_corrected <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                        dimred = "PCA_HVG_cv2_corrected",
                                        colour_by = "batch.processing", # "batch.processing", "PAG.arearegistration"
                                        shape_by = "cell.type"#, # "mouse.id"
                                        #size_by = "detected"
                                        ) + ggtitle("PCA_HVG_cv2_corrected") 

multiplot(PCA_HVG_cv2_counts, PCA_HVG_cv2_logcounts_raw, PCA_HVG_cv2_logcounts, PCA_HVG_cv2_corrected, cols = 2)
```

Hint: try "Ctss", "Cd14", or "Cd68" as `colour_by = `. This will show how the more separated cells in the t-SNE plots are actually macrophages.

We can also look at the change in the `plotExplanatoryVariables()` output after normalization and batch correction:
```{r}
for(n in assayNames(PAG_sceset_qc_norm_filt)) {
    print(
      plotExplanatoryVariables(PAG_sceset_qc_norm_filt,
                               nvars_to_plot = 17,
                               exprs_values = n,
                               variables = c("mouse.id",
                                             "mouse.sex",
                                             "mouse.age",
                                             "mouse.singlehousedays",
                                             "cell.type",
                                             "cell.fluorophor",
                                             "slice.number",
                                             "slice.depth",
                                             "PAG.areacollection",
                                             "PAG.hemisphere",
                                             "PAG.APaxis",
                                             "time.sinceslicinghour",
                                             "PAGarea_celltype",
                                             "batch.processing",
                                             "batch.sequencing_round",
                                             "detected",
                                             "total"
                                             )
                               ) + ggtitle(n)
      ) 
}
```

## Step 5.3 | Dimensionality reduction
Many scRNA-seq analysis procedures involve comparing cells based on their expression values across multiple genes. For example, clustering aims to identify cells with similar transcriptomic profiles by computing Euclidean distances across genes. In these applications, each individual gene represents a dimension of the data. More intuitively, if we had a scRNA-seq data set with two genes, we could make a two-dimensional plot where each axis represents the expression of one gene and each point in the plot represents a cell. This concept can be extended to data sets with thousands of genes where each cell’s expression profile defines its location in the high-dimensional expression space.

As the name suggests, dimensionality reduction aims to reduce the number of separate dimensions in the data. This is possible because different genes are correlated if they are affected by the same biological process. Thus, we do not need to store separate information for individual genes, but can instead compress multiple features into a single dimension, e.g., an “eigengene” (Langfelder and Horvath 2007). This reduces computational work in downstream analyses, as calculations only need to be performed for a few dimensions rather than thousands of genes; reduces noise by averaging across multiple genes to obtain a more precise representation of the patterns in the data; and enables effective plotting of the data, for those of us who are not capable of visualizing more than 3 dimensions

### 5.3.1 | Principal component analysis
Principal components analysis (PCA) discovers axes in high-dimensional space that capture the largest amount of variation. This is best understood by imagining each axis as a line. Say we draw a line anywhere, and we move all cells in our data set onto this line by the shortest path. The variance captured by this axis is defined as the variance across cells along that line. In PCA, the first axis (or “principal component”, PC) is chosen such that it captures the greatest variance across cells. The next PC is chosen such that it is orthogonal to the first and captures the greatest remaining amount of variation, and so on.

By definition, the top PCs capture the dominant factors of heterogeneity in the data set. Thus, we can perform dimensionality reduction by restricting downstream analyses to the top PCs. This strategy is simple, highly effective and widely used throughout the data sciences. It takes advantage of the well-studied theoretical properties of the PCA - namely, that a low-rank approximation formed from the top PCs is the optimal approximation of the original data for a given matrix rank. It also allows us to use a wide range of fast PCA implementations for scalable and efficient data analysis.

When applying PCA to scRNA-seq data, our assumption is that biological processes affect multiple genes in a coordinated manner. This means that the earlier PCs are likely to represent biological structure as more variation can be captured by considering the correlated behaviour of many genes. By comparison, random technical or biological noise is expected to affect each gene independently. There is unlikely to be an axis that can capture random variation across many genes, meaning that noise should mostly be concentrated in the later PCs. This motivates the use of the earlier PCs in our downstream analyses, which concentrates the biological signal to simultaneously reduce computational work and remove noise.

We perform the PCA on the log-normalized expression values using the `runPCA()` function from _scater_. By default, `runPCA()` will compute the first 50 PCs and store them in the `reducedDims()` of the output SingleCellExperiment object. We can use only the top 2000 genes with the largest biological components to reduce computational work and noise. Specifically, PCA is generally robust to random noise but an excess of it may cause the earlier PCs to capture noise instead of biological structure. This effect can be avoided - or at least mitigated - by restricting the PCA to a subset of HVGs.
```{r}
# We already performed PCA using all HVG in the previous step. 
reducedDimNames(PAG_sceset_qc_norm_filt)
```

```{r}
plotReducedDim(PAG_sceset_qc_norm_filt,
               dimred = "PCA_HVG_var_corrected",
               colour_by = "cell.type", # "batch.processing", "PAG.arearegistration"
               shape_by = "PAG.arearegistration"#, # "mouse.id"
               #size_by = "detected"
               ) + ggtitle("PCA_HVG_var_corrected") 
               
plotReducedDim(PAG_sceset_qc_norm_filt,
               dimred = "PCA_HVG_cv2_corrected",
               colour_by = "cell.type", # "batch.processing", "PAG.arearegistration"
               shape_by = "PAG.arearegistration"#, # "mouse.id"
               #size_by = "detected"
               ) + ggtitle("PCA_HVG_cv2_corrected") 
```

### 5.3.2 | Choosing the number of PCs
How many of the top PCs should we retain for downstream analyses? The choice of the number of PCs d is a decision that is analogous to the choice of the number of HVGs to use. Using more PCs will avoid discarding biological signal in later PCs, at the cost of retaining more noise. Most practitioners will simply set d to a “reasonable” but arbitrary value, typically ranging from 10 to 50. This is often satisfactory provided it is coupled with sufficient testing of alternative values to explore other perspectives of the data at a different bias-variance trade-off. Nonetheless, we will describe some more data-driven strategies to guide a suitable choice of d.

We can look at the proportion of variance explained by each PC and keep the components before the "elbow" in the curve of a scree plot:
```{r}
library(PCAtools)
set.seed(1991)

for(n in reducedDimNames(PAG_sceset_qc_norm_filt)[2:length(reducedDimNames(PAG_sceset_qc_norm_filt))]) { # First reducedDimNames instance is the PCA with the ColData for QC
  principal_components <- reducedDim(PAG_sceset_qc_norm_filt, n)
  proportion_var <- attr(principal_components, "percentVar")
  chosen_elbow <- PCAtools::findElbowPoint(proportion_var)
  print(chosen_elbow)
  plot(proportion_var, xlab = "PC", ylab = "Proportion of variance explained (%)", main = n)
  abline(v = chosen_elbow, col = "red")
}
```

Our assumption is that each of the top PCs capturing biological signal should explain much more variance than the remaining PCs. Thus, there should be a sharp drop in the percentage of variance explained when we move past the last “biological” PC. This manifests as an elbow in the scree plot, the location of which serves as a natural choice for d.

From a practical perspective, the use of the elbow point tends to retain fewer PCs compared to other methods. The definition of “much more variance” is relative so, in order to be retained, later PCs must explain an amount of variance that is comparable to that explained by the first few PCs. Strong biological variation in the early PCs will shift the elbow to the left, potentially excluding weaker (but still interesting) variation in the next PCs immediately following the elbow.

### 5.3.3 | Denoising expression values using PCA
Another strategy is to retain all PCs until the percentage of total variation explained reaches some threshold. For example, we might retain the top set of PCs that explains 80% of the total variation in the data. Of course, it would be pointless to swap one arbitrary parameter d for another T. Instead, we derive a suitable value for T by calculating the proportion of variance in the data that is attributed to the biological component. This is done using the `denoisePCA()` function with the variance modelling results from `modelGeneVar()` or related functions, where T is defined as the ratio of the sum of the biological components to the sum of total variances. Here, explicit feature selection is not strictly necessary, as `denoisePCA()` will automatically restrict the PCA to genes with positive biological components to ensure that T is always a positive value.

* `denoisePCA()` will only use genes that have positive biological components, i.e., variances greater than the fitted trend. This guarantees that the total technical variance to be discarded will not be greater than the total variance in the data.
* For the `technical = ` argument, the function will accept the trend function directly or a vector of technical components per gene. We can supply the DataFrame from `modelGeneVar()` to allow the function to adjust for the loss of residual degrees of freedom after batch correction. Specifically, the variance in the batch-corrected matrix is slightly understated, requiring some rescaling of the technical components to compensate.
* No filtering is performed on abundance here, which ensures that PCs corresponding to rare subpopulations can still be detected. Discreteness is less of an issue as low-abundance genes also have lower variance, thus reducing their contribution to the PCA.
* It is also possible to obtain a low-rank approximation of the original expression matrix, capturing the variance equivalent to the retained PCs. This is useful for denoising prior to downstream procedures that require gene-wise expression values.
```{r}
set.seed(1991)

# Run denoisePCA for hvg_var_out_no_spikes_filt
dim(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes)
var_model_no_spikes$filter <- rownames(var_model_no_spikes) %in% rownames(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes)
var_model_no_spikes_filt <- var_model_no_spikes[var_model_no_spikes$filter==TRUE, ]
dim(var_model_no_spikes_filt)

PAG_var_denoised <- denoisePCA(PAG_sceset_qc_norm_filt,
                               technical = var_model_no_spikes_filt, 
                               subset.row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_filt,
                               assay.type = "logcounts")

print(ncol(reducedDim(PAG_var_denoised, "PCA")))

plotReducedDim(PAG_var_denoised,
               dimred = "PCA",
               colour_by = "cell.type",
               shape_by = "PAG.arearegistration",
               by_exprs_values = "logcounts")

# Run denoisePCA for hvg_var_out_no_spikes_block_filt
dim(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes_block)
var_model_no_spikes_block$filter <- rownames(var_model_no_spikes_block) %in% rownames(PAG_sceset_qc_norm_filt)
dim(var_model_no_spikes_block)
var_model_no_spikes_block_filt <- var_model_no_spikes_block[var_model_no_spikes_block$filter==TRUE, ]
dim(var_model_no_spikes_block_filt)

PAG_var_block_denoised <- denoisePCA(PAG_sceset_qc_norm_filt,
                                     technical = var_model_no_spikes_block_filt, 
                                     subset.row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block_filt,
                                     assay.type = "logcounts")

print(ncol(reducedDim(PAG_var_block_denoised, "PCA")))

plotReducedDim(PAG_var_block_denoised,
               dimred = "PCA",
               colour_by = "cell.type",
               shape_by = "PAG.arearegistration",
               by_exprs_values = "logcounts")
```

The function returns a `SingleCellExperiment` object containing the PC scores for each cell in the `reducedDims` slot, where cells are rows and PCs are columns. The aim is to eliminate technical noise and enrich for biological signal in the retained PCs. This improves resolution of the underlying biology during downstream procedures such as clustering.

The dimensionality of the output represents the lower bound on the number of PCs required to retain all biological variation. Any fewer PCs will definitely discard some aspect of biological signal. Note that the converse is not true, i.e., there is no guarantee that the retained PCs capture all of the signal, which is only generally possible if no dimensionality reduction is performed at all. The returned value of _d_ provides a reasonable choice of rank when we want to retain as much signal as possible while still removing some noise.

From a practical perspective, the `denoisePCA()` approach usually retains more PCs than the elbow point method. This is because the former does not compare PCs to each other and thus does not discard PCs corresponding to secondary factors of variation. The downside is that many minor aspects of variation may not be interesting (e.g., transcriptional bursting) and their retention would only add irrelevant noise. Thus, whether this is a “better” approach depends on the analyst’s willingness to increase noise in order to preserve weaker biological signals.

Incidentally, `denoisePCA()` imposes internal caps on the number of PCs that can be chosen in this manner. By default, the number is bounded within the “reasonable” limits of 5 and 50 to avoid selection of too few PCs (when technical noise is high relative to biological variation) or too many PCs (when technical noise is very low).

## Step 5.4 | Visualization of the data after dimensionality reduction
Dimensionality reduction allows us to compress the data into 2 or 3 dimensions for plotting. 

### 5.4.1 | Visualization with PCA
Principal component analysis (PCA) is a statistical procedure that uses a transformation to convert a set of observations into a set of values of linearly uncorrelated variables called principal components (PCs). The number of principal components is less than or equal to the number of original variables. Mathematically, the PCs correspond to the eigenvectors of the covariance matrix. The eigenvectors are sorted by eigenvalue so that the first principal component accounts for as much of the variability in the data as possible, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.

We can apply PCA to visualize the relationships between cells in our dataset. Cells with similar expression profiles should be located close together in the plot, while dissimilar cells should be far apart. Ideally, we do this using the `logcounts` or the `corrected` slot of our `SingleCellExperiment` object. Log-transformation reduces the variance on the first principal component, separates some biological effects, and makes the distribution of the expression values more normal. BUT - log-transformation is not a proper normalisation step and is not enough to account for different technical factors between the cells (e.g. sequencing depth). Therefore, we should not use `logcounts_raw` for our downstream analysis, but instead use the __logcounts__ slot of the `SingleCellExperiment` object as a minimum suitable data, which is not just log-transformed, but also normalised by library size. By default `scater` only uses the top 500 most variable genes to calculate the PCA, but this can be adjusted by changing the __ntop__ argument. We can also choose to only use our HVGs, or any given list of genes we may obtain by other feature selection strategies.

The problem here is that PCA is a linear technique, i.e., only variation along a line in high-dimensional space is captured by each PC. As such, it cannot efficiently pack differences in d dimensions into the first 2 PCs.
```{r}
reducedDimNames(PAG_sceset_qc_norm_filt)
```

```{r}
plotReducedDim(PAG_sceset_qc_norm_filt,
               dimred = "PCA_HVG_var_corrected",
               colour_by = "cell.type", # "batch.processing", "PAG.arearegistration"
               shape_by = "PAG.arearegistration"#, # "mouse.id"
               #size_by = "detected"
               ) + ggtitle("PCA_HVG_var_corrected") 
               
plotReducedDim(PAG_sceset_qc_norm_filt,
               dimred = "PCA_HVG_cv2_corrected",
               colour_by = "cell.type", # "batch.processing", "PAG.arearegistration"
               shape_by = "PAG.arearegistration"#, # "mouse.id"
               #size_by = "detected"
               ) + ggtitle("PCA_HVG_cv2_corrected") 
```

One workaround is to plot several of the top PCs against each other in pairwise plots. Cells with similar expression profiles should be located close together in the plot, while dissimilar cells should be far apart. Additional components can be visualized by increasing the `ncomponents` argument in `plotPCA` to construct pairwise plots. The percentage of variance explained by each component can also be obtained by running `plotPCA` with `return_SCESet = TRUE`, and then calling `reducedDimension` on the returned object. However, it is difficult to interpret multiple plots simultaneously, and even this approach is not sufficient to separate some of the annotated subpopulations.
```{r}
plotReducedDim(PAG_sceset_qc_norm_filt,
               dimred = "PCA_HVG_var_corrected",
               ncomponents = 4,
               colour_by = "cell.type",
               shape_by = "PAG.arearegistration",
               by_exprs_values = "corrected")
```

Some advantages of using PCA for visualization are that it is predictable and will not introduce artificial structure in the visualization. It is also deterministic and robust to small changes in the input values. However, PCA is usually not satisfactory for visualization of complex populations.

### 5.4.2 | t-SNE (t-stochastic neighbor embedding)
Another widely used approach for dimensionality reduction is the t-stochastic neighbour embedding (t-SNE) method (Van der Maaten and Hinton 2008). tSNE combines dimensionality reduction with random walks on the nearest-neighbour network to map high dimensional data to a 2-dimensional space while preserving local distances between cells. t-SNE tends to work better than PCA for separating cells in more diverse populations. This is because the former can directly capture non-linear relationships in high-dimensional space, whereas the latter must represent them (suboptimally) as linear components. However, this improvement comes at the cost of more computational effort and complexity. In particular, the non-linear and stochastic nature of the algorithm means that running it multiple times on the same dataset will result in different plots, making the results more difficult to intuitively interpret. Users should thus run the algorithm several times to ensure that the results are representative, and then set a `seed` to ensure that the chosen results are reproducible. It is also advisable to test different settings of the `perplexity` parameter as this will affect the distribution of points in the low-dimensional space.

tSNE plots are the _de facto_ standard for visualization of scRNA-seq data, but should *NOT* be used for clustering. Furthermore, tSNE requires you to provide a value of `perplexity` which reflects the number of neighbours used to build the nearest-neighbour network - a high value creates a dense network which clumps cells together, whereas a low value makes the network more sparse allowing groups of cells to separate from each other. `scater` uses a default `perplexity` equal to the total number of cells divided by five (rounded down). You should try different perplexities and run it several times to make sure you are not getting a weird result. If your data are very homogeneous the tSNE plots are going to be hard to interpret, whereas if they contain very well defined clusters it will visualize them very nicely.

One of the main disadvantages of t-SNE is that it is much more computationally intensive than other visualization methods. We mitigate this effect by setting `dimred="PCA"` in `runtTSNE()`, which instructs the function to perform the t-SNE calculations on the top PCs to exploit the data compaction and noise removal provided by the PCA. It is possible to run t-SNE on the original expression matrix but this is less efficient.

Another issue with t-SNE is that it requires the user to be aware of additional parameters. It involves a random initialization so we need to (i) repeat the visualization several times to ensure that the results are representative and (ii) set the seed to ensure that the chosen results are reproducible. The `perplexity` is another important parameter that determines the granularity of the visualization. Low perplexities will favour resolution of finer structure, possibly to the point that the visualization is compromised by random noise. Thus, it is advisable to test different perplexity values to ensure that the choice of perplexity does not drive the interpretation of the plot. As a rule of thumb, N^1/2 shouldn't be far off the right perplexity, according to [this post by Nikolay Oskolkov](https://towardsdatascience.com/how-to-tune-hyperparameters-of-tsne-7c0596a18868)

Finally, it is tempting to interpret the t-SNE results as a “map” of single-cell identities. This is generally unwise as any such interpretation is easily misled by the size and positions of the visual clusters. Specifically, t-SNE will inflate dense clusters and compress sparse ones, such that we cannot use the size as a measure of subpopulation heterogeneity. Similarly, t-SNE is not obliged to preserve the relative locations of non-neighboring clusters, such that we cannot use their positions to determine relationships between distant clusters. Despite its shortcomings, t-SNE is a proven tool for general-purpose visualization of scRNA-seq data and remains a popular choice in many analysis pipelines.
```{r}
# Compute tSNE with different perplexities:
set.seed(1991)
range_of_perplexities = list(5, 10, 15, 20, 30, 40, 50, 75) # N^1/2 shouldn't be far off
for(n in range_of_perplexities) {
  PAG_sceset_qc_norm_filt <- runTSNE(PAG_sceset_qc_norm_filt,
                                     exprs_values = "corrected",
                                     subset_row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block_filt,
                                     dimred = "PCA_HVG_var_corrected", 
                                     n_dimred = 25,
                                     perplexity = n, # Perplexity parameter (should not be bigger than 3 * perplexity < nrow(X) - 1)
                                     theta = 0.5, # Speed/accuracy trade-off (increase for less accuracy), set to 0.0 for exact TSNE (default: 0.5)
                                     max_iter = 5000, # Default is 1000. 
                                     eta = 1000, # Learning rate (default: 200) - increased as per suggestion of Kobak and Berens, Nature Communications 2019
                                     exaggeration_factor = 12, # Default: 12
                                     name = paste0("tSNE_perplexity_", n)
  )
}

reducedDimNames(PAG_sceset_qc_norm_filt)

tSNE_HVG_var_corrected_5 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                           dimred = "tSNE_perplexity_5",
                                           colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                           shape_by = "cell.type"#, # "mouse.id"
                                           #size_by = "detected"
                                           ) + ggtitle("tSNE_perplexity_5") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_var_corrected_10 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_perplexity_10",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_perplexity_10") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_var_corrected_15 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_perplexity_15",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_perplexity_15") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_var_corrected_20 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_perplexity_20",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_perplexity_20") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_var_corrected_30 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_perplexity_30",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_perplexity_30") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_var_corrected_40 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_perplexity_40",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_perplexity_40") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_var_corrected_50 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_perplexity_50",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_perplexity_50") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_var_corrected_75 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_perplexity_75",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_perplexity_75") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

multiplot(tSNE_HVG_var_corrected_5, tSNE_HVG_var_corrected_10, tSNE_HVG_var_corrected_15, tSNE_HVG_var_corrected_20, 
          cols = 2)
multiplot(tSNE_HVG_var_corrected_30, tSNE_HVG_var_corrected_40, tSNE_HVG_var_corrected_50, tSNE_HVG_var_corrected_75, 
          cols = 2)
```

Repeat using the PCA results obtained with `HVG_cv2_corrected`:
```{r}
# Compute tSNE with different perplexities:
set.seed(1991)
range_of_perplexities = list(5, 10, 15, 20, 30, 40, 50, 75) # N^1/2 shouldn't be far off
for(n in range_of_perplexities) {
  PAG_sceset_qc_norm_filt <- runTSNE(PAG_sceset_qc_norm_filt,
                                     exprs_values = "corrected",
                                     subset_row = metadata(PAG_sceset_qc_norm_filt)$hvg_cv2_out_no_spikes_filt,
                                     dimred = "PCA_HVG_cv2_corrected", 
                                     n_dimred = 25,
                                     perplexity = n, # Perplexity parameter (should not be bigger than 3 * perplexity < nrow(X) - 1)
                                     theta = 0.5, # Speed/accuracy trade-off (increase for less accuracy), set to 0.0 for exact TSNE (default: 0.5)
                                     max_iter = 5000, # Default is 1000. 
                                     eta = 1000, # Learning rate (default: 200) - increased as per suggestion of Kobak and Berens, Nature Communications 2019
                                     exaggeration_factor = 12, # Default: 12
                                     name = paste0("tSNE_cv2_perplexity_", n)
  )
}

reducedDimNames(PAG_sceset_qc_norm_filt)

tSNE_HVG_cv2_corrected_5 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                           dimred = "tSNE_cv2_perplexity_5",
                                           colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                           shape_by = "cell.type"#, # "mouse.id"
                                           #size_by = "detected"
                                           ) + ggtitle("tSNE_cv2_perplexity_5") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_cv2_corrected_10 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_cv2_perplexity_10",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_cv2_perplexity_10") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_cv2_corrected_15 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_cv2_perplexity_15",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_cv2_perplexity_15") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_cv2_corrected_20 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_cv2_perplexity_20",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_cv2_perplexity_20") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_cv2_corrected_30 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_cv2_perplexity_30",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_cv2_perplexity_30") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_cv2_corrected_40 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                           dimred = "tSNE_cv2_perplexity_40",
                                           colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                           shape_by = "cell.type"#, # "mouse.id"
                                           #size_by = "detected"
                                           ) + ggtitle("tSNE_cv2_perplexity_40") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_cv2_corrected_50 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_cv2_perplexity_50",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_cv2_perplexity_50") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))

tSNE_HVG_cv2_corrected_75 <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                            dimred = "tSNE_cv2_perplexity_75",
                                            colour_by = "Cd68", # "batch.processing", "PAG.arearegistration", "Cd68"
                                            shape_by = "cell.type"#, # "mouse.id"
                                            #size_by = "detected"
                                            ) + ggtitle("tSNE_cv2_perplexity_75") #+ coord_cartesian(xlim = c(-35, 35), ylim = c(-35, 35))


multiplot(tSNE_HVG_cv2_corrected_5, tSNE_HVG_cv2_corrected_10, tSNE_HVG_cv2_corrected_15, tSNE_HVG_cv2_corrected_20,
          cols = 2)
multiplot(tSNE_HVG_cv2_corrected_30, tSNE_HVG_cv2_corrected_40, tSNE_HVG_cv2_corrected_50, tSNE_HVG_cv2_corrected_75, 
          cols = 2)
```

### 5.4.3 | UMAP (uniform manifold approximation and projection)
The uniform manifold approximation and projection (UMAP) method (McInnes, Healy, and Melville 2018) is an alternative to t-SNE for non-linear dimensionality reduction. It is roughly similar to t-SNE in that it also tries to find a low-dimensional representation that preserves relationships between neighbors in high-dimensional space. However, the two methods are based on different theory, represented by differences in the various graph weighting equations, which leads to a different visualization.
```{r}
set.seed(1991)
range_of_neighbors = list(5, 10, 15, 20)
range_of_min_dist = list(0.01, 0.05, 0.1, 0.2)

for(j in range_of_neighbors) {
  for (k in range_of_min_dist) {
    PAG_sceset_qc_norm_filt <- runUMAP(PAG_sceset_qc_norm_filt,
                                       exprs_values = "corrected",
                                       subset_row = metadata(PAG_sceset_qc_norm_filt)$hvg_var_out_no_spikes_block_filt, 
                                       dimred = "PCA_HVG_var_corrected",
                                       n_dimred = 25, # Choose the number of dimensions to use from the specificed dimred                                        
                                       n_neighbors = j, #  Larger values result in more global views of the manifold, while smaller values result in more local data being preserved.
                                       metric = "euclidean",
                                       n_epochs = 500, # Number of epochs to use during the optimization of the embedded coordinates.
                                       learning_rate = 1,
                                       min_dist = k, # Low values will result in densely packed regions, that will likely more faithfully represent the manifold structure.
                                       name = paste0("UMAP_var_corrected_", j, "_neighbors_", k, "_min_dist"))
  }
}

reducedDimNames(PAG_sceset_qc_norm_filt)

#### n_neighbors = 5
UMAP_var_corrected_5_neighbors_0.01_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_5_neighbors_0.01_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_5_neighbors_0.01_min_dist")
UMAP_var_corrected_5_neighbors_0.05_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_5_neighbors_0.05_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_5_neighbors_0.05_min_dist")
UMAP_var_corrected_5_neighbors_0.1_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                              dimred = "UMAP_var_corrected_5_neighbors_0.1_min_dist",
                                                              colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                              shape_by = "cell.type"#, # "mouse.id"
                                                              #size_by = "detected"
                                                              ) + ggtitle("UMAP_var_corrected_5_neighbors_0.1_min_dist")
UMAP_var_corrected_5_neighbors_0.2_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_5_neighbors_0.2_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_5_neighbors_0.2_min_dist")

#### n_neighbors = 10
UMAP_var_corrected_10_neighbors_0.01_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_var_corrected_10_neighbors_0.01_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_var_corrected_10_neighbors_0.01_min_dist")
UMAP_var_corrected_10_neighbors_0.05_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_var_corrected_10_neighbors_0.05_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_var_corrected_10_neighbors_0.05_min_dist")
UMAP_var_corrected_10_neighbors_0.1_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_10_neighbors_0.1_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_10_neighbors_0.1_min_dist")
UMAP_var_corrected_10_neighbors_0.2_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_10_neighbors_0.2_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_10_neighbors_0.2_min_dist")

#### n_neighbors = 15
UMAP_var_corrected_15_neighbors_0.01_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_var_corrected_15_neighbors_0.01_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_var_corrected_15_neighbors_0.01_min_dist")
UMAP_var_corrected_15_neighbors_0.05_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_var_corrected_15_neighbors_0.05_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_var_corrected_15_neighbors_0.05_min_dist")
UMAP_var_corrected_15_neighbors_0.1_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_15_neighbors_0.1_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_15_neighbors_0.1_min_dist")
UMAP_var_corrected_15_neighbors_0.2_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_15_neighbors_0.2_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_15_neighbors_0.2_min_dist")

#### n_neighbors = 20
UMAP_var_corrected_20_neighbors_0.01_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_var_corrected_20_neighbors_0.01_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_var_corrected_20_neighbors_0.01_min_dist")
UMAP_var_corrected_20_neighbors_0.05_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_var_corrected_20_neighbors_0.05_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_var_corrected_20_neighbors_0.05_min_dist")
UMAP_var_corrected_20_neighbors_0.1_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_20_neighbors_0.1_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_20_neighbors_0.1_min_dist")
UMAP_var_corrected_20_neighbors_0.2_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_var_corrected_20_neighbors_0.2_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration", "Cd68"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_var_corrected_20_neighbors_0.2_min_dist")

#### Plot them all:
multiplot(UMAP_var_corrected_5_neighbors_0.01_min_dist, UMAP_var_corrected_5_neighbors_0.05_min_dist,
          UMAP_var_corrected_5_neighbors_0.1_min_dist, UMAP_var_corrected_5_neighbors_0.2_min_dist,
          cols = 2)
multiplot(UMAP_var_corrected_10_neighbors_0.01_min_dist, UMAP_var_corrected_10_neighbors_0.05_min_dist,
          UMAP_var_corrected_10_neighbors_0.1_min_dist, UMAP_var_corrected_10_neighbors_0.2_min_dist,
          cols = 2)
multiplot(UMAP_var_corrected_15_neighbors_0.01_min_dist, UMAP_var_corrected_15_neighbors_0.05_min_dist,
          UMAP_var_corrected_15_neighbors_0.1_min_dist, UMAP_var_corrected_15_neighbors_0.2_min_dist,
          cols = 2)
multiplot(UMAP_var_corrected_20_neighbors_0.01_min_dist, UMAP_var_corrected_20_neighbors_0.05_min_dist,
          UMAP_var_corrected_20_neighbors_0.1_min_dist, UMAP_var_corrected_20_neighbors_0.2_min_dist,
          cols = 2)
```

```{r}
set.seed(1991)
range_of_neighbors = list(5, 10, 15, 20)
range_of_min_dist = list(0.01, 0.05, 0.1, 0.2)

for(j in range_of_neighbors) {
  for (k in range_of_min_dist) {
    PAG_sceset_qc_norm_filt <- runUMAP(PAG_sceset_qc_norm_filt,
                                       exprs_values = "corrected",
                                       subset_row = metadata(PAG_sceset_qc_norm_filt)$hvg_cv2_out_no_spikes_filt, 
                                       dimred = "PCA_HVG_cv2_corrected",
                                       n_dimred = 25, # Choose the number of dimensions to use from the specificed dimred                                        
                                       n_neighbors = j, #  Larger values result in more global views of the manifold, while smaller values result in more local data being preserved.
                                       metric = "euclidean",
                                       n_epochs = 500, # Number of epochs to use during the optimization of the embedded coordinates.
                                       learning_rate = 1,
                                       min_dist = k, # Low values will result in densely packed regions, that will likely more faithfully represent the manifold structure.
                                       name = paste0("UMAP_cv2_corrected_", j, "_neighbors_", k, "_min_dist"))
  }
}

reducedDimNames(PAG_sceset_qc_norm_filt)

#### n_neighbors = 5
UMAP_cv2_corrected_5_neighbors_0.01_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_5_neighbors_0.01_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_5_neighbors_0.01_min_dist")
UMAP_cv2_corrected_5_neighbors_0.05_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_5_neighbors_0.05_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_5_neighbors_0.05_min_dist")
UMAP_cv2_corrected_5_neighbors_0.1_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                              dimred = "UMAP_cv2_corrected_5_neighbors_0.1_min_dist",
                                                              colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                              shape_by = "cell.type"#, # "mouse.id"
                                                              #size_by = "detected"
                                                              ) + ggtitle("UMAP_cv2_corrected_5_neighbors_0.1_min_dist")
UMAP_cv2_corrected_5_neighbors_0.2_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_5_neighbors_0.2_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_5_neighbors_0.2_min_dist")

#### n_neighbors = 10
UMAP_cv2_corrected_10_neighbors_0.01_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_cv2_corrected_10_neighbors_0.01_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_cv2_corrected_10_neighbors_0.01_min_dist")
UMAP_cv2_corrected_10_neighbors_0.05_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_cv2_corrected_10_neighbors_0.05_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_cv2_corrected_10_neighbors_0.05_min_dist")
UMAP_cv2_corrected_10_neighbors_0.1_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_10_neighbors_0.1_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_10_neighbors_0.1_min_dist")
UMAP_cv2_corrected_10_neighbors_0.2_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_10_neighbors_0.2_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_10_neighbors_0.2_min_dist")

#### n_neighbors = 15
UMAP_cv2_corrected_15_neighbors_0.01_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_cv2_corrected_15_neighbors_0.01_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_cv2_corrected_15_neighbors_0.01_min_dist")
UMAP_cv2_corrected_15_neighbors_0.05_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_cv2_corrected_15_neighbors_0.05_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_cv2_corrected_15_neighbors_0.05_min_dist")
UMAP_cv2_corrected_15_neighbors_0.1_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_15_neighbors_0.1_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_15_neighbors_0.1_min_dist")
UMAP_cv2_corrected_15_neighbors_0.2_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_15_neighbors_0.2_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_15_neighbors_0.2_min_dist")

#### n_neighbors = 20 
UMAP_cv2_corrected_20_neighbors_0.01_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_cv2_corrected_20_neighbors_0.01_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_cv2_corrected_20_neighbors_0.01_min_dist")
UMAP_cv2_corrected_20_neighbors_0.05_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                                dimred = "UMAP_cv2_corrected_20_neighbors_0.05_min_dist",
                                                                colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                                shape_by = "cell.type"#, # "mouse.id"
                                                                #size_by = "detected"
                                                                ) + ggtitle("UMAP_cv2_corrected_20_neighbors_0.05_min_dist")
UMAP_cv2_corrected_20_neighbors_0.1_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_20_neighbors_0.1_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_20_neighbors_0.1_min_dist")
UMAP_cv2_corrected_20_neighbors_0.2_min_dist <- plotReducedDim(PAG_sceset_qc_norm_filt,
                                                               dimred = "UMAP_cv2_corrected_20_neighbors_0.2_min_dist",
                                                               colour_by = "PAG.arearegistration", # "batch.processing", "PAG.arearegistration"
                                                               shape_by = "cell.type"#, # "mouse.id"
                                                               #size_by = "detected"
                                                               ) + ggtitle("UMAP_cv2_corrected_20_neighbors_0.2_min_dist")

#### Plot them all:
multiplot(UMAP_cv2_corrected_5_neighbors_0.01_min_dist, UMAP_cv2_corrected_5_neighbors_0.05_min_dist,
          UMAP_cv2_corrected_5_neighbors_0.1_min_dist, UMAP_cv2_corrected_5_neighbors_0.2_min_dist,
          cols = 2)
multiplot(UMAP_cv2_corrected_10_neighbors_0.01_min_dist, UMAP_cv2_corrected_10_neighbors_0.05_min_dist,
          UMAP_cv2_corrected_10_neighbors_0.1_min_dist, UMAP_var_corrected_10_neighbors_0.2_min_dist,
          cols = 2)
multiplot(UMAP_cv2_corrected_15_neighbors_0.01_min_dist, UMAP_cv2_corrected_15_neighbors_0.05_min_dist,
          UMAP_cv2_corrected_15_neighbors_0.1_min_dist, UMAP_cv2_corrected_15_neighbors_0.2_min_dist,
          cols = 2)
multiplot(UMAP_cv2_corrected_20_neighbors_0.01_min_dist, UMAP_cv2_corrected_20_neighbors_0.05_min_dist,
          UMAP_cv2_corrected_20_neighbors_0.1_min_dist, UMAP_cv2_corrected_20_neighbors_0.2_min_dist,
          cols = 2)
```

Compared to t-SNE, the UMAP visualization tends to have more compact visual clusters with more empty space between them. It also attempts to preserve more of the global structure than t-SNE. From a practical perspective, UMAP is much faster than t-SNE, which may be an important consideration for large datasets. UMAP also involves a series of randomization steps so setting the seed is critical. 

Like t-SNE, UMAP has its own suite of hyperparameters that affect the visualization. Of these, the number of neighbours (`n_neighbors`) and the minimum distance between embedded points (`min_dist`) have the greatest effect on the granularity of the output. If these values are too low, random noise will be incorrectly treated as high-resolution structure, while values that are too high will discard fine structure altogether in favour of obtaining an accurate overview of the entire dataset. Again, it is a good idea to test a range of values for these parameters to ensure that they do not compromise any conclusions drawn from a UMAP plot.

It is arguable whether the UMAP or t-SNE visualizations are more useful or aesthetically pleasing. UMAP aims to preserve more global structure but this necessarily reduces resolution within each visual cluster. However, UMAP is unarguably much faster, and for that reason alone, it is increasingly displacing t-SNE as the method of choice for visualizing large scRNA-seq data sets.

### 5.4.4 | Final notes on interpeting dimensionality reduction plots
Dimensionality reduction for visualization necessarily involves discarding information and distorting the distances between cells in order to fit high-dimensional data into a 2-dimensional space. One might wonder whether the results of such extreme data compression can be trusted. Some of our more quantitative colleagues consider such visualizations to be more artistic than scientific, fit for little but impressing collaborators and reviewers. Perhaps this perspective is not entirely invalid, but we suggest that there is some value to be extracted from them provided that they are accompanied by an analysis of a higher-rank representation.

To illustrate, consider the interaction between clustering and t-SNE. As a general rule, we would not perform clustering on the t-SNE coordinates. Rather, we would cluster on the first 10-50 PCs and then visualize the cluster identities on the t-SNE plot. This ensures that clustering makes use of the information that was lost during compression into two dimensions. The t-SNE plot can then be used for a diagnostic inspection of the clustering output. In particular, the plot is most useful for checking whether two clusters are actually neighboring subclusters or whether a cluster can be split into further subclusters, which are generally safe interpretations of t-SNE coordinates.

It is worth elaborating on why we should not perform downstream analyses directly on the t-SNE or UMAP coordinates. Let us put aside the fact that operating on the high-dimensional representations preserves more information; from a naive perspective, using the t-SNE coordinates is very tempting as it ensures that any results are immediately consistent with the visualizations (regardless of whether they are right). However, this can actually be considered a disservice as it masks the uncertainty of the results, leading us to place more confidence in them than is warranted. Rather than being errors, major discrepancies can instead be useful for motivating further investigation into the more ambiguous parts of the dataset; conversely, the lack of discrepancies increases trust in the results.

## Step 5.5 | Save the filtered, corrected, and denoised SingleCellExperiment object
```{r}
saveRDS(PAG_sceset_qc_norm_filt, file = "PAG_sceset_qc_norm_filt_corr.rds")
print("Part 5 - Done!")
```

```{r}
sessionInfo()
```